{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network as layer\n",
    "import torch.nn as nn\n",
    "\n",
    "nc = 3\n",
    "nz = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "lr     = 0.001\n",
    "beta1  = 0.0     \n",
    "beta2  = 0.99     \n",
    "imageSize = 128\n",
    "batchSize = 16\n",
    "\n",
    "outf = \"./celeba_result/\"\n",
    "des_dir = \"./celeba/\"\n",
    "\n",
    "dataset = dset.ImageFolder(root=des_dir,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.CenterCrop(224),\n",
    "                               transforms.Resize(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size= batchSize,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers import *\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def make_dense(self, k_in, k_growth, n, options):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(Dense(layer.deconv(k_in, k_growth, 3, 1, 1, **options)))\n",
    "            k_in += k_growth\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        options = {'leaky':True, 'bn':False, 'wn':True, 'pixel':True}\n",
    "        layers = []\n",
    "        layers.append(layer.deconv(nz, 48, 4, 1, 3, **options))\n",
    "        \n",
    "        layers.append(Dense(layer.deconv(48, 24, 3, 1, 1, **options))) #(24 -> 36)\n",
    "        layers.append(Dense(layer.deconv(72, 24, 3, 1, 1, **options))) #(36 -> 48)\n",
    "        layers.append(Dense(layer.deconv(96, 24, 3, 1, 1, **options))) #(48 -> 60)\n",
    "        #4x4\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(Dense(layer.deconv(120, 24, 3, 1, 1, **options))) #(60 -> 72)\n",
    "        layers.append(Dense(layer.deconv(144, 24, 3, 1, 1, **options))) #(72 -> 84)\n",
    "        layers.append(Dense(layer.deconv(168, 24, 3, 1, 1, **options))) #(84 -> 96)\n",
    "        \n",
    "        layers.append(layer.deconv(192, 96, 3, 1, 1, **options)) #(96 -> 48)\n",
    "        layers.append(layer.deconv(96, 48, 3, 1, 1, **options)) #(48 -> 24)   \n",
    "        #8x8\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(Dense(layer.deconv(48, 24, 3, 1, 1, **options))) #(24 -> 36)\n",
    "        layers.append(Dense(layer.deconv(72, 24, 3, 1, 1, **options))) #(36 -> 48)\n",
    "        layers.append(Dense(layer.deconv(96, 24, 3, 1, 1, **options))) #(48 -> 60)\n",
    "        #4x4\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(Dense(layer.deconv(120, 24, 3, 1, 1, **options))) #(60 -> 72)\n",
    "        layers.append(Dense(layer.deconv(144, 24, 3, 1, 1, **options))) #(72 -> 84)\n",
    "        layers.append(Dense(layer.deconv(168, 24, 3, 1, 1, **options))) #(84 -> 96)\n",
    "        \n",
    "        layers.append(layer.deconv(192, 96, 3, 1, 1, **options)) #(96 -> 48)\n",
    "        layers.append(layer.deconv(96, 48, 3, 1, 1, **options)) #(48 -> 24)  \n",
    "        #32x32\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(Dense(layer.deconv(48, 24, 3, 1, 1, **options))) #(24 -> 36)\n",
    "        layers.append(Dense(layer.deconv(72, 24, 3, 1, 1, **options))) #(36 -> 48)\n",
    "        layers.append(Dense(layer.deconv(96, 24, 3, 1, 1, **options))) #(48 -> 60)\n",
    "        #4x4\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(Dense(layer.deconv(120, 24, 3, 1, 1, **options))) #(60 -> 72)\n",
    "        layers.append(Dense(layer.deconv(144, 24, 3, 1, 1, **options))) #(72 -> 84)\n",
    "        layers.append(Dense(layer.deconv(168, 24, 3, 1, 1, **options))) #(84 -> 96)\n",
    "        \n",
    "        layers.append(layer.deconv(192, 96, 3, 1, 1, **options)) #(96 -> 48)\n",
    "        layers.append(layer.deconv(96, 48, 3, 1, 1, **options)) #(48 -> 24)  \n",
    "        \n",
    "        layers.append(layer.deconv(48, nc, 1, 1, 0, leaky=True, bn=False, wn=True, pixel=True, only=True))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "netG = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        options = {'leaky':True, 'bn':False, 'wn':True, 'pixel':False, 'gdrop':True}\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(layer.conv(nc, 48, 1, 1, 0, **options)) #(nc -> 24)\n",
    "        \n",
    "        layers.append(Dense(layer.conv(48, 24, 3, 1, 1, **options))) #(24 -> 36)\n",
    "        layers.append(Dense(layer.conv(72, 24, 3, 1, 1, **options))) #(36 -> 48)\n",
    "        layers.append(Dense(layer.conv(96, 24, 3, 1, 1, **options))) #(48 -> 60)\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2)) \n",
    "        #64 x 64\n",
    "        layers.append(Dense(layer.conv(120, 24, 3, 1, 1, **options))) #(60 -> 72)\n",
    "        layers.append(Dense(layer.conv(144, 24, 3, 1, 1, **options))) #(72 -> 84)\n",
    "        layers.append(Dense(layer.conv(168, 24, 3, 1, 1, **options))) #(84 -> 96)\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2))\n",
    "        #32 x 32\n",
    "        layers.append(Dense(layer.conv(192, 24, 3, 1, 1, **options))) #(96 -> 108)\n",
    "        layers.append(Dense(layer.conv(216, 24, 3, 1, 1, **options))) #(108 -> 102)\n",
    "        layers.append(Dense(layer.conv(240, 24, 3, 1, 1, **options))) #(120 -> 132)\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2)) \n",
    "        #16 x 16\n",
    "        layers.append(Dense(layer.conv(264, 24, 3, 1, 1, **options))) #(132 -> 144)\n",
    "        layers.append(Dense(layer.conv(288, 24, 3, 1, 1, **options))) #(144 -> 156)\n",
    "        layers.append(Dense(layer.conv(312, 24, 3, 1, 1, **options))) #(156 -> 168)\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2)) \n",
    "        #8 x 8\n",
    "        layers.append(Dense(layer.conv(336, 24, 3, 1, 1, **options))) #(168 -> 180)\n",
    "        layers.append(Dense(layer.conv(360, 24, 3, 1, 1, **options))) #(180 -> 192)\n",
    "        layers.append(Dense(layer.conv(384, 24, 3, 1, 1, **options))) #(192 -> 204)\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2)) \n",
    "        #4 x 4\n",
    "        layers.append(Dense(layer.conv(408, 24, 3, 1, 1, **options))) #(204 -> 216)\n",
    "        layers.append(Dense(layer.conv(432, 24, 3, 1, 1, **options))) #(216 -> 228)\n",
    "        layers.append(Dense(layer.conv(456, 24, 3, 1, 1, **options))) #(228 -> 240)\n",
    "        \n",
    "        layers.append(minibatch_std_concat_layer())\n",
    "        layers.append(layer.conv(480 + 1, 480, 3, 1, 1, **options))\n",
    "        layers.append(layer.conv(480, 480, 4, 1, 0, **options))\n",
    "        layers.append(layer.linear(480, 1, sig=False, wn=True))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "netD = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, imageSize,imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "label_real = torch.FloatTensor(batchSize)\n",
    "label_fake = torch.FloatTensor(batchSize)\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "criterion.cuda()\n",
    "input, label_real, label_fake = input.cuda(), label_real.cuda(), label_fake.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "label_real.resize_(batchSize, 1).fill_(1)\n",
    "label_fake.resize_(batchSize, 1).fill_(0)\n",
    "label_real = Variable(label_real)\n",
    "label_fake = Variable(label_fake)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "schedulerD = optim.lr_scheduler.MultiStepLR(optimizerD, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "schedulerG = optim.lr_scheduler.MultiStepLR(optimizerG, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "result_dict = {}\n",
    "loss_D,loss_G,score_D,score_G1,score_G2 = [],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_d_ = None\n",
    "def add_noise(x, d_fake):\n",
    "    global _d_\n",
    "    if _d_ is not None:\n",
    "        _d_ = _d_ * 0.9 + torch.mean(d_fake).data[0] * 0.1\n",
    "        strength = 0.2 * max(0, _d_ - 0.5)**2\n",
    "        z = np.random.randn(*x.size()).astype(np.float32) * strength\n",
    "        z = Variable(torch.from_numpy(z)).cuda()\n",
    "        return x + z\n",
    "    else:\n",
    "        _d_ = 0.0\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][0/12663] Loss_D: 1.0065 Loss_G: 0.8456 D(x): -0.0032 D(G(z)): -0.0053\n",
      "[0/100][250/12663] Loss_D: 0.5099 Loss_G: 0.2344 D(x): 0.3273 D(G(z)): 0.2334\n",
      "[0/100][500/12663] Loss_D: 0.3765 Loss_G: 0.4738 D(x): 0.6177 D(G(z)): 0.4683\n",
      "[0/100][750/12663] Loss_D: 0.5991 Loss_G: 0.3290 D(x): 0.2638 D(G(z)): 0.2261\n",
      "[0/100][1000/12663] Loss_D: 0.4057 Loss_G: 0.7633 D(x): 0.7909 D(G(z)): 0.5825\n",
      "[0/100][1250/12663] Loss_D: 0.3975 Loss_G: 0.4438 D(x): 0.6175 D(G(z)): 0.4909\n",
      "[0/100][1500/12663] Loss_D: 0.3116 Loss_G: 0.4804 D(x): 0.7072 D(G(z)): 0.4572\n",
      "[0/100][1750/12663] Loss_D: 0.5843 Loss_G: 0.4661 D(x): 0.2519 D(G(z)): 0.0666\n",
      "[0/100][2000/12663] Loss_D: 0.4086 Loss_G: 0.3627 D(x): 0.5229 D(G(z)): 0.3803\n",
      "[0/100][2250/12663] Loss_D: 0.2994 Loss_G: 0.6142 D(x): 0.6705 D(G(z)): 0.3963\n",
      "[0/100][2500/12663] Loss_D: 0.3638 Loss_G: 0.5205 D(x): 0.5720 D(G(z)): 0.3586\n",
      "[0/100][2750/12663] Loss_D: 0.4430 Loss_G: 0.4740 D(x): 0.5202 D(G(z)): 0.4470\n",
      "[0/100][3000/12663] Loss_D: 0.6679 Loss_G: 1.1393 D(x): 0.8685 D(G(z)): 0.7873\n",
      "[0/100][3250/12663] Loss_D: 0.5341 Loss_G: 0.6938 D(x): 0.6773 D(G(z)): 0.6340\n",
      "[0/100][3500/12663] Loss_D: 0.4164 Loss_G: 0.4222 D(x): 0.5765 D(G(z)): 0.4632\n",
      "[0/100][3750/12663] Loss_D: 0.2224 Loss_G: 1.1590 D(x): 0.7240 D(G(z)): 0.3525\n",
      "[0/100][4000/12663] Loss_D: 0.3478 Loss_G: 0.5010 D(x): 0.6519 D(G(z)): 0.4495\n",
      "[0/100][4250/12663] Loss_D: 0.3258 Loss_G: 0.5786 D(x): 0.6527 D(G(z)): 0.4183\n",
      "[0/100][4500/12663] Loss_D: 0.8515 Loss_G: 0.0782 D(x): 0.0847 D(G(z)): -0.0158\n",
      "[0/100][4750/12663] Loss_D: 0.7464 Loss_G: 1.0977 D(x): 1.0699 D(G(z)): 0.8328\n",
      "[0/100][5000/12663] Loss_D: 0.3607 Loss_G: 0.3871 D(x): 0.4790 D(G(z)): 0.2498\n",
      "[0/100][5250/12663] Loss_D: 0.4334 Loss_G: 0.4775 D(x): 0.4864 D(G(z)): 0.3752\n",
      "[0/100][5500/12663] Loss_D: 0.4091 Loss_G: 0.4027 D(x): 0.5793 D(G(z)): 0.4448\n",
      "[0/100][5750/12663] Loss_D: 0.4191 Loss_G: 0.3567 D(x): 0.4696 D(G(z)): 0.3296\n",
      "[0/100][6000/12663] Loss_D: 0.3073 Loss_G: 0.2331 D(x): 0.5214 D(G(z)): 0.2048\n",
      "[0/100][6250/12663] Loss_D: 0.3540 Loss_G: 0.4715 D(x): 0.5863 D(G(z)): 0.3894\n"
     ]
    }
   ],
   "source": [
    "niter = 100\n",
    "d_fake_save = None\n",
    "for epoch in range(niter):\n",
    "    schedulerD.step()\n",
    "    schedulerG.step()\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        # train D\n",
    "        netD.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "\n",
    "        real_cpu = real_cpu.cuda()\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "\n",
    "        inputv = Variable(input)\n",
    "        inputv = add_noise(inputv, d_fake_save)\n",
    "        \n",
    "        d_real = netD(inputv)\n",
    "        d_real_mean = d_real.data.mean()\n",
    "        \n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        \n",
    "        d_fake_save = d_fake = netD(fake.detach())\n",
    "        d_fake_mean = d_fake.data.mean()\n",
    "        \n",
    "        loss_d = criterion(d_real, label_real) + criterion(d_fake, label_fake)\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # train G\n",
    "        netG.zero_grad()\n",
    "        d_fake = netD(fake)\n",
    "        loss_g = criterion(d_fake, label_real.detach())\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%250 == 0:\n",
    "            fake = netG(fixed_noise)\n",
    "            image = fake.data.mul_(2).add_(0.5)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03dstep_%04d.png' % (outf, epoch, i),\n",
    "                    normalize=True)\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f'\n",
    "                  % (epoch, niter, i, len(dataloader),\n",
    "                     loss_d.data[0], loss_g.data[0], d_real_mean, d_fake_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

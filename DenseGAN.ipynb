{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network as layer\n",
    "import torch.nn as nn\n",
    "\n",
    "nc = 3\n",
    "nz = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "lr     = 0.0002\n",
    "beta1  = 0.5   \n",
    "beta2  = 0.99     \n",
    "imageSize = 128\n",
    "batchSize = 32\n",
    "\n",
    "outf = \"./celeba_result/\"\n",
    "des_dir = \"./celeba/\"\n",
    "\n",
    "dataset = dset.ImageFolder(root=des_dir,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.CenterCrop(224),\n",
    "                               transforms.Resize(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size= batchSize,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers import *\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def make_dense(self, k_in, k_growth, n, options):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(Dense(layer.conv(k_in, k_growth, 3, 1, 1, **options)))\n",
    "            k_in += k_growth\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        options = {'leaky':False, 'bn':True, 'wn':False, 'pixel':False}\n",
    "        layers = []\n",
    "        k_growth = 12\n",
    "        layers.append(layer.conv(nz, 256, 4, 1, 3, **options))\n",
    "        \n",
    "        layers.append(self.make_dense(256, k_growth, 3, options))\n",
    "        layers.append(layer.conv(292, 256, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(256, 256, 3, 1, 1, **options))\n",
    "        # 4 x 4\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(self.make_dense(256, k_growth, 3, options))\n",
    "        layers.append(layer.conv(292, 256, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(256, 256, 3, 1, 1, **options))\n",
    "        # 8 x 8\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(self.make_dense(256, k_growth, 3, options))\n",
    "        layers.append(layer.conv(292, 256, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(256, 256, 3, 1, 1, **options))\n",
    "        # 16 x 16\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(self.make_dense(256, k_growth, 3, options))\n",
    "        layers.append(layer.conv(292, 128, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(128, 128, 3, 1, 1, **options))\n",
    "        # 32 x 32\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(self.make_dense(128, k_growth, 3, options))\n",
    "        layers.append(layer.conv(164, 64, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(64, 64, 3, 1, 1, **options))\n",
    "        # 64 x 64\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(self.make_dense(64, k_growth, 3, options))\n",
    "        layers.append(layer.conv(100, 32, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(32, 32, 3, 1, 1, **options))\n",
    "        # 128 x 128\n",
    "        layers.append(layer.conv(32, nc, 1, 1, 0, leaky=True, bn=False, wn=True, pixel=True, only=True))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "netG = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def make_dense(self, k_in, k_growth, n, options):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(Dense(layer.conv(k_in, k_growth, 3, 1, 1, **options)))\n",
    "            k_in += k_growth\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        options = {'leaky':True, 'bn':False, 'wn':True, 'pixel':False, 'gdrop':True}\n",
    "        \n",
    "        k_growth = 6\n",
    "        layers = []\n",
    "        # 128 x 128\n",
    "        layers.append(self.make_dense(nc, k_growth, 6, options))\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2)) \n",
    "        # 64 x 64\n",
    "        layers.append(self.make_dense(nc + 6 * k_growth, k_growth, 12, options))\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2)) \n",
    "        # 32 x 32\n",
    "        layers.append(self.make_dense(nc + 18 * k_growth, k_growth, 12, options))\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2)) \n",
    "        # 16 x 16\n",
    "        layers.append(self.make_dense(nc + 30 * k_growth, k_growth, 12, options))\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2)) \n",
    "        # 8 x 8\n",
    "        layers.append(self.make_dense(nc + 42 * k_growth, k_growth, 18, options))\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2)) \n",
    "        # 4 x 4\n",
    "        layers.append(self.make_dense(nc + 60 * k_growth, k_growth, 18, options))\n",
    "        \n",
    "        \n",
    "        layers.append(minibatch_std_concat_layer())\n",
    "        layers.append(layer.conv(nc + 78 * k_growth + 1, nc + 78 * k_growth, 3, 1, 1, **options))\n",
    "        layers.append(layer.conv(nc + 78 * k_growth, nc + 78 * k_growth, 4, 1, 0, **options))\n",
    "        layers.append(layer.linear(nc + 78 * k_growth, 1, sig=False, wn=True))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "netD = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, imageSize,imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "label_real = torch.FloatTensor(batchSize)\n",
    "label_fake = torch.FloatTensor(batchSize)\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "criterion.cuda()\n",
    "input, label_real, label_fake = input.cuda(), label_real.cuda(), label_fake.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "label_real.resize_(batchSize, 1).fill_(1)\n",
    "label_fake.resize_(batchSize, 1).fill_(0)\n",
    "label_real = Variable(label_real)\n",
    "label_fake = Variable(label_fake)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "schedulerD = optim.lr_scheduler.MultiStepLR(optimizerD, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "schedulerG = optim.lr_scheduler.MultiStepLR(optimizerG, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "result_dict = {}\n",
    "loss_D,loss_G,score_D,score_G1,score_G2 = [],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_d_ = None\n",
    "def add_noise(x, d_fake):\n",
    "    global _d_\n",
    "    if _d_ is not None:\n",
    "        _d_ = _d_ * 0.9 + torch.mean(d_fake).data[0] * 0.1\n",
    "        strength = 0.2 * max(0, _d_ - 0.5)**2\n",
    "        z = np.random.randn(*x.size()).astype(np.float32) * strength\n",
    "        z = Variable(torch.from_numpy(z)).cuda()\n",
    "        return x + z\n",
    "    else:\n",
    "        _d_ = 0.0\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][0/6330] Loss_D: 1.2027 Loss_G: 1.1229 D(x): -0.0933 D(G(z)): -0.0815\n",
      "[0/100][250/6330] Loss_D: 0.4413 Loss_G: 0.3645 D(x): 0.5307 D(G(z)): 0.3967\n",
      "[0/100][500/6330] Loss_D: 0.5284 Loss_G: 0.2709 D(x): 0.4723 D(G(z)): 0.4965\n",
      "[0/100][750/6330] Loss_D: 0.5209 Loss_G: 0.2421 D(x): 0.4970 D(G(z)): 0.5173\n",
      "[0/100][1000/6330] Loss_D: 0.5134 Loss_G: 0.2552 D(x): 0.4856 D(G(z)): 0.4987\n",
      "[0/100][1250/6330] Loss_D: 0.5138 Loss_G: 0.2429 D(x): 0.4987 D(G(z)): 0.5123\n",
      "[0/100][1500/6330] Loss_D: 0.5106 Loss_G: 0.2516 D(x): 0.4906 D(G(z)): 0.5010\n",
      "[0/100][1750/6330] Loss_D: 0.5160 Loss_G: 0.2438 D(x): 0.4987 D(G(z)): 0.5144\n",
      "[0/100][2000/6330] Loss_D: 0.5088 Loss_G: 0.2517 D(x): 0.4940 D(G(z)): 0.5027\n",
      "[0/100][2250/6330] Loss_D: 0.5069 Loss_G: 0.2514 D(x): 0.4944 D(G(z)): 0.5012\n",
      "[0/100][2500/6330] Loss_D: 0.5098 Loss_G: 0.2495 D(x): 0.4872 D(G(z)): 0.4967\n",
      "[0/100][2750/6330] Loss_D: 0.5088 Loss_G: 0.2534 D(x): 0.4852 D(G(z)): 0.4935\n",
      "[0/100][3000/6330] Loss_D: 0.5057 Loss_G: 0.2465 D(x): 0.5018 D(G(z)): 0.5075\n",
      "[0/100][3250/6330] Loss_D: 0.5047 Loss_G: 0.2453 D(x): 0.4989 D(G(z)): 0.5035\n",
      "[0/100][3500/6330] Loss_D: 0.5044 Loss_G: 0.2450 D(x): 0.5024 D(G(z)): 0.5067\n",
      "[0/100][3750/6330] Loss_D: 0.5055 Loss_G: 0.2512 D(x): 0.4960 D(G(z)): 0.5014\n",
      "[0/100][4000/6330] Loss_D: 0.5050 Loss_G: 0.2547 D(x): 0.4923 D(G(z)): 0.4972\n",
      "[0/100][4250/6330] Loss_D: 0.5024 Loss_G: 0.2495 D(x): 0.5010 D(G(z)): 0.5033\n",
      "[0/100][4500/6330] Loss_D: 0.5044 Loss_G: 0.2455 D(x): 0.5024 D(G(z)): 0.5066\n",
      "[0/100][4750/6330] Loss_D: 0.5025 Loss_G: 0.2539 D(x): 0.4969 D(G(z)): 0.4993\n",
      "[0/100][5000/6330] Loss_D: 0.5052 Loss_G: 0.2463 D(x): 0.4999 D(G(z)): 0.5049\n",
      "[0/100][5250/6330] Loss_D: 0.5014 Loss_G: 0.2494 D(x): 0.4984 D(G(z)): 0.4996\n",
      "[0/100][5500/6330] Loss_D: 0.4993 Loss_G: 0.2537 D(x): 0.4998 D(G(z)): 0.4991\n",
      "[0/100][5750/6330] Loss_D: 0.5033 Loss_G: 0.2484 D(x): 0.4989 D(G(z)): 0.5021\n",
      "[0/100][6000/6330] Loss_D: 0.5007 Loss_G: 0.2502 D(x): 0.4998 D(G(z)): 0.5004\n",
      "[0/100][6250/6330] Loss_D: 0.5023 Loss_G: 0.2501 D(x): 0.4979 D(G(z)): 0.5002\n",
      "[1/100][0/6330] Loss_D: 0.5020 Loss_G: 0.2485 D(x): 0.5024 D(G(z)): 0.5044\n",
      "[1/100][250/6330] Loss_D: 0.5018 Loss_G: 0.2491 D(x): 0.4980 D(G(z)): 0.4996\n",
      "[1/100][500/6330] Loss_D: 0.5011 Loss_G: 0.2498 D(x): 0.4986 D(G(z)): 0.4997\n",
      "[1/100][750/6330] Loss_D: 0.4990 Loss_G: 0.2474 D(x): 0.5040 D(G(z)): 0.5029\n",
      "[1/100][1000/6330] Loss_D: 0.5020 Loss_G: 0.2443 D(x): 0.4996 D(G(z)): 0.5016\n",
      "[1/100][1250/6330] Loss_D: 0.5026 Loss_G: 0.2505 D(x): 0.4993 D(G(z)): 0.5019\n",
      "[1/100][1500/6330] Loss_D: 0.5010 Loss_G: 0.2488 D(x): 0.4997 D(G(z)): 0.5005\n",
      "[1/100][1750/6330] Loss_D: 0.5023 Loss_G: 0.2488 D(x): 0.4981 D(G(z)): 0.5005\n",
      "[1/100][2000/6330] Loss_D: 0.5017 Loss_G: 0.2507 D(x): 0.4979 D(G(z)): 0.4996\n",
      "[1/100][2250/6330] Loss_D: 0.5011 Loss_G: 0.2498 D(x): 0.4994 D(G(z)): 0.5005\n",
      "[1/100][2500/6330] Loss_D: 0.4995 Loss_G: 0.2531 D(x): 0.5007 D(G(z)): 0.5002\n",
      "[1/100][2750/6330] Loss_D: 0.5009 Loss_G: 0.2510 D(x): 0.4972 D(G(z)): 0.4981\n",
      "[1/100][3000/6330] Loss_D: 0.5030 Loss_G: 0.2488 D(x): 0.4959 D(G(z)): 0.4988\n",
      "[1/100][3250/6330] Loss_D: 0.5010 Loss_G: 0.2494 D(x): 0.4987 D(G(z)): 0.4996\n",
      "[1/100][3500/6330] Loss_D: 0.5012 Loss_G: 0.2518 D(x): 0.5013 D(G(z)): 0.5025\n",
      "[1/100][3750/6330] Loss_D: 0.5011 Loss_G: 0.2520 D(x): 0.5009 D(G(z)): 0.5020\n",
      "[1/100][4000/6330] Loss_D: 0.5012 Loss_G: 0.2518 D(x): 0.4986 D(G(z)): 0.4998\n",
      "[1/100][4250/6330] Loss_D: 0.5020 Loss_G: 0.2517 D(x): 0.4987 D(G(z)): 0.5007\n",
      "[1/100][4500/6330] Loss_D: 0.5020 Loss_G: 0.2481 D(x): 0.4974 D(G(z)): 0.4994\n",
      "[1/100][4750/6330] Loss_D: 0.5011 Loss_G: 0.2526 D(x): 0.4973 D(G(z)): 0.4984\n",
      "[1/100][5000/6330] Loss_D: 0.5019 Loss_G: 0.2526 D(x): 0.4977 D(G(z)): 0.4995\n",
      "[1/100][5250/6330] Loss_D: 0.5019 Loss_G: 0.2473 D(x): 0.5000 D(G(z)): 0.5017\n",
      "[1/100][5500/6330] Loss_D: 0.4990 Loss_G: 0.2566 D(x): 0.5021 D(G(z)): 0.5009\n",
      "[1/100][5750/6330] Loss_D: 0.5005 Loss_G: 0.2504 D(x): 0.4981 D(G(z)): 0.4986\n",
      "[1/100][6000/6330] Loss_D: 0.5011 Loss_G: 0.2515 D(x): 0.4962 D(G(z)): 0.4972\n",
      "[1/100][6250/6330] Loss_D: 0.5013 Loss_G: 0.2512 D(x): 0.4976 D(G(z)): 0.4987\n",
      "[2/100][0/6330] Loss_D: 0.5011 Loss_G: 0.2496 D(x): 0.5009 D(G(z)): 0.5020\n",
      "[2/100][250/6330] Loss_D: 0.4984 Loss_G: 0.2500 D(x): 0.5041 D(G(z)): 0.5024\n",
      "[2/100][500/6330] Loss_D: 0.5000 Loss_G: 0.2484 D(x): 0.4999 D(G(z)): 0.4998\n",
      "[2/100][750/6330] Loss_D: 0.5013 Loss_G: 0.2509 D(x): 0.4963 D(G(z)): 0.4975\n",
      "[2/100][1000/6330] Loss_D: 0.5000 Loss_G: 0.2504 D(x): 0.5042 D(G(z)): 0.5041\n",
      "[2/100][1250/6330] Loss_D: 0.5008 Loss_G: 0.2488 D(x): 0.5009 D(G(z)): 0.5017\n",
      "[2/100][1500/6330] Loss_D: 0.4995 Loss_G: 0.2555 D(x): 0.4978 D(G(z)): 0.4973\n",
      "[2/100][1750/6330] Loss_D: 0.5004 Loss_G: 0.2524 D(x): 0.4999 D(G(z)): 0.5003\n",
      "[2/100][2000/6330] Loss_D: 0.5017 Loss_G: 0.2498 D(x): 0.4973 D(G(z)): 0.4990\n",
      "[2/100][2250/6330] Loss_D: 0.5011 Loss_G: 0.2501 D(x): 0.5001 D(G(z)): 0.5012\n",
      "[2/100][2500/6330] Loss_D: 0.5011 Loss_G: 0.2518 D(x): 0.4979 D(G(z)): 0.4990\n",
      "[2/100][2750/6330] Loss_D: 0.5015 Loss_G: 0.2486 D(x): 0.4998 D(G(z)): 0.5012\n",
      "[2/100][3000/6330] Loss_D: 0.5005 Loss_G: 0.2492 D(x): 0.4980 D(G(z)): 0.4985\n",
      "[2/100][3250/6330] Loss_D: 0.5002 Loss_G: 0.2515 D(x): 0.5001 D(G(z)): 0.5002\n",
      "[2/100][3500/6330] Loss_D: 0.5013 Loss_G: 0.2512 D(x): 0.4977 D(G(z)): 0.4990\n",
      "[2/100][3750/6330] Loss_D: 0.4986 Loss_G: 0.2533 D(x): 0.5020 D(G(z)): 0.5005\n",
      "[2/100][4000/6330] Loss_D: 0.5009 Loss_G: 0.2491 D(x): 0.5001 D(G(z)): 0.5010\n",
      "[2/100][4250/6330] Loss_D: 0.5006 Loss_G: 0.2520 D(x): 0.4978 D(G(z)): 0.4983\n",
      "[2/100][4500/6330] Loss_D: 0.5012 Loss_G: 0.2484 D(x): 0.4994 D(G(z)): 0.5005\n",
      "[2/100][4750/6330] Loss_D: 0.5005 Loss_G: 0.2519 D(x): 0.4967 D(G(z)): 0.4972\n",
      "[2/100][5000/6330] Loss_D: 0.5012 Loss_G: 0.2503 D(x): 0.4987 D(G(z)): 0.4999\n",
      "[2/100][5250/6330] Loss_D: 0.5004 Loss_G: 0.2494 D(x): 0.4990 D(G(z)): 0.4993\n",
      "[2/100][5500/6330] Loss_D: 0.5031 Loss_G: 0.2489 D(x): 0.4972 D(G(z)): 0.5002\n",
      "[2/100][5750/6330] Loss_D: 0.5024 Loss_G: 0.2498 D(x): 0.4981 D(G(z)): 0.5002\n",
      "[2/100][6000/6330] Loss_D: 0.5011 Loss_G: 0.2508 D(x): 0.4992 D(G(z)): 0.5002\n",
      "[2/100][6250/6330] Loss_D: 0.5014 Loss_G: 0.2513 D(x): 0.4980 D(G(z)): 0.4994\n",
      "[3/100][0/6330] Loss_D: 0.4992 Loss_G: 0.2499 D(x): 0.5019 D(G(z)): 0.5010\n",
      "[3/100][250/6330] Loss_D: 0.5008 Loss_G: 0.2493 D(x): 0.4997 D(G(z)): 0.5004\n",
      "[3/100][500/6330] Loss_D: 0.5025 Loss_G: 0.2489 D(x): 0.4964 D(G(z)): 0.4988\n",
      "[3/100][750/6330] Loss_D: 0.5007 Loss_G: 0.2481 D(x): 0.4992 D(G(z)): 0.4999\n",
      "[3/100][1000/6330] Loss_D: 0.5010 Loss_G: 0.2513 D(x): 0.4990 D(G(z)): 0.5000\n",
      "[3/100][1250/6330] Loss_D: 0.5017 Loss_G: 0.2500 D(x): 0.4979 D(G(z)): 0.4996\n",
      "[3/100][1500/6330] Loss_D: 0.5021 Loss_G: 0.2481 D(x): 0.4954 D(G(z)): 0.4975\n",
      "[3/100][1750/6330] Loss_D: 0.4994 Loss_G: 0.2528 D(x): 0.4991 D(G(z)): 0.4984\n",
      "[3/100][2000/6330] Loss_D: 0.5001 Loss_G: 0.2493 D(x): 0.5005 D(G(z)): 0.5006\n",
      "[3/100][2250/6330] Loss_D: 0.5013 Loss_G: 0.2492 D(x): 0.4962 D(G(z)): 0.4973\n",
      "[3/100][2500/6330] Loss_D: 0.5007 Loss_G: 0.2510 D(x): 0.4993 D(G(z)): 0.5000\n",
      "[3/100][2750/6330] Loss_D: 0.4999 Loss_G: 0.2518 D(x): 0.5003 D(G(z)): 0.5002\n",
      "[3/100][3000/6330] Loss_D: 0.5003 Loss_G: 0.2534 D(x): 0.4997 D(G(z)): 0.5000\n",
      "[3/100][3250/6330] Loss_D: 0.4999 Loss_G: 0.2507 D(x): 0.5021 D(G(z)): 0.5019\n",
      "[3/100][3500/6330] Loss_D: 0.5005 Loss_G: 0.2510 D(x): 0.4991 D(G(z)): 0.4995\n",
      "[3/100][3750/6330] Loss_D: 0.5008 Loss_G: 0.2500 D(x): 0.5008 D(G(z)): 0.5015\n",
      "[3/100][4000/6330] Loss_D: 0.5001 Loss_G: 0.2496 D(x): 0.5009 D(G(z)): 0.5010\n",
      "[3/100][4250/6330] Loss_D: 0.5008 Loss_G: 0.2505 D(x): 0.4991 D(G(z)): 0.4999\n",
      "[3/100][4500/6330] Loss_D: 0.5019 Loss_G: 0.2477 D(x): 0.4991 D(G(z)): 0.5009\n",
      "[3/100][4750/6330] Loss_D: 0.5000 Loss_G: 0.2498 D(x): 0.5001 D(G(z)): 0.5000\n",
      "[3/100][5000/6330] Loss_D: 0.5007 Loss_G: 0.2479 D(x): 0.5007 D(G(z)): 0.5013\n",
      "[3/100][5250/6330] Loss_D: 0.5004 Loss_G: 0.2496 D(x): 0.4997 D(G(z)): 0.5000\n",
      "[3/100][5500/6330] Loss_D: 0.5004 Loss_G: 0.2488 D(x): 0.5004 D(G(z)): 0.5008\n",
      "[3/100][5750/6330] Loss_D: 0.5007 Loss_G: 0.2497 D(x): 0.4995 D(G(z)): 0.5002\n",
      "[3/100][6000/6330] Loss_D: 0.5008 Loss_G: 0.2505 D(x): 0.4985 D(G(z)): 0.4992\n",
      "[3/100][6250/6330] Loss_D: 0.5004 Loss_G: 0.2501 D(x): 0.5003 D(G(z)): 0.5007\n",
      "[4/100][0/6330] Loss_D: 0.4999 Loss_G: 0.2506 D(x): 0.4998 D(G(z)): 0.4996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/100][250/6330] Loss_D: 0.5004 Loss_G: 0.2504 D(x): 0.4996 D(G(z)): 0.4999\n",
      "[4/100][500/6330] Loss_D: 0.5005 Loss_G: 0.2511 D(x): 0.5005 D(G(z)): 0.5010\n",
      "[4/100][750/6330] Loss_D: 0.5012 Loss_G: 0.2493 D(x): 0.4982 D(G(z)): 0.4993\n",
      "[4/100][1000/6330] Loss_D: 0.5005 Loss_G: 0.2523 D(x): 0.4984 D(G(z)): 0.4989\n",
      "[4/100][1250/6330] Loss_D: 0.5009 Loss_G: 0.2493 D(x): 0.4978 D(G(z)): 0.4987\n",
      "[4/100][1500/6330] Loss_D: 0.5004 Loss_G: 0.2503 D(x): 0.4988 D(G(z)): 0.4991\n",
      "[4/100][1750/6330] Loss_D: 0.5004 Loss_G: 0.2486 D(x): 0.5009 D(G(z)): 0.5013\n",
      "[4/100][2000/6330] Loss_D: 0.5004 Loss_G: 0.2491 D(x): 0.4980 D(G(z)): 0.4984\n",
      "[4/100][2250/6330] Loss_D: 0.4999 Loss_G: 0.2497 D(x): 0.4987 D(G(z)): 0.4985\n",
      "[4/100][2500/6330] Loss_D: 0.5010 Loss_G: 0.2508 D(x): 0.4985 D(G(z)): 0.4994\n",
      "[4/100][2750/6330] Loss_D: 0.5009 Loss_G: 0.2479 D(x): 0.5001 D(G(z)): 0.5010\n",
      "[4/100][3000/6330] Loss_D: 0.5009 Loss_G: 0.2487 D(x): 0.4997 D(G(z)): 0.5006\n",
      "[4/100][3250/6330] Loss_D: 0.5012 Loss_G: 0.2482 D(x): 0.4997 D(G(z)): 0.5009\n",
      "[4/100][3500/6330] Loss_D: 0.5009 Loss_G: 0.2505 D(x): 0.5007 D(G(z)): 0.5016\n",
      "[4/100][3750/6330] Loss_D: 0.5005 Loss_G: 0.2511 D(x): 0.5004 D(G(z)): 0.5009\n",
      "[4/100][4000/6330] Loss_D: 0.4997 Loss_G: 0.2507 D(x): 0.5015 D(G(z)): 0.5012\n",
      "[4/100][4250/6330] Loss_D: 0.5010 Loss_G: 0.2488 D(x): 0.5010 D(G(z)): 0.5019\n",
      "[4/100][4500/6330] Loss_D: 0.5003 Loss_G: 0.2513 D(x): 0.5000 D(G(z)): 0.5002\n",
      "[4/100][4750/6330] Loss_D: 0.5005 Loss_G: 0.2503 D(x): 0.4994 D(G(z)): 0.4999\n",
      "[4/100][5000/6330] Loss_D: 0.5006 Loss_G: 0.2514 D(x): 0.4993 D(G(z)): 0.4999\n",
      "[4/100][5250/6330] Loss_D: 0.5002 Loss_G: 0.2501 D(x): 0.5005 D(G(z)): 0.5007\n",
      "[4/100][5500/6330] Loss_D: 0.4999 Loss_G: 0.2503 D(x): 0.5006 D(G(z)): 0.5005\n",
      "[4/100][5750/6330] Loss_D: 0.4994 Loss_G: 0.2523 D(x): 0.5019 D(G(z)): 0.5012\n",
      "[4/100][6000/6330] Loss_D: 0.5009 Loss_G: 0.2498 D(x): 0.4996 D(G(z)): 0.5005\n",
      "[4/100][6250/6330] Loss_D: 0.5001 Loss_G: 0.2505 D(x): 0.5012 D(G(z)): 0.5012\n",
      "[5/100][0/6330] Loss_D: 0.4995 Loss_G: 0.2513 D(x): 0.5001 D(G(z)): 0.4996\n",
      "[5/100][250/6330] Loss_D: 0.5010 Loss_G: 0.2497 D(x): 0.4979 D(G(z)): 0.4989\n",
      "[5/100][500/6330] Loss_D: 0.5008 Loss_G: 0.2494 D(x): 0.5011 D(G(z)): 0.5019\n",
      "[5/100][750/6330] Loss_D: 0.5005 Loss_G: 0.2510 D(x): 0.5009 D(G(z)): 0.5014\n",
      "[5/100][1000/6330] Loss_D: 0.5010 Loss_G: 0.2501 D(x): 0.4999 D(G(z)): 0.5009\n",
      "[5/100][1250/6330] Loss_D: 0.5005 Loss_G: 0.2501 D(x): 0.5010 D(G(z)): 0.5014\n",
      "[5/100][1500/6330] Loss_D: 0.4999 Loss_G: 0.2515 D(x): 0.5018 D(G(z)): 0.5017\n",
      "[5/100][1750/6330] Loss_D: 0.5013 Loss_G: 0.2492 D(x): 0.4995 D(G(z)): 0.5008\n",
      "[5/100][2000/6330] Loss_D: 0.5002 Loss_G: 0.2503 D(x): 0.5009 D(G(z)): 0.5010\n",
      "[5/100][2250/6330] Loss_D: 0.5004 Loss_G: 0.2504 D(x): 0.5001 D(G(z)): 0.5004\n",
      "[5/100][2500/6330] Loss_D: 0.5004 Loss_G: 0.2494 D(x): 0.5005 D(G(z)): 0.5009\n",
      "[5/100][2750/6330] Loss_D: 0.4999 Loss_G: 0.2501 D(x): 0.4974 D(G(z)): 0.4973\n",
      "[5/100][3000/6330] Loss_D: 0.5011 Loss_G: 0.2511 D(x): 0.4990 D(G(z)): 0.5000\n",
      "[5/100][3250/6330] Loss_D: 0.4995 Loss_G: 0.2492 D(x): 0.5007 D(G(z)): 0.5001\n",
      "[5/100][3500/6330] Loss_D: 0.5010 Loss_G: 0.2503 D(x): 0.4975 D(G(z)): 0.4984\n",
      "[5/100][3750/6330] Loss_D: 0.5003 Loss_G: 0.2504 D(x): 0.5001 D(G(z)): 0.5004\n",
      "[5/100][4000/6330] Loss_D: 0.5001 Loss_G: 0.2502 D(x): 0.5006 D(G(z)): 0.5007\n",
      "[5/100][4250/6330] Loss_D: 0.5007 Loss_G: 0.2500 D(x): 0.4993 D(G(z)): 0.4999\n",
      "[5/100][4500/6330] Loss_D: 0.4996 Loss_G: 0.2502 D(x): 0.5014 D(G(z)): 0.5009\n",
      "[5/100][4750/6330] Loss_D: 0.4986 Loss_G: 0.2509 D(x): 0.5005 D(G(z)): 0.4990\n",
      "[5/100][5000/6330] Loss_D: 0.4992 Loss_G: 0.2501 D(x): 0.5018 D(G(z)): 0.5010\n",
      "[5/100][5250/6330] Loss_D: 0.4999 Loss_G: 0.2502 D(x): 0.5012 D(G(z)): 0.5011\n",
      "[5/100][5500/6330] Loss_D: 0.5000 Loss_G: 0.2507 D(x): 0.4995 D(G(z)): 0.4995\n",
      "[5/100][5750/6330] Loss_D: 0.5006 Loss_G: 0.2495 D(x): 0.4998 D(G(z)): 0.5004\n",
      "[5/100][6000/6330] Loss_D: 0.5004 Loss_G: 0.2505 D(x): 0.5000 D(G(z)): 0.5003\n",
      "[5/100][6250/6330] Loss_D: 0.5007 Loss_G: 0.2503 D(x): 0.4990 D(G(z)): 0.4997\n",
      "[6/100][0/6330] Loss_D: 0.4994 Loss_G: 0.2506 D(x): 0.5008 D(G(z)): 0.5001\n",
      "[6/100][250/6330] Loss_D: 0.5004 Loss_G: 0.2504 D(x): 0.4997 D(G(z)): 0.5000\n",
      "[6/100][500/6330] Loss_D: 0.4990 Loss_G: 0.2515 D(x): 0.5016 D(G(z)): 0.5006\n",
      "[6/100][750/6330] Loss_D: 0.5005 Loss_G: 0.2494 D(x): 0.4992 D(G(z)): 0.4996\n",
      "[6/100][1000/6330] Loss_D: 0.5007 Loss_G: 0.2491 D(x): 0.5003 D(G(z)): 0.5010\n",
      "[6/100][1250/6330] Loss_D: 0.4997 Loss_G: 0.2518 D(x): 0.5026 D(G(z)): 0.5023\n",
      "[6/100][1500/6330] Loss_D: 0.5006 Loss_G: 0.2503 D(x): 0.4999 D(G(z)): 0.5004\n",
      "[6/100][1750/6330] Loss_D: 0.5008 Loss_G: 0.2501 D(x): 0.4996 D(G(z)): 0.5004\n",
      "[6/100][2000/6330] Loss_D: 0.4999 Loss_G: 0.2513 D(x): 0.5002 D(G(z)): 0.5000\n",
      "[6/100][2250/6330] Loss_D: 0.5005 Loss_G: 0.2504 D(x): 0.4989 D(G(z)): 0.4994\n",
      "[6/100][2500/6330] Loss_D: 0.5013 Loss_G: 0.2503 D(x): 0.4994 D(G(z)): 0.5006\n",
      "[6/100][2750/6330] Loss_D: 0.5001 Loss_G: 0.2501 D(x): 0.5006 D(G(z)): 0.5006\n",
      "[6/100][3000/6330] Loss_D: 0.4997 Loss_G: 0.2520 D(x): 0.5009 D(G(z)): 0.5006\n",
      "[6/100][3250/6330] Loss_D: 0.4999 Loss_G: 0.2508 D(x): 0.5005 D(G(z)): 0.5003\n",
      "[6/100][3500/6330] Loss_D: 0.5006 Loss_G: 0.2488 D(x): 0.5003 D(G(z)): 0.5009\n",
      "[6/100][3750/6330] Loss_D: 0.5007 Loss_G: 0.2484 D(x): 0.4980 D(G(z)): 0.4986\n",
      "[6/100][4000/6330] Loss_D: 0.5006 Loss_G: 0.2497 D(x): 0.5006 D(G(z)): 0.5012\n",
      "[6/100][4250/6330] Loss_D: 0.4992 Loss_G: 0.2511 D(x): 0.5006 D(G(z)): 0.4998\n",
      "[6/100][4500/6330] Loss_D: 0.5006 Loss_G: 0.2490 D(x): 0.4995 D(G(z)): 0.5001\n",
      "[6/100][4750/6330] Loss_D: 0.5011 Loss_G: 0.2483 D(x): 0.4994 D(G(z)): 0.5004\n",
      "[6/100][5000/6330] Loss_D: 0.5004 Loss_G: 0.2497 D(x): 0.5025 D(G(z)): 0.5028\n",
      "[6/100][5250/6330] Loss_D: 0.4998 Loss_G: 0.2524 D(x): 0.5036 D(G(z)): 0.5034\n",
      "[6/100][5500/6330] Loss_D: 0.5002 Loss_G: 0.2501 D(x): 0.4999 D(G(z)): 0.5001\n",
      "[6/100][5750/6330] Loss_D: 0.5014 Loss_G: 0.2501 D(x): 0.4982 D(G(z)): 0.4995\n",
      "[6/100][6000/6330] Loss_D: 0.4986 Loss_G: 0.2506 D(x): 0.5012 D(G(z)): 0.4997\n",
      "[6/100][6250/6330] Loss_D: 0.4997 Loss_G: 0.2500 D(x): 0.5013 D(G(z)): 0.5009\n",
      "[7/100][0/6330] Loss_D: 0.5006 Loss_G: 0.2515 D(x): 0.4997 D(G(z)): 0.5003\n",
      "[7/100][250/6330] Loss_D: 0.5003 Loss_G: 0.2500 D(x): 0.4999 D(G(z)): 0.5002\n",
      "[7/100][500/6330] Loss_D: 0.5003 Loss_G: 0.2500 D(x): 0.5015 D(G(z)): 0.5018\n",
      "[7/100][750/6330] Loss_D: 0.4993 Loss_G: 0.2523 D(x): 0.5013 D(G(z)): 0.5005\n",
      "[7/100][1000/6330] Loss_D: 0.5000 Loss_G: 0.2495 D(x): 0.5014 D(G(z)): 0.5014\n",
      "[7/100][1250/6330] Loss_D: 0.4995 Loss_G: 0.2512 D(x): 0.5016 D(G(z)): 0.5011\n",
      "[7/100][1500/6330] Loss_D: 0.5001 Loss_G: 0.2524 D(x): 0.5007 D(G(z)): 0.5008\n",
      "[7/100][1750/6330] Loss_D: 0.4991 Loss_G: 0.2521 D(x): 0.4987 D(G(z)): 0.4977\n",
      "[7/100][2000/6330] Loss_D: 0.4992 Loss_G: 0.2517 D(x): 0.5020 D(G(z)): 0.5011\n",
      "[7/100][2250/6330] Loss_D: 0.4989 Loss_G: 0.2554 D(x): 0.5020 D(G(z)): 0.5009\n",
      "[7/100][2500/6330] Loss_D: 0.4975 Loss_G: 0.2535 D(x): 0.5013 D(G(z)): 0.4988\n",
      "[7/100][2750/6330] Loss_D: 0.5007 Loss_G: 0.2551 D(x): 0.4982 D(G(z)): 0.4988\n",
      "[7/100][3000/6330] Loss_D: 0.5005 Loss_G: 0.2491 D(x): 0.5011 D(G(z)): 0.5016\n",
      "[7/100][3250/6330] Loss_D: 0.4989 Loss_G: 0.2501 D(x): 0.5037 D(G(z)): 0.5025\n",
      "[7/100][3500/6330] Loss_D: 0.4983 Loss_G: 0.2520 D(x): 0.5013 D(G(z)): 0.4996\n",
      "[7/100][3750/6330] Loss_D: 0.5009 Loss_G: 0.2531 D(x): 0.4976 D(G(z)): 0.4984\n",
      "[7/100][4000/6330] Loss_D: 0.4973 Loss_G: 0.2408 D(x): 0.4995 D(G(z)): 0.4968\n",
      "[7/100][4250/6330] Loss_D: 0.4976 Loss_G: 0.2531 D(x): 0.5108 D(G(z)): 0.5082\n",
      "[7/100][4500/6330] Loss_D: 0.4974 Loss_G: 0.2511 D(x): 0.5032 D(G(z)): 0.5005\n",
      "[7/100][4750/6330] Loss_D: 0.4964 Loss_G: 0.2424 D(x): 0.5024 D(G(z)): 0.4987\n",
      "[7/100][5000/6330] Loss_D: 0.4934 Loss_G: 0.2553 D(x): 0.5012 D(G(z)): 0.4946\n",
      "[7/100][5250/6330] Loss_D: 0.4979 Loss_G: 0.2478 D(x): 0.5084 D(G(z)): 0.5061\n",
      "[7/100][5500/6330] Loss_D: 0.4982 Loss_G: 0.2523 D(x): 0.5032 D(G(z)): 0.5013\n",
      "[7/100][5750/6330] Loss_D: 0.4960 Loss_G: 0.2536 D(x): 0.4911 D(G(z)): 0.4867\n",
      "[7/100][6000/6330] Loss_D: 0.4984 Loss_G: 0.2591 D(x): 0.5204 D(G(z)): 0.5180\n",
      "[7/100][6250/6330] Loss_D: 0.4969 Loss_G: 0.2737 D(x): 0.5164 D(G(z)): 0.5128\n",
      "[8/100][0/6330] Loss_D: 0.5035 Loss_G: 0.2372 D(x): 0.5087 D(G(z)): 0.5119\n",
      "[8/100][250/6330] Loss_D: 0.4957 Loss_G: 0.2476 D(x): 0.5120 D(G(z)): 0.5074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/100][500/6330] Loss_D: 0.4944 Loss_G: 0.2673 D(x): 0.5094 D(G(z)): 0.5036\n",
      "[8/100][750/6330] Loss_D: 0.4969 Loss_G: 0.2631 D(x): 0.5169 D(G(z)): 0.5132\n",
      "[8/100][1000/6330] Loss_D: 0.4915 Loss_G: 0.2651 D(x): 0.5167 D(G(z)): 0.5078\n",
      "[8/100][1250/6330] Loss_D: 0.4778 Loss_G: 0.2650 D(x): 0.5368 D(G(z)): 0.5129\n",
      "[8/100][1500/6330] Loss_D: 0.4733 Loss_G: 0.2516 D(x): 0.5070 D(G(z)): 0.4796\n",
      "[8/100][1750/6330] Loss_D: 0.4734 Loss_G: 0.2779 D(x): 0.5066 D(G(z)): 0.4794\n",
      "[8/100][2000/6330] Loss_D: 0.4611 Loss_G: 0.3456 D(x): 0.5758 D(G(z)): 0.5298\n",
      "[8/100][2250/6330] Loss_D: 0.4366 Loss_G: 0.3020 D(x): 0.5393 D(G(z)): 0.4732\n",
      "[8/100][2500/6330] Loss_D: 0.4965 Loss_G: 0.2603 D(x): 0.5604 D(G(z)): 0.5504\n",
      "[8/100][2750/6330] Loss_D: 0.4904 Loss_G: 0.2733 D(x): 0.5475 D(G(z)): 0.5343\n",
      "[8/100][3000/6330] Loss_D: 0.4963 Loss_G: 0.3069 D(x): 0.5340 D(G(z)): 0.5281\n",
      "[8/100][3250/6330] Loss_D: 0.4781 Loss_G: 0.2623 D(x): 0.5527 D(G(z)): 0.5271\n",
      "[8/100][3500/6330] Loss_D: 0.4542 Loss_G: 0.3004 D(x): 0.4984 D(G(z)): 0.4499\n",
      "[8/100][3750/6330] Loss_D: 0.5002 Loss_G: 0.2154 D(x): 0.4714 D(G(z)): 0.4696\n",
      "[8/100][4000/6330] Loss_D: 0.4793 Loss_G: 0.2937 D(x): 0.5024 D(G(z)): 0.4810\n",
      "[8/100][4250/6330] Loss_D: 0.4553 Loss_G: 0.2713 D(x): 0.4859 D(G(z)): 0.4366\n",
      "[8/100][4500/6330] Loss_D: 0.4885 Loss_G: 0.3437 D(x): 0.5287 D(G(z)): 0.5157\n",
      "[8/100][4750/6330] Loss_D: 0.4633 Loss_G: 0.2971 D(x): 0.5481 D(G(z)): 0.5087\n",
      "[8/100][5000/6330] Loss_D: 0.4961 Loss_G: 0.2706 D(x): 0.5318 D(G(z)): 0.5260\n",
      "[8/100][5250/6330] Loss_D: 0.4851 Loss_G: 0.4104 D(x): 0.5581 D(G(z)): 0.5381\n",
      "[8/100][5500/6330] Loss_D: 0.4565 Loss_G: 0.3668 D(x): 0.4994 D(G(z)): 0.4535\n",
      "[8/100][5750/6330] Loss_D: 0.4622 Loss_G: 0.3198 D(x): 0.5140 D(G(z)): 0.4750\n",
      "[8/100][6000/6330] Loss_D: 0.4750 Loss_G: 0.3754 D(x): 0.5163 D(G(z)): 0.4907\n",
      "[8/100][6250/6330] Loss_D: 0.5251 Loss_G: 0.4180 D(x): 0.5945 D(G(z)): 0.5999\n",
      "[9/100][0/6330] Loss_D: 0.4746 Loss_G: 0.2625 D(x): 0.5455 D(G(z)): 0.5168\n",
      "[9/100][250/6330] Loss_D: 0.4888 Loss_G: 0.1397 D(x): 0.4043 D(G(z)): 0.3654\n",
      "[9/100][500/6330] Loss_D: 0.4656 Loss_G: 0.4665 D(x): 0.5681 D(G(z)): 0.5278\n",
      "[9/100][750/6330] Loss_D: 0.4875 Loss_G: 0.2583 D(x): 0.5071 D(G(z)): 0.4941\n",
      "[9/100][1000/6330] Loss_D: 0.4146 Loss_G: 0.3735 D(x): 0.6698 D(G(z)): 0.5524\n",
      "[9/100][1250/6330] Loss_D: 0.4093 Loss_G: 0.2905 D(x): 0.5909 D(G(z)): 0.4916\n",
      "[9/100][1500/6330] Loss_D: 0.5031 Loss_G: 0.2918 D(x): 0.5632 D(G(z)): 0.5584\n",
      "[9/100][1750/6330] Loss_D: 0.4120 Loss_G: 0.4030 D(x): 0.6479 D(G(z)): 0.5360\n",
      "[9/100][2000/6330] Loss_D: 0.4940 Loss_G: 0.1346 D(x): 0.4119 D(G(z)): 0.3839\n",
      "[9/100][2250/6330] Loss_D: 0.5038 Loss_G: 0.2613 D(x): 0.4919 D(G(z)): 0.4953\n",
      "[9/100][2500/6330] Loss_D: 0.4984 Loss_G: 0.1977 D(x): 0.4451 D(G(z)): 0.4359\n",
      "[9/100][2750/6330] Loss_D: 0.5122 Loss_G: 0.2542 D(x): 0.4506 D(G(z)): 0.4572\n",
      "[9/100][3000/6330] Loss_D: 0.5427 Loss_G: 0.2404 D(x): 0.4080 D(G(z)): 0.4378\n",
      "[9/100][3250/6330] Loss_D: 0.5186 Loss_G: 0.2212 D(x): 0.5237 D(G(z)): 0.5396\n",
      "[9/100][3500/6330] Loss_D: 0.5617 Loss_G: 0.2468 D(x): 0.3487 D(G(z)): 0.3698\n",
      "[9/100][3750/6330] Loss_D: 0.3974 Loss_G: 0.3597 D(x): 0.6067 D(G(z)): 0.4921\n",
      "[9/100][4000/6330] Loss_D: 0.4249 Loss_G: 0.2673 D(x): 0.4813 D(G(z)): 0.3937\n",
      "[9/100][4250/6330] Loss_D: 0.4772 Loss_G: 0.2973 D(x): 0.4790 D(G(z)): 0.4531\n",
      "[9/100][4500/6330] Loss_D: 0.4150 Loss_G: 0.4190 D(x): 0.4738 D(G(z)): 0.3705\n",
      "[9/100][4750/6330] Loss_D: 0.4071 Loss_G: 0.4448 D(x): 0.6095 D(G(z)): 0.5041\n",
      "[9/100][5000/6330] Loss_D: 0.5035 Loss_G: 0.2521 D(x): 0.5294 D(G(z)): 0.5304\n",
      "[9/100][5250/6330] Loss_D: 0.4780 Loss_G: 0.3118 D(x): 0.4620 D(G(z)): 0.4336\n",
      "[9/100][5500/6330] Loss_D: 0.4848 Loss_G: 0.2567 D(x): 0.5361 D(G(z)): 0.5189\n",
      "[9/100][5750/6330] Loss_D: 0.4498 Loss_G: 0.3702 D(x): 0.5454 D(G(z)): 0.4926\n",
      "[9/100][6000/6330] Loss_D: 0.5356 Loss_G: 0.2566 D(x): 0.5353 D(G(z)): 0.5646\n",
      "[9/100][6250/6330] Loss_D: 0.5269 Loss_G: 0.2835 D(x): 0.4660 D(G(z)): 0.4909\n",
      "[10/100][0/6330] Loss_D: 0.4689 Loss_G: 0.2655 D(x): 0.4594 D(G(z)): 0.4194\n",
      "[10/100][250/6330] Loss_D: 0.5123 Loss_G: 0.3842 D(x): 0.5546 D(G(z)): 0.5598\n",
      "[10/100][500/6330] Loss_D: 0.5288 Loss_G: 0.2706 D(x): 0.4739 D(G(z)): 0.5013\n",
      "[10/100][750/6330] Loss_D: 0.4814 Loss_G: 0.3568 D(x): 0.6017 D(G(z)): 0.5674\n",
      "[10/100][1000/6330] Loss_D: 0.4673 Loss_G: 0.3517 D(x): 0.5640 D(G(z)): 0.5261\n",
      "[10/100][1250/6330] Loss_D: 0.4967 Loss_G: 0.4538 D(x): 0.5493 D(G(z)): 0.5414\n",
      "[10/100][1500/6330] Loss_D: 0.4700 Loss_G: 0.2686 D(x): 0.5015 D(G(z)): 0.4701\n",
      "[10/100][1750/6330] Loss_D: 0.5061 Loss_G: 0.3024 D(x): 0.5398 D(G(z)): 0.5420\n",
      "[10/100][2000/6330] Loss_D: 0.4920 Loss_G: 0.3548 D(x): 0.6112 D(G(z)): 0.5831\n",
      "[10/100][2250/6330] Loss_D: 0.5479 Loss_G: 0.3610 D(x): 0.4828 D(G(z)): 0.5288\n",
      "[10/100][2500/6330] Loss_D: 0.4331 Loss_G: 0.3726 D(x): 0.5341 D(G(z)): 0.4641\n",
      "[10/100][2750/6330] Loss_D: 0.4776 Loss_G: 0.3423 D(x): 0.5512 D(G(z)): 0.5241\n",
      "[10/100][3000/6330] Loss_D: 0.4500 Loss_G: 0.3686 D(x): 0.5650 D(G(z)): 0.5101\n",
      "[10/100][3250/6330] Loss_D: 0.5373 Loss_G: 0.2724 D(x): 0.3719 D(G(z)): 0.3770\n",
      "[10/100][3500/6330] Loss_D: 0.5058 Loss_G: 0.3397 D(x): 0.5608 D(G(z)): 0.5585\n",
      "[10/100][3750/6330] Loss_D: 0.5108 Loss_G: 0.2906 D(x): 0.4942 D(G(z)): 0.5031\n",
      "[10/100][4000/6330] Loss_D: 0.4796 Loss_G: 0.2113 D(x): 0.5408 D(G(z)): 0.5177\n",
      "[10/100][4250/6330] Loss_D: 0.4759 Loss_G: 0.3146 D(x): 0.4660 D(G(z)): 0.4360\n",
      "[10/100][4500/6330] Loss_D: 0.4888 Loss_G: 0.3347 D(x): 0.5371 D(G(z)): 0.5230\n",
      "[10/100][4750/6330] Loss_D: 0.4774 Loss_G: 0.2112 D(x): 0.4805 D(G(z)): 0.4548\n",
      "[10/100][5000/6330] Loss_D: 0.4898 Loss_G: 0.3170 D(x): 0.5078 D(G(z)): 0.4968\n"
     ]
    }
   ],
   "source": [
    "niter = 100\n",
    "d_fake_save = None\n",
    "for epoch in range(niter):\n",
    "    schedulerD.step()\n",
    "    schedulerG.step()\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        # train D\n",
    "        netD.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "\n",
    "        real_cpu = real_cpu.cuda()\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "\n",
    "        inputv = Variable(input)\n",
    "        inputv = add_noise(inputv, d_fake_save)\n",
    "        \n",
    "        d_real = netD(inputv)\n",
    "        d_real_mean = d_real.data.mean()\n",
    "        \n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        \n",
    "        d_fake_save = d_fake = netD(fake.detach())\n",
    "        d_fake_mean = d_fake.data.mean()\n",
    "        \n",
    "        loss_d = criterion(d_real, label_real) + criterion(d_fake, label_fake)\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # train G\n",
    "        netG.zero_grad()\n",
    "        d_fake = netD(fake)\n",
    "        loss_g = criterion(d_fake, label_real.detach())\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%250 == 0:\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03dstep_%04d.png' % (outf, epoch, i),\n",
    "                    normalize=True)\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f'\n",
    "                  % (epoch, niter, i, len(dataloader),\n",
    "                     loss_d.data[0], loss_g.data[0], d_real_mean, d_fake_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO LIST FROM GANHACKS\n",
    "# https://github.com/soumith/ganhacks\n",
    "# ✓ Normalize the inputs\n",
    "# ✓ A modified loss function\n",
    "# ✓ Use a spherical Z\n",
    "# ✓ BatchNorm\n",
    "# Aviod sparse gradients\n",
    "# ✓ Use soft and noisy labels\n",
    "# ✓ DCGAN\n",
    "# Use stability tricks from RL\n",
    "# ✓ Use SGD for discriminator ADAM for generator\n",
    "# ✓ Add noise to inputs\n",
    "# Batch Discrimination (for diversity)\n",
    "# ✓ Use dropouts in G in both train and test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network as layer\n",
    "import torch.nn as nn\n",
    "\n",
    "nc = 3\n",
    "nz = 512\n",
    "nfeature = 736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "from Folder import ImageFeatureFolder\n",
    "\n",
    "lr     = 0.0002\n",
    "beta1  = 0.0   \n",
    "beta2  = 0.99     \n",
    "imageSize = 64\n",
    "batchSize = 64\n",
    "\n",
    "outf = \"./celeba_result/\"\n",
    "des_dir = \"./celeba/\"\n",
    "embed_dir = \"./celeba_embed/\"\n",
    "\n",
    "dataset = ImageFeatureFolder(root=des_dir,\n",
    "                            feature_dir=embed_dir,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.CenterCrop(178),\n",
    "                                transforms.Resize(imageSize),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size= batchSize,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers import *\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def make_dense(self, k_in, k_growth, n, options):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(Dense(layer.conv(k_in, k_growth, 3, 1, 1, **options)))\n",
    "            k_in += k_growth\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        options = {'leaky':True, 'bn':True, 'wn':False, 'pixel':True, 'gdrop':True}\n",
    "        \n",
    "        self.deconv1 = layer.deconv(nz, 512, 4, 1, 0, **options)\n",
    "        self.conv_feature = layer.conv(512, 256, 3, 2, 1, leaky=True)\n",
    "        \n",
    "        \n",
    "        layers = []\n",
    "        # cat(deconv1, conv_feature) will be feeded\n",
    "        # 4 x 4\n",
    "        layers.append(layer.deconv(512 + 256, 256, 4, 2, 1, **options))\n",
    "        # 8 x 8\n",
    "        layers.append(layer.deconv(256, 128, 4, 2, 1, **options))\n",
    "        # 16 x 16\n",
    "        layers.append(layer.deconv(128, 64, 4, 2, 1, **options))\n",
    "        # 32 x 32\n",
    "        layers.append(layer.deconv(64, nc, 4, 2, 1, gdrop=options['gdrop'], only=True))\n",
    "        # 64 x 64\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        self.deconv2 = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x, feature):\n",
    "        x = self.deconv1(x)\n",
    "        feature = self.conv_feature(feature.view(-1, 512, 7, 7))\n",
    "        x = torch.cat([x, feature], 1)\n",
    "        x = self.deconv2(x)\n",
    "        return x\n",
    "    \n",
    "netG = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def make_dense(self, k_in, k_growth, n, options):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(Dense(layer.conv(k_in, k_growth, 3, 1, 1, **options)))\n",
    "            k_in += k_growth\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        options = {'leaky':True, 'bn':False, 'wn':False, 'pixel':False, 'gdrop':False}\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # 64 x 64\n",
    "        layers.append(layer.conv(nc, 64, 4, 2, 1, **options))\n",
    "        # 32 x 32\n",
    "        layers.append(layer.conv(64, 128, 4, 2, 1, **options))\n",
    "        # 16 x 16\n",
    "        layers.append(layer.conv(128, 256, 4, 2, 1, **options))\n",
    "        # 8 x 8\n",
    "        layers.append(layer.conv(256, 512, 4, 2, 1, **options))\n",
    "        # 4 x 4\n",
    "        self.conv1 = nn.Sequential(*layers)\n",
    "        self.conv_feature = layer.conv(512, 256, 3, 2, 1, leaky=True)\n",
    "        self.conv2 = layer.conv(512 + 256, 1, 4, 1, 0, **options)\n",
    "        # 1 x 1\n",
    "    \n",
    "    def forward(self, x, feature):\n",
    "        x = self.conv1(x)\n",
    "        feature = self.conv_feature(feature.view(-1, 512, 7, 7))\n",
    "        x = torch.cat([x, feature], 1)\n",
    "        x = self.conv2(x)\n",
    "        return x.view(-1, 1)\n",
    "    \n",
    "netD = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, imageSize,imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "feature = torch.FloatTensor(batchSize, 512, 7, 7)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "fixed_feature = dataset.feature[1]\n",
    "fixed_feature = fixed_feature.repeat(batchSize, 1, 1, 1)\n",
    "\n",
    "label_real = torch.FloatTensor(batchSize)\n",
    "label_real_smooth = torch.FloatTensor(batchSize)\n",
    "label_fake = torch.FloatTensor(batchSize)\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "criterion.cuda()\n",
    "input, feature, noise = input.cuda(), feature.cuda(), noise.cuda()\n",
    "label_real, label_real_smooth, label_fake = label_real.cuda(), label_real_smooth.cuda(), label_fake.cuda()\n",
    "fixed_noise, fixed_feature = fixed_noise.cuda(), fixed_feature.cuda()\n",
    "\n",
    "label_real.resize_(batchSize, 1).fill_(1)\n",
    "label_fake.resize_(batchSize, 1).fill_(0)\n",
    "label_real_smooth.resize_(batchSize, 1).fill_(0.8)\n",
    "label_real = Variable(label_real)\n",
    "label_fake = Variable(label_fake)\n",
    "label_real_smooth = Variable(label_real_smooth)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netD.load_state_dict(torch.load(outf + 'netD_epoch_042.pth'))\n",
    "# netG.load_state_dict(torch.load(outf + 'netG_epoch_042.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise)\n",
    "fixed_feature = Variable(fixed_feature)\n",
    "\n",
    "# setup optimizer\n",
    "# optimizerD = optim.SGD(netD.parameters(), lr = lr, momentum=0.9)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "schedulerD = optim.lr_scheduler.MultiStepLR(optimizerD, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "schedulerG = optim.lr_scheduler.MultiStepLR(optimizerG, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "result_dict = {}\n",
    "loss_D,loss_G,score_D,score_G1,score_G2 = [],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_d_ = None\n",
    "def add_noise(x, d_fake):\n",
    "    global _d_\n",
    "    if _d_ is not None:\n",
    "        _d_ = _d_ * 0.9 + torch.mean(d_fake).data[0] * 0.1\n",
    "        strength = 0.2 * max(0, _d_ - 0.5)**2\n",
    "        z = np.random.randn(*x.size()).astype(np.float32) * strength\n",
    "        z = Variable(torch.from_numpy(z)).cuda()\n",
    "        return x + z\n",
    "    else:\n",
    "        _d_ = 0.0\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][0/3165] Loss_D: 0.5498 Loss_G: 0.1301 D(x): 0.0669 D(G(z)): 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][250/3165] Loss_D: 0.3185 Loss_G: 0.3430 D(x): 0.3739 D(G(z)): 0.3672\n",
      "[0/100][500/3165] Loss_D: 0.3127 Loss_G: 0.4305 D(x): 0.4227 D(G(z)): 0.4098\n",
      "[0/100][750/3165] Loss_D: 0.3132 Loss_G: 0.3371 D(x): 0.3543 D(G(z)): 0.3354\n",
      "[0/100][1000/3165] Loss_D: 0.3102 Loss_G: 0.4232 D(x): 0.3917 D(G(z)): 0.3734\n",
      "[0/100][1250/3165] Loss_D: 0.3024 Loss_G: 0.4495 D(x): 0.4215 D(G(z)): 0.3930\n",
      "[0/100][1500/3165] Loss_D: 0.2974 Loss_G: 0.6385 D(x): 0.5053 D(G(z)): 0.4498\n",
      "[0/100][1750/3165] Loss_D: 0.3989 Loss_G: 0.9594 D(x): 0.5601 D(G(z)): 0.5620\n",
      "[0/100][2000/3165] Loss_D: 0.2647 Loss_G: 0.4333 D(x): 0.4214 D(G(z)): 0.3334\n",
      "[0/100][2250/3165] Loss_D: 0.2875 Loss_G: 0.3899 D(x): 0.3842 D(G(z)): 0.3266\n",
      "[0/100][2500/3165] Loss_D: 0.2444 Loss_G: 0.6588 D(x): 0.5049 D(G(z)): 0.3678\n",
      "[0/100][2750/3165] Loss_D: 0.2418 Loss_G: 0.4910 D(x): 0.4320 D(G(z)): 0.2991\n",
      "[0/100][3000/3165] Loss_D: 0.2944 Loss_G: 0.4998 D(x): 0.4692 D(G(z)): 0.4135\n",
      "[1/100][0/3165] Loss_D: 0.3052 Loss_G: 0.6838 D(x): 0.5933 D(G(z)): 0.4806\n",
      "[1/100][250/3165] Loss_D: 0.2835 Loss_G: 0.6264 D(x): 0.5130 D(G(z)): 0.4293\n",
      "[1/100][500/3165] Loss_D: 0.3931 Loss_G: 0.3317 D(x): 0.2043 D(G(z)): 0.1565\n",
      "[1/100][750/3165] Loss_D: 0.2177 Loss_G: 0.9409 D(x): 0.6179 D(G(z)): 0.3838\n",
      "[1/100][1000/3165] Loss_D: 0.2204 Loss_G: 0.8465 D(x): 0.6275 D(G(z)): 0.3976\n",
      "[1/100][1250/3165] Loss_D: 0.2374 Loss_G: 0.8793 D(x): 0.6383 D(G(z)): 0.4201\n",
      "[1/100][1500/3165] Loss_D: 0.2762 Loss_G: 0.5094 D(x): 0.3859 D(G(z)): 0.2592\n",
      "[1/100][1750/3165] Loss_D: 0.2338 Loss_G: 0.7233 D(x): 0.5671 D(G(z)): 0.3717\n",
      "[1/100][2000/3165] Loss_D: 0.1677 Loss_G: 0.7444 D(x): 0.5653 D(G(z)): 0.2598\n",
      "[1/100][2250/3165] Loss_D: 0.2028 Loss_G: 0.6667 D(x): 0.5313 D(G(z)): 0.3176\n",
      "[1/100][2500/3165] Loss_D: 0.1820 Loss_G: 0.4900 D(x): 0.4500 D(G(z)): 0.1876\n",
      "[1/100][2750/3165] Loss_D: 0.1608 Loss_G: 0.6956 D(x): 0.5272 D(G(z)): 0.2013\n",
      "[1/100][3000/3165] Loss_D: 0.1352 Loss_G: 0.8829 D(x): 0.6150 D(G(z)): 0.2273\n",
      "[2/100][0/3165] Loss_D: 0.0962 Loss_G: 0.9908 D(x): 0.7160 D(G(z)): 0.1573\n",
      "[2/100][250/3165] Loss_D: 0.2013 Loss_G: 0.5392 D(x): 0.4217 D(G(z)): 0.1560\n",
      "[2/100][500/3165] Loss_D: 0.2715 Loss_G: 0.9904 D(x): 0.6951 D(G(z)): 0.4556\n",
      "[2/100][750/3165] Loss_D: 0.1788 Loss_G: 0.3701 D(x): 0.4258 D(G(z)): 0.0802\n",
      "[2/100][1000/3165] Loss_D: 0.1279 Loss_G: 0.6528 D(x): 0.5484 D(G(z)): 0.1286\n",
      "[2/100][1250/3165] Loss_D: 0.2854 Loss_G: 0.4588 D(x): 0.3026 D(G(z)): 0.0950\n",
      "[2/100][1500/3165] Loss_D: 0.4273 Loss_G: 0.1677 D(x): 0.1639 D(G(z)): 0.0198\n",
      "[2/100][1750/3165] Loss_D: 0.1408 Loss_G: 0.7486 D(x): 0.5848 D(G(z)): 0.2018\n",
      "[2/100][2000/3165] Loss_D: 0.1612 Loss_G: 0.7208 D(x): 0.5667 D(G(z)): 0.2257\n",
      "[2/100][2250/3165] Loss_D: 0.1536 Loss_G: 0.6621 D(x): 0.5413 D(G(z)): 0.1917\n",
      "[2/100][2500/3165] Loss_D: 0.2483 Loss_G: 0.9221 D(x): 0.6622 D(G(z)): 0.4026\n",
      "[2/100][2750/3165] Loss_D: 0.0962 Loss_G: 1.0031 D(x): 0.6972 D(G(z)): 0.1752\n",
      "[2/100][3000/3165] Loss_D: 0.2095 Loss_G: 0.4561 D(x): 0.3875 D(G(z)): 0.0897\n",
      "[3/100][0/3165] Loss_D: 0.1477 Loss_G: 0.7928 D(x): 0.5771 D(G(z)): 0.2245\n",
      "[3/100][250/3165] Loss_D: 0.1596 Loss_G: 1.0788 D(x): 0.7275 D(G(z)): 0.3397\n",
      "[3/100][500/3165] Loss_D: 0.2345 Loss_G: 1.0463 D(x): 0.7777 D(G(z)): 0.4271\n",
      "[3/100][750/3165] Loss_D: 0.5002 Loss_G: 0.0526 D(x): 0.1094 D(G(z)): -0.0103\n",
      "[3/100][1000/3165] Loss_D: 0.1474 Loss_G: 0.8377 D(x): 0.6173 D(G(z)): 0.2516\n",
      "[3/100][1250/3165] Loss_D: 0.2150 Loss_G: 0.9400 D(x): 0.5814 D(G(z)): 0.3430\n",
      "[3/100][1500/3165] Loss_D: 0.2167 Loss_G: 0.5204 D(x): 0.3992 D(G(z)): 0.1252\n",
      "[3/100][1750/3165] Loss_D: 0.1701 Loss_G: 0.4264 D(x): 0.4346 D(G(z)): 0.0556\n",
      "[3/100][2000/3165] Loss_D: 0.1660 Loss_G: 0.6904 D(x): 0.5382 D(G(z)): 0.2350\n",
      "[3/100][2250/3165] Loss_D: 0.1829 Loss_G: 0.6015 D(x): 0.4619 D(G(z)): 0.1662\n",
      "[3/100][2500/3165] Loss_D: 0.1503 Loss_G: 0.7420 D(x): 0.5512 D(G(z)): 0.1780\n",
      "[3/100][2750/3165] Loss_D: 0.2477 Loss_G: 0.4140 D(x): 0.3434 D(G(z)): 0.0565\n",
      "[3/100][3000/3165] Loss_D: 0.1688 Loss_G: 1.0352 D(x): 0.7271 D(G(z)): 0.3037\n",
      "[4/100][0/3165] Loss_D: 0.1350 Loss_G: 0.5600 D(x): 0.4959 D(G(z)): 0.0544\n",
      "[4/100][250/3165] Loss_D: 0.1499 Loss_G: 0.6706 D(x): 0.5333 D(G(z)): 0.1494\n",
      "[4/100][500/3165] Loss_D: 0.1951 Loss_G: 0.5566 D(x): 0.4401 D(G(z)): 0.1296\n",
      "[4/100][750/3165] Loss_D: 0.2365 Loss_G: 1.0296 D(x): 0.7035 D(G(z)): 0.4034\n",
      "[4/100][1000/3165] Loss_D: 0.1583 Loss_G: 0.8336 D(x): 0.5748 D(G(z)): 0.2126\n",
      "[4/100][1250/3165] Loss_D: 0.1485 Loss_G: 0.8277 D(x): 0.6342 D(G(z)): 0.2283\n",
      "[4/100][1500/3165] Loss_D: 0.2123 Loss_G: 0.5962 D(x): 0.3896 D(G(z)): 0.0897\n",
      "[4/100][1750/3165] Loss_D: 0.4357 Loss_G: 0.5664 D(x): 0.1688 D(G(z)): -0.0267\n",
      "[4/100][2000/3165] Loss_D: 0.1913 Loss_G: 0.4875 D(x): 0.4424 D(G(z)): 0.1330\n",
      "[4/100][2250/3165] Loss_D: 0.1427 Loss_G: 0.6481 D(x): 0.5204 D(G(z)): 0.1276\n",
      "[4/100][2500/3165] Loss_D: 0.1673 Loss_G: 0.6024 D(x): 0.4802 D(G(z)): 0.1374\n",
      "[4/100][2750/3165] Loss_D: 0.1294 Loss_G: 0.8407 D(x): 0.5962 D(G(z)): 0.1957\n",
      "[4/100][3000/3165] Loss_D: 0.1066 Loss_G: 0.9434 D(x): 0.6560 D(G(z)): 0.1708\n",
      "[5/100][0/3165] Loss_D: 0.1334 Loss_G: 0.7271 D(x): 0.5595 D(G(z)): 0.1830\n",
      "[5/100][250/3165] Loss_D: 0.1250 Loss_G: 1.0079 D(x): 0.7108 D(G(z)): 0.2069\n",
      "[5/100][500/3165] Loss_D: 0.1124 Loss_G: 1.0435 D(x): 0.7242 D(G(z)): 0.2467\n",
      "[5/100][750/3165] Loss_D: 0.2059 Loss_G: 1.0851 D(x): 0.7694 D(G(z)): 0.3701\n",
      "[5/100][1000/3165] Loss_D: 0.1429 Loss_G: 0.5733 D(x): 0.5044 D(G(z)): 0.0936\n",
      "[5/100][1250/3165] Loss_D: 0.2244 Loss_G: 0.5521 D(x): 0.3681 D(G(z)): 0.0351\n",
      "[5/100][1500/3165] Loss_D: 0.2191 Loss_G: 0.4522 D(x): 0.3799 D(G(z)): 0.0218\n",
      "[5/100][1750/3165] Loss_D: 0.1122 Loss_G: 1.0075 D(x): 0.6482 D(G(z)): 0.1750\n",
      "[5/100][2000/3165] Loss_D: 0.1175 Loss_G: 1.0006 D(x): 0.6317 D(G(z)): 0.1925\n",
      "[5/100][2250/3165] Loss_D: 0.1495 Loss_G: 0.6220 D(x): 0.4741 D(G(z)): 0.0978\n",
      "[5/100][2500/3165] Loss_D: 0.1251 Loss_G: 0.7390 D(x): 0.5797 D(G(z)): 0.1278\n",
      "[5/100][2750/3165] Loss_D: 0.1738 Loss_G: 0.5840 D(x): 0.4591 D(G(z)): 0.0748\n",
      "[5/100][3000/3165] Loss_D: 0.1694 Loss_G: 0.5301 D(x): 0.4563 D(G(z)): 0.0530\n",
      "[6/100][0/3165] Loss_D: 0.0682 Loss_G: 1.0185 D(x): 0.7124 D(G(z)): 0.0727\n",
      "[6/100][250/3165] Loss_D: 0.1477 Loss_G: 1.0573 D(x): 0.7503 D(G(z)): 0.2749\n",
      "[6/100][500/3165] Loss_D: 0.0764 Loss_G: 0.9889 D(x): 0.7048 D(G(z)): 0.0429\n",
      "[6/100][750/3165] Loss_D: 0.1378 Loss_G: 0.5108 D(x): 0.5237 D(G(z)): 0.0852\n",
      "[6/100][1000/3165] Loss_D: 0.2287 Loss_G: 0.5000 D(x): 0.3686 D(G(z)): -0.0221\n",
      "[6/100][1250/3165] Loss_D: 0.1010 Loss_G: 0.9359 D(x): 0.6436 D(G(z)): 0.1323\n",
      "[6/100][1500/3165] Loss_D: 0.1926 Loss_G: 1.0632 D(x): 0.7774 D(G(z)): 0.3548\n",
      "[6/100][1750/3165] Loss_D: 0.1968 Loss_G: 0.5741 D(x): 0.3972 D(G(z)): 0.0082\n",
      "[6/100][2000/3165] Loss_D: 0.1482 Loss_G: 0.5159 D(x): 0.4661 D(G(z)): 0.0200\n",
      "[6/100][2250/3165] Loss_D: 0.1037 Loss_G: 1.0959 D(x): 0.7085 D(G(z)): 0.1821\n",
      "[6/100][2500/3165] Loss_D: 0.0797 Loss_G: 1.0627 D(x): 0.7763 D(G(z)): 0.1397\n",
      "[6/100][2750/3165] Loss_D: 0.2387 Loss_G: 0.4976 D(x): 0.3703 D(G(z)): -0.0026\n",
      "[6/100][3000/3165] Loss_D: 0.0930 Loss_G: 1.0666 D(x): 0.7542 D(G(z)): 0.1902\n",
      "[7/100][0/3165] Loss_D: 0.1167 Loss_G: 0.7428 D(x): 0.5572 D(G(z)): 0.1195\n",
      "[7/100][250/3165] Loss_D: 0.1061 Loss_G: 0.7130 D(x): 0.5708 D(G(z)): 0.1025\n",
      "[7/100][500/3165] Loss_D: 0.1138 Loss_G: 1.0925 D(x): 0.7699 D(G(z)): 0.2319\n",
      "[7/100][750/3165] Loss_D: 0.1010 Loss_G: 1.0735 D(x): 0.7916 D(G(z)): 0.1991\n",
      "[7/100][1000/3165] Loss_D: 0.0903 Loss_G: 0.9172 D(x): 0.6484 D(G(z)): 0.1253\n",
      "[7/100][1250/3165] Loss_D: 0.1085 Loss_G: 0.6524 D(x): 0.5495 D(G(z)): 0.0491\n",
      "[7/100][1500/3165] Loss_D: 0.0786 Loss_G: 0.9383 D(x): 0.6726 D(G(z)): 0.0665\n",
      "[7/100][1750/3165] Loss_D: 0.0963 Loss_G: 1.1110 D(x): 0.8090 D(G(z)): 0.1986\n",
      "[7/100][2000/3165] Loss_D: 0.1575 Loss_G: 1.1283 D(x): 0.8521 D(G(z)): 0.2892\n",
      "[7/100][2250/3165] Loss_D: 0.0803 Loss_G: 1.0721 D(x): 0.7582 D(G(z)): 0.1621\n",
      "[7/100][2500/3165] Loss_D: 0.1159 Loss_G: 0.7026 D(x): 0.5463 D(G(z)): 0.0009\n",
      "[7/100][2750/3165] Loss_D: 0.0965 Loss_G: 1.0397 D(x): 0.6792 D(G(z)): 0.1772\n",
      "[7/100][3000/3165] Loss_D: 0.0828 Loss_G: 1.0432 D(x): 0.7190 D(G(z)): 0.1589\n",
      "[8/100][0/3165] Loss_D: 0.1334 Loss_G: 0.6323 D(x): 0.5225 D(G(z)): 0.0921\n",
      "[8/100][250/3165] Loss_D: 0.0595 Loss_G: 1.0272 D(x): 0.7406 D(G(z)): 0.0435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/100][500/3165] Loss_D: 0.0881 Loss_G: 0.7723 D(x): 0.6092 D(G(z)): 0.0778\n",
      "[8/100][750/3165] Loss_D: 0.0718 Loss_G: 0.7811 D(x): 0.6309 D(G(z)): 0.0553\n",
      "[8/100][1000/3165] Loss_D: 0.0592 Loss_G: 0.9680 D(x): 0.6970 D(G(z)): 0.1168\n",
      "[8/100][1250/3165] Loss_D: 0.0835 Loss_G: 0.8367 D(x): 0.6430 D(G(z)): 0.1120\n",
      "[8/100][1500/3165] Loss_D: 0.0850 Loss_G: 1.0326 D(x): 0.7398 D(G(z)): 0.1418\n",
      "[8/100][1750/3165] Loss_D: 0.0494 Loss_G: 1.0452 D(x): 0.7617 D(G(z)): 0.0687\n",
      "[8/100][2000/3165] Loss_D: 0.1482 Loss_G: 0.6538 D(x): 0.4860 D(G(z)): 0.0115\n",
      "[8/100][2250/3165] Loss_D: 0.0767 Loss_G: 0.8839 D(x): 0.6479 D(G(z)): 0.1000\n",
      "[8/100][2500/3165] Loss_D: 0.0678 Loss_G: 1.0789 D(x): 0.8181 D(G(z)): 0.1301\n",
      "[8/100][2750/3165] Loss_D: 0.1483 Loss_G: 0.5067 D(x): 0.4845 D(G(z)): 0.0229\n",
      "[8/100][3000/3165] Loss_D: 0.1026 Loss_G: 1.0574 D(x): 0.7094 D(G(z)): 0.2130\n",
      "[9/100][0/3165] Loss_D: 0.1093 Loss_G: 0.5446 D(x): 0.5388 D(G(z)): 0.0215\n",
      "[9/100][250/3165] Loss_D: 0.1274 Loss_G: 1.0513 D(x): 0.7394 D(G(z)): 0.2461\n",
      "[9/100][500/3165] Loss_D: 0.0717 Loss_G: 1.1061 D(x): 0.8340 D(G(z)): 0.1255\n",
      "[9/100][750/3165] Loss_D: 0.0566 Loss_G: 1.0510 D(x): 0.7555 D(G(z)): 0.0546\n",
      "[9/100][1000/3165] Loss_D: 0.0856 Loss_G: 0.6358 D(x): 0.6199 D(G(z)): 0.0251\n",
      "[9/100][1250/3165] Loss_D: 0.0728 Loss_G: 0.8893 D(x): 0.6532 D(G(z)): 0.0410\n",
      "[9/100][1500/3165] Loss_D: 0.0715 Loss_G: 1.0877 D(x): 0.7693 D(G(z)): 0.1327\n",
      "[9/100][1750/3165] Loss_D: 0.0777 Loss_G: 1.0514 D(x): 0.7005 D(G(z)): 0.1496\n",
      "[9/100][2000/3165] Loss_D: 0.0854 Loss_G: 1.0073 D(x): 0.6764 D(G(z)): 0.1148\n",
      "[9/100][2250/3165] Loss_D: 0.0771 Loss_G: 0.8860 D(x): 0.6600 D(G(z)): 0.0830\n",
      "[9/100][2500/3165] Loss_D: 0.0816 Loss_G: 1.0648 D(x): 0.7668 D(G(z)): 0.1677\n",
      "[9/100][2750/3165] Loss_D: 0.0845 Loss_G: 0.8010 D(x): 0.6287 D(G(z)): 0.0695\n",
      "[9/100][3000/3165] Loss_D: 0.1213 Loss_G: 1.1070 D(x): 0.8147 D(G(z)): 0.2381\n",
      "[10/100][0/3165] Loss_D: 0.1083 Loss_G: 1.1103 D(x): 0.7893 D(G(z)): 0.2123\n",
      "[10/100][250/3165] Loss_D: 0.0582 Loss_G: 0.8472 D(x): 0.6675 D(G(z)): 0.0587\n",
      "[10/100][500/3165] Loss_D: 0.0684 Loss_G: 0.9738 D(x): 0.7079 D(G(z)): 0.0680\n",
      "[10/100][750/3165] Loss_D: 0.0661 Loss_G: 1.1421 D(x): 0.8531 D(G(z)): 0.1132\n",
      "[10/100][1000/3165] Loss_D: 0.0958 Loss_G: 1.1075 D(x): 0.8044 D(G(z)): 0.2142\n",
      "[10/100][1250/3165] Loss_D: 0.0521 Loss_G: 0.9595 D(x): 0.7017 D(G(z)): 0.0198\n",
      "[10/100][1500/3165] Loss_D: 0.1376 Loss_G: 0.6595 D(x): 0.4932 D(G(z)): 0.0178\n",
      "[10/100][1750/3165] Loss_D: 0.0662 Loss_G: 1.1205 D(x): 0.7924 D(G(z)): 0.1258\n",
      "[10/100][2000/3165] Loss_D: 0.0660 Loss_G: 0.9072 D(x): 0.6712 D(G(z)): 0.0735\n",
      "[10/100][2250/3165] Loss_D: 0.0691 Loss_G: 1.0472 D(x): 0.7605 D(G(z)): 0.1237\n",
      "[10/100][2500/3165] Loss_D: 0.1260 Loss_G: 0.8249 D(x): 0.5250 D(G(z)): 0.0567\n",
      "[10/100][2750/3165] Loss_D: 0.1664 Loss_G: 0.4549 D(x): 0.4566 D(G(z)): -0.0167\n",
      "[10/100][3000/3165] Loss_D: 0.0921 Loss_G: 0.6099 D(x): 0.5906 D(G(z)): -0.0034\n",
      "[11/100][0/3165] Loss_D: 0.0665 Loss_G: 0.9164 D(x): 0.6670 D(G(z)): 0.1022\n",
      "[11/100][250/3165] Loss_D: 0.0761 Loss_G: 0.9778 D(x): 0.7126 D(G(z)): 0.1182\n",
      "[11/100][500/3165] Loss_D: 0.0486 Loss_G: 1.0458 D(x): 0.7601 D(G(z)): 0.0629\n",
      "[11/100][750/3165] Loss_D: 0.0501 Loss_G: 1.0154 D(x): 0.7227 D(G(z)): 0.0475\n",
      "[11/100][1000/3165] Loss_D: 0.0823 Loss_G: 0.9812 D(x): 0.6792 D(G(z)): 0.0943\n",
      "[11/100][1250/3165] Loss_D: 0.0458 Loss_G: 1.0579 D(x): 0.7938 D(G(z)): 0.0569\n",
      "[11/100][1500/3165] Loss_D: 0.0460 Loss_G: 1.0385 D(x): 0.7188 D(G(z)): 0.0548\n",
      "[11/100][1750/3165] Loss_D: 0.0666 Loss_G: 1.0568 D(x): 0.6976 D(G(z)): 0.1121\n",
      "[11/100][2000/3165] Loss_D: 0.0615 Loss_G: 1.0588 D(x): 0.7672 D(G(z)): 0.0820\n",
      "[11/100][2250/3165] Loss_D: 0.0593 Loss_G: 1.0689 D(x): 0.7677 D(G(z)): 0.0838\n",
      "[11/100][2500/3165] Loss_D: 0.0696 Loss_G: 1.1421 D(x): 0.8311 D(G(z)): 0.1458\n",
      "[11/100][2750/3165] Loss_D: 0.0531 Loss_G: 0.9235 D(x): 0.7002 D(G(z)): 0.0582\n",
      "[11/100][3000/3165] Loss_D: 0.0496 Loss_G: 1.0443 D(x): 0.7518 D(G(z)): 0.0622\n",
      "[12/100][0/3165] Loss_D: 0.0520 Loss_G: 1.0380 D(x): 0.7753 D(G(z)): 0.0916\n",
      "[12/100][250/3165] Loss_D: 0.0775 Loss_G: 1.0933 D(x): 0.8360 D(G(z)): 0.1507\n",
      "[12/100][500/3165] Loss_D: 0.0530 Loss_G: 0.7874 D(x): 0.6696 D(G(z)): -0.0060\n",
      "[12/100][750/3165] Loss_D: 0.0547 Loss_G: 1.1617 D(x): 0.8094 D(G(z)): 0.0947\n",
      "[12/100][1000/3165] Loss_D: 0.0602 Loss_G: 0.8300 D(x): 0.6685 D(G(z)): 0.0182\n",
      "[12/100][1250/3165] Loss_D: 0.0526 Loss_G: 1.0649 D(x): 0.8006 D(G(z)): 0.0575\n",
      "[12/100][1500/3165] Loss_D: 0.0408 Loss_G: 0.9914 D(x): 0.7436 D(G(z)): 0.0100\n",
      "[12/100][1750/3165] Loss_D: 0.0893 Loss_G: 1.0871 D(x): 0.7427 D(G(z)): 0.1365\n",
      "[12/100][2000/3165] Loss_D: 0.0761 Loss_G: 1.1144 D(x): 0.7877 D(G(z)): 0.1363\n",
      "[12/100][2250/3165] Loss_D: 0.0515 Loss_G: 1.0216 D(x): 0.7138 D(G(z)): 0.0523\n",
      "[12/100][2500/3165] Loss_D: 0.0583 Loss_G: 0.8479 D(x): 0.6597 D(G(z)): 0.0455\n",
      "[12/100][2750/3165] Loss_D: 0.0726 Loss_G: 0.9792 D(x): 0.7318 D(G(z)): 0.0789\n",
      "[12/100][3000/3165] Loss_D: 0.0477 Loss_G: 1.0762 D(x): 0.7925 D(G(z)): 0.0739\n",
      "[13/100][0/3165] Loss_D: 0.0517 Loss_G: 1.0558 D(x): 0.7323 D(G(z)): 0.0916\n",
      "[13/100][250/3165] Loss_D: 0.0519 Loss_G: 1.0352 D(x): 0.7274 D(G(z)): 0.0730\n",
      "[13/100][500/3165] Loss_D: 0.0481 Loss_G: 1.0989 D(x): 0.8380 D(G(z)): 0.0592\n",
      "[13/100][750/3165] Loss_D: 0.0500 Loss_G: 1.1442 D(x): 0.8489 D(G(z)): 0.1009\n",
      "[13/100][1000/3165] Loss_D: 0.0528 Loss_G: 1.1130 D(x): 0.8198 D(G(z)): 0.1034\n",
      "[13/100][1250/3165] Loss_D: 0.0591 Loss_G: 0.9556 D(x): 0.7033 D(G(z)): 0.0500\n",
      "[13/100][1500/3165] Loss_D: 0.0709 Loss_G: 1.1125 D(x): 0.7931 D(G(z)): 0.1451\n",
      "[13/100][1750/3165] Loss_D: 0.0712 Loss_G: 0.7376 D(x): 0.6492 D(G(z)): 0.0087\n",
      "[13/100][2000/3165] Loss_D: 0.0821 Loss_G: 0.9796 D(x): 0.6668 D(G(z)): 0.1004\n",
      "[13/100][2250/3165] Loss_D: 0.1854 Loss_G: 0.6084 D(x): 0.4232 D(G(z)): -0.0231\n",
      "[13/100][2500/3165] Loss_D: 0.1284 Loss_G: 0.5859 D(x): 0.5131 D(G(z)): -0.0118\n",
      "[13/100][2750/3165] Loss_D: 0.0629 Loss_G: 1.1520 D(x): 0.8323 D(G(z)): 0.1285\n",
      "[13/100][3000/3165] Loss_D: 0.1133 Loss_G: 0.5952 D(x): 0.5472 D(G(z)): 0.0058\n",
      "[14/100][0/3165] Loss_D: 0.0493 Loss_G: 1.0447 D(x): 0.6961 D(G(z)): 0.0656\n",
      "[14/100][250/3165] Loss_D: 0.0571 Loss_G: 1.0206 D(x): 0.6982 D(G(z)): 0.0825\n",
      "[14/100][500/3165] Loss_D: 0.0550 Loss_G: 0.9321 D(x): 0.6802 D(G(z)): 0.0418\n",
      "[14/100][750/3165] Loss_D: 0.0466 Loss_G: 1.0607 D(x): 0.7516 D(G(z)): 0.0370\n",
      "[14/100][1000/3165] Loss_D: 0.0480 Loss_G: 0.9583 D(x): 0.7116 D(G(z)): 0.0225\n",
      "[14/100][1250/3165] Loss_D: 0.0708 Loss_G: 0.9807 D(x): 0.6945 D(G(z)): 0.1135\n",
      "[14/100][1500/3165] Loss_D: 0.0355 Loss_G: 1.0406 D(x): 0.7665 D(G(z)): 0.0309\n",
      "[14/100][1750/3165] Loss_D: 0.0917 Loss_G: 0.8431 D(x): 0.5662 D(G(z)): -0.0032\n",
      "[14/100][2000/3165] Loss_D: 0.0582 Loss_G: 0.9192 D(x): 0.6682 D(G(z)): 0.0475\n",
      "[14/100][2250/3165] Loss_D: 0.0665 Loss_G: 0.8632 D(x): 0.6160 D(G(z)): 0.0107\n",
      "[14/100][2500/3165] Loss_D: 0.0676 Loss_G: 0.7526 D(x): 0.6547 D(G(z)): 0.0096\n",
      "[14/100][2750/3165] Loss_D: 0.0451 Loss_G: 1.1248 D(x): 0.8163 D(G(z)): 0.0764\n",
      "[14/100][3000/3165] Loss_D: 0.1265 Loss_G: 0.9187 D(x): 0.6262 D(G(z)): 0.1082\n",
      "[15/100][0/3165] Loss_D: 0.0605 Loss_G: 0.8818 D(x): 0.6362 D(G(z)): 0.0229\n",
      "[15/100][250/3165] Loss_D: 0.0466 Loss_G: 1.1006 D(x): 0.8046 D(G(z)): 0.0824\n",
      "[15/100][500/3165] Loss_D: 0.0512 Loss_G: 0.8891 D(x): 0.6755 D(G(z)): 0.0297\n",
      "[15/100][750/3165] Loss_D: 0.0567 Loss_G: 0.8985 D(x): 0.6836 D(G(z)): 0.0500\n",
      "[15/100][1000/3165] Loss_D: 0.0417 Loss_G: 1.1260 D(x): 0.8406 D(G(z)): 0.0779\n",
      "[15/100][1250/3165] Loss_D: 0.0434 Loss_G: 1.1190 D(x): 0.8310 D(G(z)): 0.0744\n",
      "[15/100][1500/3165] Loss_D: 0.0342 Loss_G: 1.0467 D(x): 0.7453 D(G(z)): 0.0320\n",
      "[15/100][1750/3165] Loss_D: 0.0571 Loss_G: 0.9462 D(x): 0.6923 D(G(z)): 0.0587\n",
      "[15/100][2000/3165] Loss_D: 0.0546 Loss_G: 0.9781 D(x): 0.7107 D(G(z)): 0.0614\n",
      "[15/100][2250/3165] Loss_D: 0.0517 Loss_G: 1.0385 D(x): 0.7439 D(G(z)): 0.0828\n",
      "[15/100][2500/3165] Loss_D: 0.0396 Loss_G: 0.9904 D(x): 0.7269 D(G(z)): 0.0106\n",
      "[15/100][2750/3165] Loss_D: 0.0696 Loss_G: 0.8026 D(x): 0.6379 D(G(z)): 0.0279\n",
      "[15/100][3000/3165] Loss_D: 0.0377 Loss_G: 1.0621 D(x): 0.7586 D(G(z)): 0.0443\n",
      "[16/100][0/3165] Loss_D: 0.0611 Loss_G: 1.0366 D(x): 0.7543 D(G(z)): 0.0788\n",
      "[16/100][250/3165] Loss_D: 0.0397 Loss_G: 0.9178 D(x): 0.6988 D(G(z)): 0.0232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/100][500/3165] Loss_D: 0.0681 Loss_G: 1.1524 D(x): 0.8289 D(G(z)): 0.1409\n",
      "[16/100][750/3165] Loss_D: 0.0737 Loss_G: 0.8053 D(x): 0.6116 D(G(z)): 0.0085\n",
      "[16/100][1000/3165] Loss_D: 0.0534 Loss_G: 0.9350 D(x): 0.6638 D(G(z)): 0.0603\n",
      "[16/100][1250/3165] Loss_D: 0.0713 Loss_G: 1.0642 D(x): 0.7275 D(G(z)): 0.1095\n",
      "[16/100][1500/3165] Loss_D: 0.0538 Loss_G: 0.8708 D(x): 0.6643 D(G(z)): 0.0224\n",
      "[16/100][1750/3165] Loss_D: 0.0497 Loss_G: 0.9943 D(x): 0.7095 D(G(z)): 0.0315\n",
      "[16/100][2000/3165] Loss_D: 0.0447 Loss_G: 1.0390 D(x): 0.7271 D(G(z)): 0.0307\n",
      "[16/100][2250/3165] Loss_D: 0.0489 Loss_G: 1.0873 D(x): 0.7798 D(G(z)): 0.0929\n",
      "[16/100][2500/3165] Loss_D: 0.0527 Loss_G: 0.9250 D(x): 0.6865 D(G(z)): 0.0497\n",
      "[16/100][2750/3165] Loss_D: 0.0490 Loss_G: 1.0642 D(x): 0.7782 D(G(z)): 0.0690\n",
      "[16/100][3000/3165] Loss_D: 0.0720 Loss_G: 0.6075 D(x): 0.5792 D(G(z)): -0.0101\n",
      "[17/100][0/3165] Loss_D: 0.0384 Loss_G: 1.0864 D(x): 0.8010 D(G(z)): 0.0671\n",
      "[17/100][250/3165] Loss_D: 0.0258 Loss_G: 1.0658 D(x): 0.7894 D(G(z)): -0.0050\n",
      "[17/100][500/3165] Loss_D: 0.0545 Loss_G: 0.9773 D(x): 0.7164 D(G(z)): 0.0576\n",
      "[17/100][750/3165] Loss_D: 0.0597 Loss_G: 1.0523 D(x): 0.7290 D(G(z)): 0.0856\n",
      "[17/100][1000/3165] Loss_D: 0.0600 Loss_G: 1.1536 D(x): 0.8356 D(G(z)): 0.1159\n",
      "[17/100][1250/3165] Loss_D: 0.0421 Loss_G: 1.0503 D(x): 0.7521 D(G(z)): 0.0331\n",
      "[17/100][1500/3165] Loss_D: 0.0501 Loss_G: 1.0516 D(x): 0.7546 D(G(z)): 0.0613\n",
      "[17/100][1750/3165] Loss_D: 0.0372 Loss_G: 1.0863 D(x): 0.7940 D(G(z)): -0.0187\n",
      "[17/100][2000/3165] Loss_D: 0.0555 Loss_G: 1.0901 D(x): 0.7505 D(G(z)): 0.0806\n",
      "[17/100][2250/3165] Loss_D: 0.0306 Loss_G: 1.0676 D(x): 0.7631 D(G(z)): 0.0338\n",
      "[17/100][2500/3165] Loss_D: 0.0485 Loss_G: 1.1140 D(x): 0.8034 D(G(z)): 0.0813\n",
      "[17/100][2750/3165] Loss_D: 0.0304 Loss_G: 1.1057 D(x): 0.8272 D(G(z)): 0.0267\n",
      "[17/100][3000/3165] Loss_D: 0.0258 Loss_G: 1.0132 D(x): 0.7586 D(G(z)): 0.0394\n",
      "[18/100][0/3165] Loss_D: 0.0414 Loss_G: 0.9589 D(x): 0.7102 D(G(z)): 0.0265\n",
      "[18/100][250/3165] Loss_D: 0.0283 Loss_G: 0.9659 D(x): 0.7634 D(G(z)): 0.0155\n",
      "[18/100][500/3165] Loss_D: 0.0691 Loss_G: 0.8164 D(x): 0.5886 D(G(z)): -0.0332\n",
      "[18/100][750/3165] Loss_D: 0.0299 Loss_G: 1.0794 D(x): 0.7990 D(G(z)): 0.0343\n",
      "[18/100][1000/3165] Loss_D: 0.0567 Loss_G: 0.7994 D(x): 0.6353 D(G(z)): -0.0068\n",
      "[18/100][1250/3165] Loss_D: 0.0384 Loss_G: 0.9899 D(x): 0.7404 D(G(z)): 0.0169\n",
      "[18/100][1500/3165] Loss_D: 0.1154 Loss_G: 1.1621 D(x): 0.8835 D(G(z)): 0.2312\n",
      "[18/100][1750/3165] Loss_D: 0.0696 Loss_G: 1.1198 D(x): 0.8504 D(G(z)): 0.1167\n",
      "[18/100][2000/3165] Loss_D: 0.0416 Loss_G: 1.0706 D(x): 0.7813 D(G(z)): 0.0432\n",
      "[18/100][2250/3165] Loss_D: 0.1305 Loss_G: 0.6061 D(x): 0.4815 D(G(z)): 0.0110\n",
      "[18/100][2500/3165] Loss_D: 0.0299 Loss_G: 0.9007 D(x): 0.7105 D(G(z)): -0.0092\n",
      "[18/100][2750/3165] Loss_D: 0.0414 Loss_G: 0.9591 D(x): 0.7068 D(G(z)): 0.0364\n",
      "[18/100][3000/3165] Loss_D: 0.0298 Loss_G: 1.0300 D(x): 0.7885 D(G(z)): 0.0133\n",
      "[19/100][0/3165] Loss_D: 0.0366 Loss_G: 1.0175 D(x): 0.7069 D(G(z)): 0.0087\n",
      "[19/100][250/3165] Loss_D: 0.0366 Loss_G: 1.0843 D(x): 0.7762 D(G(z)): 0.0714\n",
      "[19/100][500/3165] Loss_D: 0.0552 Loss_G: 1.1380 D(x): 0.8249 D(G(z)): 0.1118\n",
      "[19/100][750/3165] Loss_D: 0.0465 Loss_G: 0.7188 D(x): 0.6454 D(G(z)): -0.0014\n",
      "[19/100][1000/3165] Loss_D: 0.0532 Loss_G: 0.8455 D(x): 0.6344 D(G(z)): -0.0167\n",
      "[19/100][1250/3165] Loss_D: 0.0255 Loss_G: 1.1043 D(x): 0.8218 D(G(z)): -0.0017\n",
      "[19/100][1500/3165] Loss_D: 0.0476 Loss_G: 1.0043 D(x): 0.7388 D(G(z)): 0.0460\n",
      "[19/100][1750/3165] Loss_D: 0.0223 Loss_G: 1.0407 D(x): 0.7648 D(G(z)): 0.0026\n",
      "[19/100][2000/3165] Loss_D: 0.0283 Loss_G: 1.0388 D(x): 0.7486 D(G(z)): 0.0223\n",
      "[19/100][2250/3165] Loss_D: 0.0466 Loss_G: 0.9677 D(x): 0.6901 D(G(z)): 0.0361\n",
      "[19/100][2500/3165] Loss_D: 0.0584 Loss_G: 0.9838 D(x): 0.6565 D(G(z)): 0.0378\n",
      "[19/100][2750/3165] Loss_D: 0.0427 Loss_G: 1.0504 D(x): 0.7522 D(G(z)): 0.0629\n",
      "[19/100][3000/3165] Loss_D: 0.0608 Loss_G: 0.8748 D(x): 0.6516 D(G(z)): 0.0345\n",
      "[20/100][0/3165] Loss_D: 0.0716 Loss_G: 0.7821 D(x): 0.6064 D(G(z)): -0.0049\n",
      "[20/100][250/3165] Loss_D: 0.0370 Loss_G: 0.9103 D(x): 0.6987 D(G(z)): 0.0077\n",
      "[20/100][500/3165] Loss_D: 0.0423 Loss_G: 0.7322 D(x): 0.6574 D(G(z)): 0.0031\n",
      "[20/100][750/3165] Loss_D: 0.0395 Loss_G: 1.1171 D(x): 0.8230 D(G(z)): 0.0683\n",
      "[20/100][1000/3165] Loss_D: 0.0437 Loss_G: 1.0783 D(x): 0.7641 D(G(z)): 0.0858\n",
      "[20/100][1250/3165] Loss_D: 0.0504 Loss_G: 1.0020 D(x): 0.7242 D(G(z)): 0.0544\n",
      "[20/100][1500/3165] Loss_D: 0.1104 Loss_G: 0.5999 D(x): 0.5310 D(G(z)): -0.0299\n",
      "[20/100][1750/3165] Loss_D: 0.0310 Loss_G: 1.0581 D(x): 0.7646 D(G(z)): 0.0393\n",
      "[20/100][2000/3165] Loss_D: 0.0644 Loss_G: 0.7721 D(x): 0.6105 D(G(z)): -0.0372\n",
      "[20/100][2250/3165] Loss_D: 0.0258 Loss_G: 1.1028 D(x): 0.8435 D(G(z)): -0.0045\n",
      "[20/100][2500/3165] Loss_D: 0.0529 Loss_G: 1.0248 D(x): 0.7438 D(G(z)): 0.0837\n",
      "[20/100][2750/3165] Loss_D: 0.0390 Loss_G: 0.8590 D(x): 0.6705 D(G(z)): -0.0023\n",
      "[20/100][3000/3165] Loss_D: 0.0343 Loss_G: 1.0571 D(x): 0.7631 D(G(z)): 0.0368\n",
      "[21/100][0/3165] Loss_D: 0.0388 Loss_G: 1.0411 D(x): 0.7662 D(G(z)): 0.0447\n",
      "[21/100][250/3165] Loss_D: 0.0393 Loss_G: 0.8366 D(x): 0.6698 D(G(z)): 0.0042\n",
      "[21/100][500/3165] Loss_D: 0.0510 Loss_G: 1.0123 D(x): 0.6916 D(G(z)): 0.0658\n",
      "[21/100][750/3165] Loss_D: 0.0549 Loss_G: 1.1636 D(x): 0.9093 D(G(z)): 0.0284\n",
      "[21/100][1000/3165] Loss_D: 0.0518 Loss_G: 1.1996 D(x): 0.8357 D(G(z)): 0.1207\n",
      "[21/100][1250/3165] Loss_D: 0.0423 Loss_G: 1.0134 D(x): 0.7287 D(G(z)): 0.0332\n",
      "[21/100][1500/3165] Loss_D: 0.0314 Loss_G: 1.0573 D(x): 0.7841 D(G(z)): 0.0062\n",
      "[21/100][1750/3165] Loss_D: 0.0391 Loss_G: 0.8381 D(x): 0.6976 D(G(z)): -0.0034\n",
      "[21/100][2000/3165] Loss_D: 0.0323 Loss_G: 0.9635 D(x): 0.6723 D(G(z)): -0.0181\n",
      "[21/100][2250/3165] Loss_D: 0.0289 Loss_G: 1.0070 D(x): 0.7560 D(G(z)): -0.0160\n",
      "[21/100][2500/3165] Loss_D: 0.0306 Loss_G: 1.1127 D(x): 0.8203 D(G(z)): 0.0290\n",
      "[21/100][2750/3165] Loss_D: 0.0385 Loss_G: 0.9951 D(x): 0.7569 D(G(z)): 0.0151\n",
      "[21/100][3000/3165] Loss_D: 0.1579 Loss_G: 1.1736 D(x): 0.7976 D(G(z)): 0.3055\n",
      "[22/100][0/3165] Loss_D: 0.0355 Loss_G: 1.0795 D(x): 0.8113 D(G(z)): 0.0432\n",
      "[22/100][250/3165] Loss_D: 0.0438 Loss_G: 1.0266 D(x): 0.7248 D(G(z)): 0.0511\n",
      "[22/100][500/3165] Loss_D: 0.0256 Loss_G: 1.1251 D(x): 0.8572 D(G(z)): -0.0089\n",
      "[22/100][750/3165] Loss_D: 0.0444 Loss_G: 1.1279 D(x): 0.8407 D(G(z)): 0.0837\n",
      "[22/100][1000/3165] Loss_D: 0.0476 Loss_G: 0.8830 D(x): 0.6725 D(G(z)): 0.0228\n",
      "[22/100][1250/3165] Loss_D: 0.0303 Loss_G: 1.0494 D(x): 0.7483 D(G(z)): 0.0382\n",
      "[22/100][1500/3165] Loss_D: 0.0365 Loss_G: 1.0350 D(x): 0.7446 D(G(z)): 0.0225\n",
      "[22/100][1750/3165] Loss_D: 0.0329 Loss_G: 0.8459 D(x): 0.6928 D(G(z)): -0.0145\n",
      "[22/100][2000/3165] Loss_D: 0.0370 Loss_G: 1.0615 D(x): 0.7570 D(G(z)): 0.0547\n",
      "[22/100][2250/3165] Loss_D: 0.0509 Loss_G: 0.7687 D(x): 0.6721 D(G(z)): 0.0105\n",
      "[22/100][2500/3165] Loss_D: 0.0404 Loss_G: 1.1283 D(x): 0.7942 D(G(z)): 0.0423\n",
      "[22/100][2750/3165] Loss_D: 0.1657 Loss_G: 0.3306 D(x): 0.4631 D(G(z)): -0.0707\n",
      "[22/100][3000/3165] Loss_D: 0.0394 Loss_G: 1.0608 D(x): 0.7525 D(G(z)): 0.0719\n",
      "[23/100][0/3165] Loss_D: 0.0310 Loss_G: 1.0406 D(x): 0.7609 D(G(z)): 0.0220\n",
      "[23/100][250/3165] Loss_D: 0.0269 Loss_G: 1.0803 D(x): 0.7935 D(G(z)): 0.0222\n",
      "[23/100][500/3165] Loss_D: 0.1140 Loss_G: 0.6470 D(x): 0.5154 D(G(z)): -0.0418\n",
      "[23/100][750/3165] Loss_D: 0.0333 Loss_G: 1.0169 D(x): 0.7354 D(G(z)): 0.0338\n",
      "[23/100][1000/3165] Loss_D: 0.0445 Loss_G: 1.1171 D(x): 0.7731 D(G(z)): 0.0773\n",
      "[23/100][1250/3165] Loss_D: 0.0322 Loss_G: 1.0967 D(x): 0.7647 D(G(z)): 0.0234\n",
      "[23/100][1500/3165] Loss_D: 0.0348 Loss_G: 1.0836 D(x): 0.7659 D(G(z)): 0.0517\n",
      "[23/100][1750/3165] Loss_D: 0.0419 Loss_G: 1.0563 D(x): 0.7575 D(G(z)): 0.0586\n",
      "[23/100][2000/3165] Loss_D: 0.0358 Loss_G: 0.8777 D(x): 0.7040 D(G(z)): -0.0089\n",
      "[23/100][2250/3165] Loss_D: 0.0354 Loss_G: 1.1401 D(x): 0.8514 D(G(z)): 0.0311\n",
      "[23/100][2500/3165] Loss_D: 0.0386 Loss_G: 1.0118 D(x): 0.7490 D(G(z)): 0.0205\n",
      "[23/100][2750/3165] Loss_D: 0.0232 Loss_G: 1.0882 D(x): 0.7880 D(G(z)): 0.0422\n",
      "[23/100][3000/3165] Loss_D: 0.0629 Loss_G: 1.0330 D(x): 0.7161 D(G(z)): 0.0805\n",
      "[24/100][0/3165] Loss_D: 0.0359 Loss_G: 1.0211 D(x): 0.7417 D(G(z)): 0.0266\n",
      "[24/100][250/3165] Loss_D: 0.0333 Loss_G: 1.1326 D(x): 0.8444 D(G(z)): 0.0654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/100][500/3165] Loss_D: 0.0261 Loss_G: 1.0676 D(x): 0.7487 D(G(z)): 0.0324\n",
      "[24/100][750/3165] Loss_D: 0.0336 Loss_G: 1.0008 D(x): 0.7452 D(G(z)): 0.0305\n",
      "[24/100][1000/3165] Loss_D: 0.0403 Loss_G: 1.0471 D(x): 0.7298 D(G(z)): 0.0516\n",
      "[24/100][1250/3165] Loss_D: 0.0340 Loss_G: 1.0662 D(x): 0.7570 D(G(z)): 0.0520\n",
      "[24/100][1500/3165] Loss_D: 0.0327 Loss_G: 1.0803 D(x): 0.7851 D(G(z)): 0.0200\n",
      "[24/100][1750/3165] Loss_D: 0.0476 Loss_G: 0.9699 D(x): 0.7056 D(G(z)): 0.0350\n",
      "[24/100][2000/3165] Loss_D: 0.0833 Loss_G: 1.1536 D(x): 0.8585 D(G(z)): 0.1921\n",
      "[24/100][2250/3165] Loss_D: 0.0427 Loss_G: 0.9410 D(x): 0.6889 D(G(z)): 0.0402\n",
      "[24/100][2500/3165] Loss_D: 0.0429 Loss_G: 1.0031 D(x): 0.7087 D(G(z)): 0.0392\n",
      "[24/100][2750/3165] Loss_D: 0.0375 Loss_G: 1.0227 D(x): 0.7389 D(G(z)): 0.0334\n",
      "[24/100][3000/3165] Loss_D: 0.0490 Loss_G: 1.1718 D(x): 0.8374 D(G(z)): 0.1006\n",
      "[25/100][0/3165] Loss_D: 0.0634 Loss_G: 1.1635 D(x): 0.8274 D(G(z)): 0.1449\n",
      "[25/100][250/3165] Loss_D: 0.0416 Loss_G: 0.8545 D(x): 0.6836 D(G(z)): -0.0054\n",
      "[25/100][500/3165] Loss_D: 0.0629 Loss_G: 0.7722 D(x): 0.6235 D(G(z)): -0.0205\n",
      "[25/100][750/3165] Loss_D: 0.0291 Loss_G: 1.0716 D(x): 0.7590 D(G(z)): 0.0273\n",
      "[25/100][1000/3165] Loss_D: 0.0261 Loss_G: 1.0396 D(x): 0.7541 D(G(z)): -0.0078\n",
      "[25/100][1250/3165] Loss_D: 0.0343 Loss_G: 1.0692 D(x): 0.7752 D(G(z)): 0.0260\n",
      "[25/100][1500/3165] Loss_D: 0.0309 Loss_G: 1.0918 D(x): 0.8045 D(G(z)): 0.0276\n",
      "[25/100][1750/3165] Loss_D: 0.0383 Loss_G: 1.0879 D(x): 0.7642 D(G(z)): 0.0633\n",
      "[25/100][2000/3165] Loss_D: 0.0245 Loss_G: 1.1084 D(x): 0.8001 D(G(z)): -0.0488\n",
      "[25/100][2250/3165] Loss_D: 0.0248 Loss_G: 1.0670 D(x): 0.7949 D(G(z)): 0.0200\n",
      "[25/100][2500/3165] Loss_D: 0.0285 Loss_G: 1.0845 D(x): 0.8113 D(G(z)): -0.0092\n",
      "[25/100][2750/3165] Loss_D: 0.0483 Loss_G: 1.1548 D(x): 0.9127 D(G(z)): 0.0733\n",
      "[25/100][3000/3165] Loss_D: 0.0309 Loss_G: 1.1180 D(x): 0.8126 D(G(z)): 0.0824\n",
      "[26/100][0/3165] Loss_D: 0.0231 Loss_G: 1.1101 D(x): 0.7748 D(G(z)): 0.0168\n",
      "[26/100][250/3165] Loss_D: 0.0677 Loss_G: 0.7097 D(x): 0.5890 D(G(z)): -0.0499\n",
      "[26/100][500/3165] Loss_D: 0.0375 Loss_G: 1.1245 D(x): 0.8144 D(G(z)): 0.0767\n",
      "[26/100][750/3165] Loss_D: 0.0342 Loss_G: 1.0517 D(x): 0.7757 D(G(z)): 0.0154\n",
      "[26/100][1000/3165] Loss_D: 0.0334 Loss_G: 0.9444 D(x): 0.6951 D(G(z)): -0.0257\n",
      "[26/100][1250/3165] Loss_D: 0.0522 Loss_G: 1.0418 D(x): 0.7493 D(G(z)): 0.1037\n",
      "[26/100][1500/3165] Loss_D: 0.0236 Loss_G: 1.0529 D(x): 0.7729 D(G(z)): -0.0008\n",
      "[26/100][1750/3165] Loss_D: 0.0212 Loss_G: 1.0426 D(x): 0.7905 D(G(z)): -0.0090\n",
      "[26/100][2000/3165] Loss_D: 0.0258 Loss_G: 1.0360 D(x): 0.7739 D(G(z)): -0.0034\n",
      "[26/100][2250/3165] Loss_D: 0.0515 Loss_G: 1.2204 D(x): 0.8728 D(G(z)): 0.0930\n",
      "[26/100][2500/3165] Loss_D: 0.0329 Loss_G: 1.0928 D(x): 0.7940 D(G(z)): 0.0377\n",
      "[26/100][2750/3165] Loss_D: 0.0262 Loss_G: 1.0837 D(x): 0.7781 D(G(z)): -0.0610\n",
      "[26/100][3000/3165] Loss_D: 0.0251 Loss_G: 1.0667 D(x): 0.7902 D(G(z)): -0.0036\n",
      "[27/100][0/3165] Loss_D: 0.0383 Loss_G: 0.8591 D(x): 0.6660 D(G(z)): -0.0101\n",
      "[27/100][250/3165] Loss_D: 0.0284 Loss_G: 1.0790 D(x): 0.7671 D(G(z)): 0.0440\n",
      "[27/100][500/3165] Loss_D: 0.0280 Loss_G: 1.1106 D(x): 0.8286 D(G(z)): 0.0166\n",
      "[27/100][750/3165] Loss_D: 0.0326 Loss_G: 1.0062 D(x): 0.7682 D(G(z)): 0.0145\n",
      "[27/100][1000/3165] Loss_D: 0.0479 Loss_G: 1.0779 D(x): 0.7357 D(G(z)): 0.0652\n",
      "[27/100][1250/3165] Loss_D: 0.0411 Loss_G: 1.1688 D(x): 0.8297 D(G(z)): 0.1001\n",
      "[27/100][1500/3165] Loss_D: 0.0369 Loss_G: 1.1200 D(x): 0.8302 D(G(z)): -0.0415\n",
      "[27/100][1750/3165] Loss_D: 0.0272 Loss_G: 1.1146 D(x): 0.8311 D(G(z)): 0.0039\n",
      "[27/100][2000/3165] Loss_D: 0.0372 Loss_G: 1.1167 D(x): 0.7510 D(G(z)): 0.0684\n",
      "[27/100][2250/3165] Loss_D: 0.0383 Loss_G: 1.0583 D(x): 0.7650 D(G(z)): 0.0634\n",
      "[27/100][2500/3165] Loss_D: 0.0845 Loss_G: 1.2288 D(x): 0.8782 D(G(z)): 0.1112\n"
     ]
    }
   ],
   "source": [
    "niter = 100\n",
    "d_fake_save = None\n",
    "for epoch in range(niter):\n",
    "    schedulerD.step()\n",
    "    schedulerG.step()\n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        # train D\n",
    "        netD.zero_grad()\n",
    "        real_cpu, embed_cpu = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "\n",
    "        real = real_cpu.cuda()\n",
    "        embed = embed_cpu.cuda()\n",
    "        input.resize_as_(real).copy_(real)\n",
    "        feature.resize_as_(embed).copy_(embed)\n",
    "        \n",
    "        inputv = Variable(input)\n",
    "        inputv = add_noise(inputv, d_fake_save)\n",
    "        featurev = Variable(feature)\n",
    "        \n",
    "        d_real = netD(inputv, featurev)\n",
    "        d_real_mean = d_real.data.mean()\n",
    "        \n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev, featurev)\n",
    "        \n",
    "        d_fake_save = d_fake = netD(fake.detach(), featurev)\n",
    "        d_fake_mean = d_fake.data.mean()\n",
    "        \n",
    "        loss_d = criterion(d_real, label_real_smooth) + criterion(d_fake, label_fake)\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # train G\n",
    "        netG.zero_grad()\n",
    "        d_fake = netD(fake, featurev)\n",
    "        loss_g = criterion(d_fake, label_real.detach())\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%250 == 0:\n",
    "            fake = netG(fixed_noise, fixed_feature)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03dstep_%04d.png' % (outf, epoch, i),\n",
    "                    normalize=True)\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f'\n",
    "                  % (epoch, niter, i, len(dataloader),\n",
    "                     loss_d.data[0], loss_g.data[0], d_real_mean, d_fake_mean))\n",
    "            \n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%03d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%03d.pth' % (outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO LIST FROM GANHACKS\n",
    "# https://github.com/soumith/ganhacks\n",
    "# ✓ Normalize the inputs\n",
    "# ✓ A modified loss function\n",
    "# ✓ Use a spherical Z\n",
    "# ✓ BatchNorm\n",
    "# Aviod sparse gradients\n",
    "# ✓ Use soft and noisy labels\n",
    "# ✓ DCGAN\n",
    "# Use stability tricks from RL\n",
    "# ✓ Use SGD for discriminator ADAM for generator\n",
    "# ✓ Add noise to inputs\n",
    "# Batch Discrimination (for diversity)\n",
    "# ✓ Use dropouts in G in both train and test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network as layer\n",
    "import torch.nn as nn\n",
    "\n",
    "nc = 3\n",
    "nz = 512\n",
    "nfeature = 736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "class ImageFeatureDataset(dset.ImageFolder):\n",
    "    def __init__(self, root, embed_dir, transform=None):\n",
    "        super().__init__(root=root, transform=transform)\n",
    "        self.embed = torch.load(embed_dir)\n",
    "#         self.landmark = torch.load(landmark_dir)\n",
    "#         self.feature = torch.cat([self.embed, self.landmark], 1)\n",
    "        self.feature = self.embed\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, target = super().__getitem__(index)\n",
    "        feature = self.feature[index]\n",
    "        return (img, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr     = 0.0002\n",
    "beta1  = 0.0   \n",
    "beta2  = 0.99     \n",
    "imageSize = 64\n",
    "batchSize = 64\n",
    "\n",
    "outf = \"./celeba_result/\"\n",
    "des_dir = \"./celeba/\"\n",
    "embed_dir = \"./celeba_embed/image_features.pth\"\n",
    "\n",
    "dataset = ImageFeatureDataset(root=des_dir,\n",
    "                            embed_dir=embed_dir,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.Resize(imageSize),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size= batchSize,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers import *\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def make_dense(self, k_in, k_growth, n, options):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(Dense(layer.conv(k_in, k_growth, 3, 1, 1, **options)))\n",
    "            k_in += k_growth\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        options = {'leaky':True, 'bn':True, 'wn':False, 'pixel':True, 'gdrop':True}\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(layer.deconv(nz + nfeature, 512, 4, 1, 0, **options))\n",
    "        # 4 x 4\n",
    "#         layers.append(layer.deconv(512, 512, 4, 2, 1, **options))\n",
    "        # 8 x 8\n",
    "        layers.append(layer.deconv(512, 256, 4, 2, 1, **options))\n",
    "        # 16 x 16\n",
    "        layers.append(layer.deconv(256, 128, 4, 2, 1, **options))\n",
    "        # 32 x 32\n",
    "        layers.append(layer.deconv(128, 64, 4, 2, 1, **options))\n",
    "        # 64 x 64\n",
    "        layers.append(layer.deconv(64, nc, 4, 2, 1, gdrop=options['gdrop'], only=True))\n",
    "        # 128 x 128\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x, feature):\n",
    "        feature = feature.view(-1, nfeature, 1, 1)\n",
    "        x = self.network(torch.cat([x, feature], 1))\n",
    "        return x\n",
    "    \n",
    "netG = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def make_dense(self, k_in, k_growth, n, options):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(Dense(layer.conv(k_in, k_growth, 3, 1, 1, **options)))\n",
    "            k_in += k_growth\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        options = {'leaky':True, 'bn':False, 'wn':False, 'pixel':False, 'gdrop':False}\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        layers.append(layer.conv(nc, 64, 4, 2, 1, **options))\n",
    "        # 64 x 64\n",
    "        layers.append(layer.conv(64, 128, 4, 2, 1, **options))\n",
    "        # 32 x 32\n",
    "        layers.append(layer.conv(128, 256, 4, 2, 1, **options))\n",
    "        # 16 x 16\n",
    "        layers.append(layer.conv(256, 512, 4, 2, 1, **options))\n",
    "        # 8 x 8\n",
    "#         layers.append(layer.conv(512, 512, 4, 2, 1, **options))\n",
    "        # 4 x 4\n",
    "        layers.append(layer.conv(512, 512, 4, 1, 0, **options))\n",
    "        # 1 x 1\n",
    "#         layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.linear = layer.linear(512 + nfeature, 1, sig=False, wn=options['wn'])\n",
    "    \n",
    "    def forward(self, x, feature):\n",
    "        x = self.network(x).squeeze()\n",
    "        x = self.linear(torch.cat([x, feature], 1))\n",
    "        return x\n",
    "    \n",
    "netD = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, imageSize,imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "feature = torch.FloatTensor(batchSize, 736)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "fixed_feature = dataset.feature[1]\n",
    "fixed_feature = fixed_feature.repeat(batchSize, 1, 1, 1)\n",
    "\n",
    "label_real = torch.FloatTensor(batchSize)\n",
    "label_real_smooth = torch.FloatTensor(batchSize)\n",
    "label_fake = torch.FloatTensor(batchSize)\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "criterion.cuda()\n",
    "input, feature, noise = input.cuda(), feature.cuda(), noise.cuda()\n",
    "label_real, label_real_smooth, label_fake = label_real.cuda(), label_real_smooth.cuda(), label_fake.cuda()\n",
    "fixed_noise, fixed_feature = fixed_noise.cuda(), fixed_feature.cuda()\n",
    "\n",
    "label_real.resize_(batchSize, 1).fill_(1)\n",
    "label_fake.resize_(batchSize, 1).fill_(0)\n",
    "label_real_smooth.resize_(batchSize, 1).fill_(0.8)\n",
    "label_real = Variable(label_real)\n",
    "label_fake = Variable(label_fake)\n",
    "label_real_smooth = Variable(label_real_smooth)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netD.load_state_dict(torch.load(outf + 'netD_epoch_042.pth'))\n",
    "# netG.load_state_dict(torch.load(outf + 'netG_epoch_042.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise)\n",
    "fixed_feature = Variable(fixed_feature)\n",
    "\n",
    "# setup optimizer\n",
    "# optimizerD = optim.SGD(netD.parameters(), lr = lr, momentum=0.9)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "schedulerD = optim.lr_scheduler.MultiStepLR(optimizerD, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "schedulerG = optim.lr_scheduler.MultiStepLR(optimizerG, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "result_dict = {}\n",
    "loss_D,loss_G,score_D,score_G1,score_G2 = [],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_d_ = None\n",
    "def add_noise(x, d_fake):\n",
    "    global _d_\n",
    "    if _d_ is not None:\n",
    "        _d_ = _d_ * 0.9 + torch.mean(d_fake).data[0] * 0.1\n",
    "        strength = 0.2 * max(0, _d_ - 0.5)**2\n",
    "        z = np.random.randn(*x.size()).astype(np.float32) * strength\n",
    "        z = Variable(torch.from_numpy(z)).cuda()\n",
    "        return x + z\n",
    "    else:\n",
    "        _d_ = 0.0\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][0/3165] Loss_D: 0.6110 Loss_G: 0.8280 D(x): 0.0190 D(G(z)): 0.0200\n",
      "[0/100][250/3165] Loss_D: 0.2822 Loss_G: 0.8090 D(x): 0.6102 D(G(z)): 0.4295\n",
      "[0/100][500/3165] Loss_D: 0.3229 Loss_G: 0.4681 D(x): 0.4384 D(G(z)): 0.4300\n",
      "[0/100][750/3165] Loss_D: 0.3036 Loss_G: 0.4802 D(x): 0.4418 D(G(z)): 0.4131\n",
      "[0/100][1000/3165] Loss_D: 0.3301 Loss_G: 0.3799 D(x): 0.2997 D(G(z)): 0.2792\n",
      "[0/100][1250/3165] Loss_D: 0.3034 Loss_G: 0.5132 D(x): 0.4593 D(G(z)): 0.4254\n",
      "[0/100][1500/3165] Loss_D: 0.2484 Loss_G: 0.5659 D(x): 0.4699 D(G(z)): 0.3558\n",
      "[0/100][1750/3165] Loss_D: 0.3120 Loss_G: 0.8984 D(x): 0.5705 D(G(z)): 0.4915\n",
      "[0/100][2000/3165] Loss_D: 0.2594 Loss_G: 0.4935 D(x): 0.3647 D(G(z)): 0.2164\n",
      "[0/100][2250/3165] Loss_D: 0.2692 Loss_G: 0.9173 D(x): 0.7184 D(G(z)): 0.4689\n",
      "[0/100][2500/3165] Loss_D: 0.1719 Loss_G: 1.2312 D(x): 0.6878 D(G(z)): 0.3367\n",
      "[0/100][2750/3165] Loss_D: 0.1851 Loss_G: 0.8431 D(x): 0.7092 D(G(z)): 0.3625\n",
      "[0/100][3000/3165] Loss_D: 0.2312 Loss_G: 1.1346 D(x): 0.7828 D(G(z)): 0.4342\n",
      "[1/100][0/3165] Loss_D: 0.1915 Loss_G: 1.0277 D(x): 0.7199 D(G(z)): 0.3945\n",
      "[1/100][250/3165] Loss_D: 0.1258 Loss_G: 0.7823 D(x): 0.5267 D(G(z)): 0.1502\n",
      "[1/100][500/3165] Loss_D: 0.2648 Loss_G: 0.4063 D(x): 0.3175 D(G(z)): 0.0578\n",
      "[1/100][750/3165] Loss_D: 0.1087 Loss_G: 0.8525 D(x): 0.6192 D(G(z)): 0.2036\n",
      "[1/100][1000/3165] Loss_D: 0.1598 Loss_G: 1.2429 D(x): 0.6781 D(G(z)): 0.3166\n",
      "[1/100][1250/3165] Loss_D: 0.2857 Loss_G: 0.4874 D(x): 0.2966 D(G(z)): 0.0258\n",
      "[1/100][1500/3165] Loss_D: 0.0750 Loss_G: 0.9995 D(x): 0.6408 D(G(z)): 0.1150\n",
      "[1/100][1750/3165] Loss_D: 0.0948 Loss_G: 0.9428 D(x): 0.6570 D(G(z)): 0.1890\n",
      "[1/100][2000/3165] Loss_D: 0.1269 Loss_G: 1.2174 D(x): 0.7597 D(G(z)): 0.2639\n",
      "[1/100][2250/3165] Loss_D: 0.2033 Loss_G: 1.2191 D(x): 0.7842 D(G(z)): 0.3987\n",
      "[1/100][2500/3165] Loss_D: 0.1483 Loss_G: 1.3017 D(x): 0.7237 D(G(z)): 0.3104\n",
      "[1/100][2750/3165] Loss_D: 0.1128 Loss_G: 1.1365 D(x): 0.7175 D(G(z)): 0.2520\n",
      "[1/100][3000/3165] Loss_D: 0.0768 Loss_G: 0.9807 D(x): 0.7012 D(G(z)): 0.1760\n",
      "[2/100][0/3165] Loss_D: 0.0886 Loss_G: 1.1795 D(x): 0.7521 D(G(z)): 0.2299\n",
      "[2/100][250/3165] Loss_D: 0.2919 Loss_G: 0.5212 D(x): 0.3013 D(G(z)): 0.0356\n",
      "[2/100][500/3165] Loss_D: 0.1739 Loss_G: 0.5321 D(x): 0.4540 D(G(z)): -0.0060\n",
      "[2/100][750/3165] Loss_D: 0.0683 Loss_G: 0.9961 D(x): 0.7246 D(G(z)): 0.1127\n",
      "[2/100][1000/3165] Loss_D: 0.2808 Loss_G: 0.6404 D(x): 0.3231 D(G(z)): -0.0084\n",
      "[2/100][1250/3165] Loss_D: 0.0912 Loss_G: 0.7981 D(x): 0.5740 D(G(z)): 0.0599\n",
      "[2/100][1500/3165] Loss_D: 0.0793 Loss_G: 1.0509 D(x): 0.7650 D(G(z)): 0.1465\n",
      "[2/100][1750/3165] Loss_D: 0.1158 Loss_G: 1.1413 D(x): 0.7568 D(G(z)): 0.2411\n",
      "[2/100][2000/3165] Loss_D: 0.0397 Loss_G: 1.0475 D(x): 0.7396 D(G(z)): 0.0678\n",
      "[2/100][2250/3165] Loss_D: 0.1381 Loss_G: 0.6321 D(x): 0.5247 D(G(z)): 0.1495\n",
      "[2/100][2500/3165] Loss_D: 0.0564 Loss_G: 1.0023 D(x): 0.7106 D(G(z)): 0.0838\n",
      "[2/100][2750/3165] Loss_D: 0.0964 Loss_G: 0.8588 D(x): 0.6186 D(G(z)): 0.1220\n",
      "[2/100][3000/3165] Loss_D: 0.0435 Loss_G: 1.1195 D(x): 0.8132 D(G(z)): 0.1195\n",
      "[3/100][0/3165] Loss_D: 0.0335 Loss_G: 1.0116 D(x): 0.7120 D(G(z)): 0.0531\n",
      "[3/100][250/3165] Loss_D: 0.1485 Loss_G: 0.4549 D(x): 0.4933 D(G(z)): 0.0060\n",
      "[3/100][500/3165] Loss_D: 0.1670 Loss_G: 0.7883 D(x): 0.6650 D(G(z)): 0.3048\n",
      "[3/100][750/3165] Loss_D: 0.1768 Loss_G: 0.9905 D(x): 0.6925 D(G(z)): 0.3428\n",
      "[3/100][1000/3165] Loss_D: 0.0613 Loss_G: 1.0294 D(x): 0.7533 D(G(z)): 0.1680\n",
      "[3/100][1250/3165] Loss_D: 0.0856 Loss_G: 0.7455 D(x): 0.5921 D(G(z)): -0.0064\n",
      "[3/100][1500/3165] Loss_D: 0.0554 Loss_G: 1.0542 D(x): 0.7997 D(G(z)): 0.1395\n",
      "[3/100][1750/3165] Loss_D: 0.0901 Loss_G: 0.7939 D(x): 0.6358 D(G(z)): 0.1020\n",
      "[3/100][2000/3165] Loss_D: 0.0592 Loss_G: 0.8724 D(x): 0.6332 D(G(z)): 0.0387\n",
      "[3/100][2250/3165] Loss_D: 0.1206 Loss_G: 0.9666 D(x): 0.7274 D(G(z)): 0.2728\n",
      "[3/100][2500/3165] Loss_D: 0.1348 Loss_G: 0.5681 D(x): 0.5401 D(G(z)): 0.0583\n",
      "[3/100][2750/3165] Loss_D: 0.0995 Loss_G: 0.8014 D(x): 0.5966 D(G(z)): 0.0533\n",
      "[3/100][3000/3165] Loss_D: 0.0499 Loss_G: 0.9377 D(x): 0.6857 D(G(z)): 0.0833\n",
      "[4/100][0/3165] Loss_D: 0.0476 Loss_G: 0.9681 D(x): 0.7270 D(G(z)): 0.0911\n",
      "[4/100][250/3165] Loss_D: 0.0946 Loss_G: 1.0802 D(x): 0.7427 D(G(z)): 0.1961\n",
      "[4/100][500/3165] Loss_D: 0.1318 Loss_G: 0.7141 D(x): 0.5354 D(G(z)): 0.0829\n",
      "[4/100][750/3165] Loss_D: 0.0439 Loss_G: 0.8624 D(x): 0.6912 D(G(z)): 0.0549\n",
      "[4/100][1000/3165] Loss_D: 0.1486 Loss_G: 0.3860 D(x): 0.5204 D(G(z)): 0.0485\n",
      "[4/100][1250/3165] Loss_D: 0.0808 Loss_G: 0.8344 D(x): 0.6873 D(G(z)): 0.1117\n",
      "[4/100][1500/3165] Loss_D: 0.0274 Loss_G: 1.0035 D(x): 0.7435 D(G(z)): 0.0565\n",
      "[4/100][1750/3165] Loss_D: 0.0811 Loss_G: 1.0397 D(x): 0.6974 D(G(z)): 0.1312\n",
      "[4/100][2000/3165] Loss_D: 0.0213 Loss_G: 0.9919 D(x): 0.7803 D(G(z)): 0.0231\n",
      "[4/100][2250/3165] Loss_D: 0.0237 Loss_G: 0.9837 D(x): 0.7804 D(G(z)): 0.0348\n",
      "[4/100][2500/3165] Loss_D: 0.1040 Loss_G: 1.0800 D(x): 0.7806 D(G(z)): 0.2611\n",
      "[4/100][2750/3165] Loss_D: 0.1733 Loss_G: 1.0752 D(x): 0.7945 D(G(z)): 0.3283\n",
      "[4/100][3000/3165] Loss_D: 0.0401 Loss_G: 0.9916 D(x): 0.7735 D(G(z)): 0.0943\n",
      "[5/100][0/3165] Loss_D: 0.0132 Loss_G: 0.9507 D(x): 0.8362 D(G(z)): -0.0575\n",
      "[5/100][250/3165] Loss_D: 0.0468 Loss_G: 0.9400 D(x): 0.7324 D(G(z)): 0.0940\n",
      "[5/100][500/3165] Loss_D: 0.0288 Loss_G: 0.9781 D(x): 0.7464 D(G(z)): 0.0152\n",
      "[5/100][750/3165] Loss_D: 0.0848 Loss_G: 0.6962 D(x): 0.6281 D(G(z)): -0.0436\n",
      "[5/100][1000/3165] Loss_D: 0.0770 Loss_G: 0.8455 D(x): 0.6857 D(G(z)): 0.0789\n",
      "[5/100][1250/3165] Loss_D: 0.0462 Loss_G: 0.7326 D(x): 0.6827 D(G(z)): 0.0138\n",
      "[5/100][1500/3165] Loss_D: 0.0991 Loss_G: 1.0711 D(x): 0.7975 D(G(z)): 0.2251\n",
      "[5/100][1750/3165] Loss_D: 0.0778 Loss_G: 1.1279 D(x): 0.7585 D(G(z)): 0.1900\n",
      "[5/100][2000/3165] Loss_D: 0.0222 Loss_G: 0.9857 D(x): 0.7560 D(G(z)): 0.0537\n",
      "[5/100][2250/3165] Loss_D: 0.0145 Loss_G: 1.0037 D(x): 0.8020 D(G(z)): 0.0315\n",
      "[5/100][2500/3165] Loss_D: 0.0582 Loss_G: 0.9642 D(x): 0.7286 D(G(z)): 0.1200\n",
      "[5/100][2750/3165] Loss_D: 0.0327 Loss_G: 1.0134 D(x): 0.7812 D(G(z)): 0.0644\n",
      "[5/100][3000/3165] Loss_D: 0.0466 Loss_G: 1.0918 D(x): 0.7774 D(G(z)): 0.1340\n",
      "[6/100][0/3165] Loss_D: 0.0352 Loss_G: 0.9854 D(x): 0.7701 D(G(z)): 0.0814\n",
      "[6/100][250/3165] Loss_D: 0.0543 Loss_G: 0.7831 D(x): 0.7029 D(G(z)): 0.0350\n",
      "[6/100][500/3165] Loss_D: 0.0191 Loss_G: 1.0025 D(x): 0.7418 D(G(z)): 0.0332\n",
      "[6/100][750/3165] Loss_D: 0.0250 Loss_G: 1.0018 D(x): 0.7761 D(G(z)): 0.0872\n",
      "[6/100][1000/3165] Loss_D: 0.0143 Loss_G: 1.0040 D(x): 0.7791 D(G(z)): 0.0337\n",
      "[6/100][1250/3165] Loss_D: 0.0122 Loss_G: 1.0036 D(x): 0.8020 D(G(z)): 0.0509\n",
      "[6/100][1500/3165] Loss_D: 0.0324 Loss_G: 0.9238 D(x): 0.7097 D(G(z)): 0.0260\n",
      "[6/100][1750/3165] Loss_D: 0.0184 Loss_G: 1.0410 D(x): 0.7717 D(G(z)): 0.0403\n",
      "[6/100][2000/3165] Loss_D: 0.0153 Loss_G: 0.9978 D(x): 0.7875 D(G(z)): 0.0506\n",
      "[6/100][2250/3165] Loss_D: 0.0628 Loss_G: 1.0615 D(x): 0.8697 D(G(z)): 0.1512\n",
      "[6/100][2500/3165] Loss_D: 0.0319 Loss_G: 1.0041 D(x): 0.7731 D(G(z)): 0.1048\n",
      "[6/100][2750/3165] Loss_D: 0.0298 Loss_G: 1.0036 D(x): 0.7524 D(G(z)): 0.0409\n",
      "[6/100][3000/3165] Loss_D: 0.0603 Loss_G: 1.0010 D(x): 0.7695 D(G(z)): 0.1257\n",
      "[7/100][0/3165] Loss_D: 0.0412 Loss_G: 1.0521 D(x): 0.8023 D(G(z)): 0.1263\n",
      "[7/100][250/3165] Loss_D: 0.0410 Loss_G: 1.0187 D(x): 0.7622 D(G(z)): 0.1123\n",
      "[7/100][500/3165] Loss_D: 0.0159 Loss_G: 1.0065 D(x): 0.7993 D(G(z)): 0.0431\n",
      "[7/100][750/3165] Loss_D: 0.0897 Loss_G: 0.8802 D(x): 0.7391 D(G(z)): 0.1536\n",
      "[7/100][1000/3165] Loss_D: 0.0383 Loss_G: 1.0374 D(x): 0.7756 D(G(z)): 0.1009\n",
      "[7/100][1250/3165] Loss_D: 0.0346 Loss_G: 1.0006 D(x): 0.7575 D(G(z)): 0.0723\n",
      "[7/100][1500/3165] Loss_D: 0.0148 Loss_G: 1.0371 D(x): 0.7825 D(G(z)): -0.0086\n",
      "[7/100][1750/3165] Loss_D: 0.0506 Loss_G: 0.9306 D(x): 0.7257 D(G(z)): 0.0097\n",
      "[7/100][2000/3165] Loss_D: 0.0083 Loss_G: 1.0288 D(x): 0.7908 D(G(z)): 0.0194\n",
      "[7/100][2250/3165] Loss_D: 0.0776 Loss_G: 1.0920 D(x): 0.7799 D(G(z)): 0.1794\n",
      "[7/100][2500/3165] Loss_D: 0.0161 Loss_G: 0.9676 D(x): 0.7550 D(G(z)): 0.0356\n",
      "[7/100][2750/3165] Loss_D: 0.0276 Loss_G: 1.0551 D(x): 0.7823 D(G(z)): 0.0821\n",
      "[7/100][3000/3165] Loss_D: 0.0372 Loss_G: 0.9765 D(x): 0.7543 D(G(z)): 0.0564\n",
      "[8/100][0/3165] Loss_D: 0.0160 Loss_G: 0.9977 D(x): 0.8731 D(G(z)): 0.0283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/100][250/3165] Loss_D: 0.0410 Loss_G: 0.9800 D(x): 0.7707 D(G(z)): 0.0696\n",
      "[8/100][500/3165] Loss_D: 0.0431 Loss_G: 0.9519 D(x): 0.8152 D(G(z)): 0.0799\n",
      "[8/100][750/3165] Loss_D: 0.0478 Loss_G: 1.0518 D(x): 0.8077 D(G(z)): 0.1522\n",
      "[8/100][1000/3165] Loss_D: 0.0266 Loss_G: 0.9762 D(x): 0.7505 D(G(z)): 0.0111\n",
      "[8/100][1250/3165] Loss_D: 0.0300 Loss_G: 0.9256 D(x): 0.7098 D(G(z)): 0.0291\n",
      "[8/100][1500/3165] Loss_D: 0.0437 Loss_G: 0.8894 D(x): 0.7054 D(G(z)): 0.0332\n",
      "[8/100][1750/3165] Loss_D: 0.0059 Loss_G: 1.0190 D(x): 0.7998 D(G(z)): -0.0078\n",
      "[8/100][2000/3165] Loss_D: 0.0784 Loss_G: 0.7515 D(x): 0.6089 D(G(z)): 0.0016\n",
      "[8/100][2250/3165] Loss_D: 0.0347 Loss_G: 0.9590 D(x): 0.7816 D(G(z)): 0.1175\n",
      "[8/100][2500/3165] Loss_D: 0.0264 Loss_G: 1.0179 D(x): 0.7840 D(G(z)): 0.0753\n",
      "[8/100][2750/3165] Loss_D: 0.0207 Loss_G: 1.0473 D(x): 0.7900 D(G(z)): 0.0762\n",
      "[8/100][3000/3165] Loss_D: 0.0546 Loss_G: 1.0081 D(x): 0.7388 D(G(z)): 0.1460\n",
      "[9/100][0/3165] Loss_D: 0.0564 Loss_G: 1.0478 D(x): 0.7947 D(G(z)): 0.1264\n",
      "[9/100][250/3165] Loss_D: 0.0205 Loss_G: 0.9336 D(x): 0.7269 D(G(z)): 0.0397\n",
      "[9/100][500/3165] Loss_D: 0.0437 Loss_G: 1.0878 D(x): 0.7764 D(G(z)): 0.1082\n",
      "[9/100][750/3165] Loss_D: 0.0028 Loss_G: 1.0048 D(x): 0.7831 D(G(z)): 0.0087\n",
      "[9/100][1000/3165] Loss_D: 0.0523 Loss_G: 1.1189 D(x): 0.7835 D(G(z)): 0.1300\n",
      "[9/100][1250/3165] Loss_D: 0.0147 Loss_G: 1.0158 D(x): 0.7809 D(G(z)): 0.0364\n",
      "[9/100][1500/3165] Loss_D: 0.0075 Loss_G: 0.9573 D(x): 0.7874 D(G(z)): -0.0340\n",
      "[9/100][1750/3165] Loss_D: 0.0090 Loss_G: 1.0327 D(x): 0.8287 D(G(z)): -0.0316\n",
      "[9/100][2000/3165] Loss_D: 0.0062 Loss_G: 0.9905 D(x): 0.7938 D(G(z)): 0.0136\n",
      "[9/100][2250/3165] Loss_D: 0.0800 Loss_G: 0.7811 D(x): 0.6295 D(G(z)): 0.0078\n",
      "[9/100][2500/3165] Loss_D: 0.0190 Loss_G: 0.9468 D(x): 0.7340 D(G(z)): 0.0025\n",
      "[9/100][2750/3165] Loss_D: 0.0052 Loss_G: 0.9997 D(x): 0.8144 D(G(z)): 0.0316\n",
      "[9/100][3000/3165] Loss_D: 0.0226 Loss_G: 0.9845 D(x): 0.7652 D(G(z)): 0.0097\n",
      "[10/100][0/3165] Loss_D: 0.0433 Loss_G: 0.8363 D(x): 0.7026 D(G(z)): 0.0081\n",
      "[10/100][250/3165] Loss_D: 0.0255 Loss_G: 1.0342 D(x): 0.7709 D(G(z)): 0.0373\n",
      "[10/100][500/3165] Loss_D: 0.0066 Loss_G: 1.0026 D(x): 0.7669 D(G(z)): 0.0083\n",
      "[10/100][750/3165] Loss_D: 0.0154 Loss_G: 1.0226 D(x): 0.7872 D(G(z)): 0.0389\n",
      "[10/100][1000/3165] Loss_D: 0.0231 Loss_G: 0.9744 D(x): 0.7498 D(G(z)): 0.0586\n",
      "[10/100][1250/3165] Loss_D: 0.0039 Loss_G: 1.0012 D(x): 0.7934 D(G(z)): 0.0126\n",
      "[10/100][1500/3165] Loss_D: 0.0440 Loss_G: 1.0360 D(x): 0.7898 D(G(z)): 0.1295\n",
      "[10/100][1750/3165] Loss_D: 0.0257 Loss_G: 0.9449 D(x): 0.7794 D(G(z)): 0.0281\n",
      "[10/100][2000/3165] Loss_D: 0.0237 Loss_G: 0.8788 D(x): 0.7403 D(G(z)): 0.0175\n",
      "[10/100][2250/3165] Loss_D: 0.0193 Loss_G: 1.0254 D(x): 0.7977 D(G(z)): 0.0478\n",
      "[10/100][2500/3165] Loss_D: 0.0883 Loss_G: 1.0303 D(x): 0.8130 D(G(z)): 0.1723\n",
      "[10/100][2750/3165] Loss_D: 0.0285 Loss_G: 0.9552 D(x): 0.7149 D(G(z)): 0.0175\n",
      "[10/100][3000/3165] Loss_D: 0.0245 Loss_G: 0.9953 D(x): 0.7836 D(G(z)): 0.0540\n",
      "[11/100][0/3165] Loss_D: 0.0218 Loss_G: 0.9826 D(x): 0.8002 D(G(z)): 0.0820\n",
      "[11/100][250/3165] Loss_D: 0.0047 Loss_G: 0.9926 D(x): 0.8087 D(G(z)): 0.0153\n",
      "[11/100][500/3165] Loss_D: 0.0078 Loss_G: 0.9908 D(x): 0.7856 D(G(z)): 0.0085\n",
      "[11/100][750/3165] Loss_D: 0.1481 Loss_G: 0.8323 D(x): 0.6438 D(G(z)): 0.2110\n",
      "[11/100][1000/3165] Loss_D: 0.0892 Loss_G: 0.6461 D(x): 0.6033 D(G(z)): 0.0112\n",
      "[11/100][1250/3165] Loss_D: 0.0601 Loss_G: 1.1198 D(x): 0.8425 D(G(z)): 0.1989\n",
      "[11/100][1500/3165] Loss_D: 0.2179 Loss_G: 1.0035 D(x): 0.7953 D(G(z)): 0.3955\n",
      "[11/100][1750/3165] Loss_D: 0.0221 Loss_G: 0.9937 D(x): 0.7841 D(G(z)): 0.0437\n",
      "[11/100][2000/3165] Loss_D: 0.0032 Loss_G: 0.9887 D(x): 0.8231 D(G(z)): 0.0109\n",
      "[11/100][2250/3165] Loss_D: 0.0096 Loss_G: 0.9742 D(x): 0.7872 D(G(z)): 0.0386\n",
      "[11/100][2500/3165] Loss_D: 0.0240 Loss_G: 1.0490 D(x): 0.7919 D(G(z)): 0.0615\n",
      "[11/100][2750/3165] Loss_D: 0.0387 Loss_G: 0.9786 D(x): 0.7734 D(G(z)): 0.0883\n",
      "[11/100][3000/3165] Loss_D: 0.0075 Loss_G: 1.0050 D(x): 0.7719 D(G(z)): 0.0180\n",
      "[12/100][0/3165] Loss_D: 0.0305 Loss_G: 1.0232 D(x): 0.7702 D(G(z)): 0.0644\n",
      "[12/100][250/3165] Loss_D: 0.0591 Loss_G: 0.7517 D(x): 0.6867 D(G(z)): -0.0010\n",
      "[12/100][500/3165] Loss_D: 0.0033 Loss_G: 0.9841 D(x): 0.7968 D(G(z)): 0.0143\n",
      "[12/100][750/3165] Loss_D: 0.0191 Loss_G: 1.0552 D(x): 0.7609 D(G(z)): 0.0351\n",
      "[12/100][1000/3165] Loss_D: 0.0139 Loss_G: 0.9693 D(x): 0.7688 D(G(z)): 0.0304\n",
      "[12/100][1250/3165] Loss_D: 0.0033 Loss_G: 1.0078 D(x): 0.7678 D(G(z)): 0.0093\n",
      "[12/100][1500/3165] Loss_D: 0.0320 Loss_G: 1.0936 D(x): 0.7958 D(G(z)): 0.0908\n"
     ]
    }
   ],
   "source": [
    "niter = 100\n",
    "d_fake_save = None\n",
    "for epoch in range(niter):\n",
    "    schedulerD.step()\n",
    "    schedulerG.step()\n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        # train D\n",
    "        netD.zero_grad()\n",
    "        real_cpu, embed_cpu = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "\n",
    "        real = real_cpu.cuda()\n",
    "        embed = embed_cpu.cuda()\n",
    "        input.resize_as_(real).copy_(real)\n",
    "        feature.resize_as_(embed).copy_(embed)\n",
    "        \n",
    "        inputv = Variable(input)\n",
    "        inputv = add_noise(inputv, d_fake_save)\n",
    "        featurev = Variable(feature)\n",
    "        \n",
    "        d_real = netD(inputv, featurev)\n",
    "        d_real_mean = d_real.data.mean()\n",
    "        \n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev, featurev)\n",
    "        \n",
    "        d_fake_save = d_fake = netD(fake.detach(), featurev)\n",
    "        d_fake_mean = d_fake.data.mean()\n",
    "        \n",
    "        loss_d = criterion(d_real, label_real_smooth) + criterion(d_fake, label_fake)\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # train G\n",
    "        netG.zero_grad()\n",
    "        d_fake = netD(fake, featurev)\n",
    "        loss_g = criterion(d_fake, label_real.detach())\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%250 == 0:\n",
    "            fake = netG(fixed_noise, fixed_feature)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03dstep_%04d.png' % (outf, epoch, i),\n",
    "                    normalize=True)\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f'\n",
    "                  % (epoch, niter, i, len(dataloader),\n",
    "                     loss_d.data[0], loss_g.data[0], d_real_mean, d_fake_mean))\n",
    "            \n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%03d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%03d.pth' % (outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

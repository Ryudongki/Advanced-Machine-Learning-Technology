{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO LIST FROM GANHACKS\n",
    "# https://github.com/soumith/ganhacks\n",
    "# ✓ Normalize the inputs\n",
    "# ✓ A modified loss function\n",
    "# ✓ Use a spherical Z\n",
    "# ✓ BatchNorm\n",
    "# Aviod sparse gradients\n",
    "# ✓ Use soft and noisy labels\n",
    "# ✓ DCGAN\n",
    "# Use stability tricks from RL\n",
    "# ✓ Use SGD for discriminator ADAM for generator\n",
    "# ✓ Add noise to inputs\n",
    "# Batch Discrimination (for diversity)\n",
    "# ✓ Use dropouts in G in both train and test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "from Folder import ImageFeatureFolder\n",
    "\n",
    "nc = 3\n",
    "nz = 512\n",
    "lr     = 0.0002\n",
    "beta1  = 0.0   \n",
    "beta2  = 0.99     \n",
    "imageSize = 128\n",
    "batchSize = 64\n",
    "\n",
    "outf = \"./celeba_result/\"\n",
    "des_dir = \"./celeba/\"\n",
    "\n",
    "dataset = dset.ImageFolder(root=des_dir,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.CenterCrop(178),\n",
    "                            transforms.Resize(imageSize),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size= batchSize,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from Models import Generator, Discriminator\n",
    "from VGG import VGG16Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Ipython\\caffeinism\\DCGAN\\custom_layers.py:131: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if initializer == 'kaiming':    kaiming_normal(self.deconv.weight, a=calculate_gain('conv2d'))\n",
      "D:\\Ipython\\caffeinism\\DCGAN\\custom_layers.py:114: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if initializer == 'kaiming':    kaiming_normal(self.conv.weight, a=calculate_gain('conv2d'))\n",
      "D:\\Ipython\\caffeinism\\DCGAN\\custom_layers.py:147: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if initializer == 'kaiming':    kaiming_normal(self.linear.weight, a=calculate_gain('linear'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "vggnet = VGG16Feature()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, imageSize,imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "feature = torch.FloatTensor(batchSize, 512, 7, 7)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "label_real = torch.FloatTensor(batchSize)\n",
    "label_real_smooth = torch.FloatTensor(batchSize)\n",
    "label_fake = torch.FloatTensor(batchSize)\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "criterion.cuda()\n",
    "vggnet.cuda()\n",
    "vggnet.eval()\n",
    "input, feature, noise = input.cuda(), feature.cuda(), noise.cuda()\n",
    "label_real, label_real_smooth, label_fake = label_real.cuda(), label_real_smooth.cuda(), label_fake.cuda()\n",
    "fixed_noise = fixed_noise.cuda()\n",
    "\n",
    "label_real.resize_(batchSize, 1).fill_(1)\n",
    "label_fake.resize_(batchSize, 1).fill_(0)\n",
    "label_real_smooth.resize_(batchSize, 1).fill_(0.9)\n",
    "label_real = Variable(label_real)\n",
    "label_fake = Variable(label_fake)\n",
    "label_real_smooth = Variable(label_real_smooth)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netD.load_state_dict(torch.load(outf + 'netD_epoch_016.pth'))\n",
    "# netG.load_state_dict(torch.load(outf + 'netG_epoch_016.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "# setup optimizer\n",
    "# optimizerD = optim.SGD(netD.parameters(), lr = lr, momentum=0.9)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "schedulerD = optim.lr_scheduler.MultiStepLR(optimizerD, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "schedulerG = optim.lr_scheduler.MultiStepLR(optimizerG, milestones=[4, 6, 8, 10, 12, 14, 17], gamma=0.87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_d_ = None\n",
    "def add_noise(x, d_fake):\n",
    "    global _d_\n",
    "    if _d_ is not None:\n",
    "        _d_ = _d_ * 0.9 + torch.mean(d_fake).data[0] * 0.1\n",
    "        strength = 0.2 * max(0.00001, _d_ - 0.5)**2\n",
    "        z = torch.FloatTensor(*x.size()).normal_(0, 1) * strength\n",
    "#         z = np.random.randn(*x.size()).astype(np.float32) * strength\n",
    "        zv = Variable(z).cuda()\n",
    "        return x + zv\n",
    "    else:\n",
    "        _d_ = 0.0\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][0/3165] Loss_D: 0.6214 Loss_G: 0.4480 D(x): 0.4255 D(G(z)): 0.4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][500/3165] Loss_D: 0.2409 Loss_G: 0.6314 D(x): 0.6232 D(G(z)): 0.2330\n",
      "[0/100][1000/3165] Loss_D: 0.2864 Loss_G: 0.7458 D(x): 0.7274 D(G(z)): 0.2962\n",
      "[0/100][1500/3165] Loss_D: 0.2300 Loss_G: 0.7988 D(x): 0.7867 D(G(z)): 0.3453\n",
      "[0/100][2000/3165] Loss_D: 0.2540 Loss_G: 0.5191 D(x): 0.5905 D(G(z)): 0.2537\n",
      "[0/100][2500/3165] Loss_D: 0.2813 Loss_G: 0.6493 D(x): 0.6224 D(G(z)): 0.3480\n",
      "[0/100][3000/3165] Loss_D: 0.2951 Loss_G: 0.5887 D(x): 0.6351 D(G(z)): 0.3677\n",
      "[1/100][0/3165] Loss_D: 0.2727 Loss_G: 0.5521 D(x): 0.6273 D(G(z)): 0.3452\n",
      "[1/100][500/3165] Loss_D: 0.2617 Loss_G: 0.7095 D(x): 0.6948 D(G(z)): 0.3095\n",
      "[1/100][1000/3165] Loss_D: 0.1715 Loss_G: 0.6547 D(x): 0.7007 D(G(z)): 0.2589\n",
      "[1/100][1500/3165] Loss_D: 0.1858 Loss_G: 0.6902 D(x): 0.7202 D(G(z)): 0.2819\n",
      "[1/100][2000/3165] Loss_D: 0.2653 Loss_G: 0.4097 D(x): 0.5124 D(G(z)): 0.1533\n",
      "[1/100][2500/3165] Loss_D: 0.2624 Loss_G: 0.6005 D(x): 0.6249 D(G(z)): 0.3233\n",
      "[1/100][3000/3165] Loss_D: 0.2092 Loss_G: 0.5795 D(x): 0.6359 D(G(z)): 0.2253\n",
      "[2/100][0/3165] Loss_D: 0.2677 Loss_G: 0.7376 D(x): 0.7243 D(G(z)): 0.3984\n",
      "[2/100][500/3165] Loss_D: 0.2259 Loss_G: 0.4868 D(x): 0.5572 D(G(z)): 0.1940\n",
      "[2/100][1000/3165] Loss_D: 0.2584 Loss_G: 0.4063 D(x): 0.4926 D(G(z)): 0.1376\n",
      "[2/100][1500/3165] Loss_D: 0.2454 Loss_G: 0.8283 D(x): 0.7432 D(G(z)): 0.3838\n",
      "[2/100][2000/3165] Loss_D: 0.3738 Loss_G: 0.2189 D(x): 0.3619 D(G(z)): 0.0730\n",
      "[2/100][2500/3165] Loss_D: 0.2562 Loss_G: 0.5611 D(x): 0.6185 D(G(z)): 0.2873\n",
      "[2/100][3000/3165] Loss_D: 0.2240 Loss_G: 0.4733 D(x): 0.5567 D(G(z)): 0.2199\n",
      "[3/100][0/3165] Loss_D: 0.2072 Loss_G: 0.4901 D(x): 0.5721 D(G(z)): 0.1630\n",
      "[3/100][500/3165] Loss_D: 0.2010 Loss_G: 0.6023 D(x): 0.6532 D(G(z)): 0.2795\n",
      "[3/100][1000/3165] Loss_D: 0.2608 Loss_G: 0.4970 D(x): 0.5765 D(G(z)): 0.2405\n",
      "[3/100][1500/3165] Loss_D: 0.1646 Loss_G: 0.7361 D(x): 0.7077 D(G(z)): 0.2164\n",
      "[3/100][2000/3165] Loss_D: 0.1950 Loss_G: 0.5844 D(x): 0.6309 D(G(z)): 0.2499\n",
      "[3/100][2500/3165] Loss_D: 0.2207 Loss_G: 0.5113 D(x): 0.5737 D(G(z)): 0.2239\n",
      "[3/100][3000/3165] Loss_D: 0.2137 Loss_G: 0.4598 D(x): 0.5438 D(G(z)): 0.1827\n",
      "[4/100][0/3165] Loss_D: 0.2656 Loss_G: 0.6313 D(x): 0.6517 D(G(z)): 0.3737\n",
      "[4/100][500/3165] Loss_D: 0.2316 Loss_G: 0.4879 D(x): 0.5673 D(G(z)): 0.2295\n",
      "[4/100][1000/3165] Loss_D: 0.2471 Loss_G: 0.7487 D(x): 0.7145 D(G(z)): 0.3714\n",
      "[4/100][1500/3165] Loss_D: 0.2503 Loss_G: 0.5543 D(x): 0.6226 D(G(z)): 0.2717\n",
      "[4/100][2000/3165] Loss_D: 0.1757 Loss_G: 0.6055 D(x): 0.6653 D(G(z)): 0.2009\n",
      "[4/100][2500/3165] Loss_D: 0.2662 Loss_G: 0.8112 D(x): 0.7632 D(G(z)): 0.4134\n",
      "[4/100][3000/3165] Loss_D: 0.2009 Loss_G: 0.5276 D(x): 0.5793 D(G(z)): 0.1605\n",
      "[5/100][0/3165] Loss_D: 0.2951 Loss_G: 0.4120 D(x): 0.4775 D(G(z)): 0.1035\n",
      "[5/100][500/3165] Loss_D: 0.1850 Loss_G: 0.6873 D(x): 0.7477 D(G(z)): 0.2252\n",
      "[5/100][1000/3165] Loss_D: 0.2653 Loss_G: 0.3578 D(x): 0.4775 D(G(z)): 0.0776\n",
      "[5/100][1500/3165] Loss_D: 0.1987 Loss_G: 0.8726 D(x): 0.8061 D(G(z)): 0.2608\n",
      "[5/100][2000/3165] Loss_D: 0.2380 Loss_G: 0.6696 D(x): 0.6872 D(G(z)): 0.3007\n",
      "[5/100][2500/3165] Loss_D: 0.2059 Loss_G: 0.4867 D(x): 0.5461 D(G(z)): 0.1280\n",
      "[5/100][3000/3165] Loss_D: 0.2282 Loss_G: 0.8029 D(x): 0.7639 D(G(z)): 0.3050\n",
      "[6/100][0/3165] Loss_D: 0.2556 Loss_G: 0.7992 D(x): 0.7880 D(G(z)): 0.3931\n",
      "[6/100][500/3165] Loss_D: 0.2255 Loss_G: 0.8516 D(x): 0.7403 D(G(z)): 0.3437\n",
      "[6/100][1000/3165] Loss_D: 0.1614 Loss_G: 0.6658 D(x): 0.6773 D(G(z)): 0.1709\n",
      "[6/100][1500/3165] Loss_D: 0.2223 Loss_G: 0.6079 D(x): 0.6397 D(G(z)): 0.2707\n",
      "[6/100][2000/3165] Loss_D: 0.1990 Loss_G: 0.5950 D(x): 0.6422 D(G(z)): 0.1674\n",
      "[6/100][2500/3165] Loss_D: 0.2113 Loss_G: 0.6141 D(x): 0.6447 D(G(z)): 0.1761\n",
      "[6/100][3000/3165] Loss_D: 0.2409 Loss_G: 0.6088 D(x): 0.6556 D(G(z)): 0.3378\n",
      "[7/100][0/3165] Loss_D: 0.3033 Loss_G: 0.9155 D(x): 0.8569 D(G(z)): 0.4712\n",
      "[7/100][500/3165] Loss_D: 0.1749 Loss_G: 0.5929 D(x): 0.6465 D(G(z)): 0.2245\n",
      "[7/100][1000/3165] Loss_D: 0.1900 Loss_G: 0.5454 D(x): 0.6101 D(G(z)): 0.1899\n",
      "[7/100][1500/3165] Loss_D: 0.1317 Loss_G: 0.6956 D(x): 0.7058 D(G(z)): 0.1699\n",
      "[7/100][2000/3165] Loss_D: 0.2159 Loss_G: 0.5416 D(x): 0.5963 D(G(z)): 0.1870\n",
      "[7/100][2500/3165] Loss_D: 0.2078 Loss_G: 0.6207 D(x): 0.6436 D(G(z)): 0.2356\n",
      "[7/100][3000/3165] Loss_D: 0.2033 Loss_G: 0.8443 D(x): 0.7621 D(G(z)): 0.3445\n",
      "[8/100][0/3165] Loss_D: 0.2048 Loss_G: 0.7172 D(x): 0.6923 D(G(z)): 0.2696\n",
      "[8/100][500/3165] Loss_D: 0.1938 Loss_G: 0.5610 D(x): 0.5884 D(G(z)): 0.1107\n",
      "[8/100][1000/3165] Loss_D: 0.1724 Loss_G: 0.7559 D(x): 0.7363 D(G(z)): 0.2733\n",
      "[8/100][1500/3165] Loss_D: 0.2118 Loss_G: 0.7296 D(x): 0.7147 D(G(z)): 0.3077\n",
      "[8/100][2000/3165] Loss_D: 0.1743 Loss_G: 0.6356 D(x): 0.6814 D(G(z)): 0.1817\n",
      "[8/100][2500/3165] Loss_D: 0.1648 Loss_G: 0.5565 D(x): 0.5926 D(G(z)): 0.0264\n",
      "[8/100][3000/3165] Loss_D: 0.1524 Loss_G: 0.6466 D(x): 0.6605 D(G(z)): 0.1371\n",
      "[9/100][0/3165] Loss_D: 0.1655 Loss_G: 0.6286 D(x): 0.6900 D(G(z)): -0.0183\n",
      "[9/100][500/3165] Loss_D: 0.1908 Loss_G: 0.5761 D(x): 0.6144 D(G(z)): 0.1535\n",
      "[9/100][1000/3165] Loss_D: 0.2174 Loss_G: 0.5586 D(x): 0.6115 D(G(z)): 0.2068\n",
      "[9/100][1500/3165] Loss_D: 0.1542 Loss_G: 0.7116 D(x): 0.7270 D(G(z)): 0.1706\n",
      "[9/100][2000/3165] Loss_D: 0.1823 Loss_G: 0.6139 D(x): 0.6361 D(G(z)): 0.1354\n",
      "[9/100][2500/3165] Loss_D: 0.1627 Loss_G: 0.7038 D(x): 0.7206 D(G(z)): 0.2330\n",
      "[9/100][3000/3165] Loss_D: 0.1767 Loss_G: 0.8927 D(x): 0.8305 D(G(z)): 0.3064\n",
      "[10/100][0/3165] Loss_D: 0.1010 Loss_G: 0.8831 D(x): 0.8421 D(G(z)): 0.1053\n",
      "[10/100][500/3165] Loss_D: 0.1673 Loss_G: 0.7406 D(x): 0.7413 D(G(z)): 0.2225\n",
      "[10/100][1000/3165] Loss_D: 0.1200 Loss_G: 0.7304 D(x): 0.7189 D(G(z)): 0.0202\n",
      "[10/100][1500/3165] Loss_D: 0.1778 Loss_G: 0.9210 D(x): 0.8421 D(G(z)): 0.3216\n",
      "[10/100][2000/3165] Loss_D: 0.1410 Loss_G: 0.8281 D(x): 0.8260 D(G(z)): 0.2609\n",
      "[10/100][2500/3165] Loss_D: 0.1107 Loss_G: 0.7573 D(x): 0.7465 D(G(z)): 0.0885\n",
      "[10/100][3000/3165] Loss_D: 0.1629 Loss_G: 0.7568 D(x): 0.7485 D(G(z)): 0.2175\n",
      "[11/100][0/3165] Loss_D: 0.1435 Loss_G: 0.6746 D(x): 0.7053 D(G(z)): 0.2121\n",
      "[11/100][500/3165] Loss_D: 0.1158 Loss_G: 0.8825 D(x): 0.8028 D(G(z)): 0.0822\n",
      "[11/100][1000/3165] Loss_D: 0.1414 Loss_G: 0.7773 D(x): 0.7620 D(G(z)): 0.2477\n",
      "[11/100][1500/3165] Loss_D: 0.1003 Loss_G: 0.9291 D(x): 0.8627 D(G(z)): 0.1698\n",
      "[11/100][2000/3165] Loss_D: 0.1324 Loss_G: 0.6190 D(x): 0.6799 D(G(z)): 0.1382\n",
      "[11/100][2500/3165] Loss_D: 0.1706 Loss_G: 0.7356 D(x): 0.7268 D(G(z)): 0.2488\n",
      "[11/100][3000/3165] Loss_D: 0.1828 Loss_G: 0.7215 D(x): 0.7170 D(G(z)): 0.2409\n",
      "[12/100][0/3165] Loss_D: 0.1581 Loss_G: 1.0681 D(x): 0.8940 D(G(z)): 0.2815\n",
      "[12/100][500/3165] Loss_D: 0.1254 Loss_G: 0.7827 D(x): 0.7764 D(G(z)): 0.2357\n",
      "[12/100][1000/3165] Loss_D: 0.1330 Loss_G: 0.8843 D(x): 0.7785 D(G(z)): 0.1694\n",
      "[12/100][1500/3165] Loss_D: 0.1499 Loss_G: 0.6565 D(x): 0.6808 D(G(z)): 0.0790\n",
      "[12/100][2000/3165] Loss_D: 0.1702 Loss_G: 0.9951 D(x): 0.8923 D(G(z)): 0.2696\n",
      "[12/100][2500/3165] Loss_D: 0.1430 Loss_G: 0.8717 D(x): 0.8123 D(G(z)): 0.2233\n",
      "[12/100][3000/3165] Loss_D: 0.0978 Loss_G: 0.8019 D(x): 0.7883 D(G(z)): 0.1522\n",
      "[13/100][0/3165] Loss_D: 0.1786 Loss_G: 1.2570 D(x): 0.9477 D(G(z)): 0.2905\n",
      "[13/100][500/3165] Loss_D: 0.1433 Loss_G: 0.6664 D(x): 0.6854 D(G(z)): 0.1066\n",
      "[13/100][1000/3165] Loss_D: 0.1815 Loss_G: 0.9591 D(x): 0.8204 D(G(z)): 0.2962\n",
      "[13/100][1500/3165] Loss_D: 0.1477 Loss_G: 0.5827 D(x): 0.6558 D(G(z)): 0.1419\n",
      "[13/100][2000/3165] Loss_D: 0.1116 Loss_G: 0.6768 D(x): 0.7004 D(G(z)): 0.0998\n",
      "[13/100][2500/3165] Loss_D: 0.0960 Loss_G: 0.8410 D(x): 0.7878 D(G(z)): 0.1174\n",
      "[13/100][3000/3165] Loss_D: 0.0736 Loss_G: 0.9785 D(x): 0.8461 D(G(z)): 0.0596\n",
      "[14/100][0/3165] Loss_D: 0.1587 Loss_G: 0.6058 D(x): 0.6543 D(G(z)): 0.0268\n",
      "[14/100][500/3165] Loss_D: 0.1406 Loss_G: 0.8952 D(x): 0.8065 D(G(z)): 0.2601\n",
      "[14/100][1000/3165] Loss_D: 0.1334 Loss_G: 0.9269 D(x): 0.8227 D(G(z)): 0.2185\n",
      "[14/100][1500/3165] Loss_D: 0.1148 Loss_G: 0.8801 D(x): 0.8083 D(G(z)): 0.2002\n",
      "[14/100][2000/3165] Loss_D: 0.1476 Loss_G: 0.9733 D(x): 0.8874 D(G(z)): 0.2908\n",
      "[14/100][2500/3165] Loss_D: 0.1071 Loss_G: 0.7199 D(x): 0.7371 D(G(z)): 0.0981\n",
      "[14/100][3000/3165] Loss_D: 0.0998 Loss_G: 0.7368 D(x): 0.7481 D(G(z)): 0.1399\n",
      "[15/100][0/3165] Loss_D: 0.1853 Loss_G: 1.0719 D(x): 0.8853 D(G(z)): 0.3265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/100][500/3165] Loss_D: 0.1139 Loss_G: 0.9307 D(x): 0.8535 D(G(z)): 0.2265\n",
      "[15/100][1000/3165] Loss_D: 0.1186 Loss_G: 0.9080 D(x): 0.8197 D(G(z)): 0.2376\n",
      "[15/100][1500/3165] Loss_D: 0.1264 Loss_G: 0.6583 D(x): 0.6952 D(G(z)): 0.1020\n",
      "[15/100][2000/3165] Loss_D: 0.2381 Loss_G: 1.2053 D(x): 0.9774 D(G(z)): 0.3892\n",
      "[15/100][2500/3165] Loss_D: 0.1425 Loss_G: 0.9535 D(x): 0.8735 D(G(z)): 0.2624\n",
      "[15/100][3000/3165] Loss_D: 0.1543 Loss_G: 0.9965 D(x): 0.8835 D(G(z)): 0.2793\n",
      "[16/100][0/3165] Loss_D: 0.1271 Loss_G: 0.6355 D(x): 0.6587 D(G(z)): 0.0369\n",
      "[16/100][500/3165] Loss_D: 0.1291 Loss_G: 0.6663 D(x): 0.6925 D(G(z)): -0.0029\n",
      "[16/100][1000/3165] Loss_D: 0.1282 Loss_G: 0.7312 D(x): 0.7221 D(G(z)): 0.0640\n",
      "[16/100][1500/3165] Loss_D: 0.1100 Loss_G: 0.7635 D(x): 0.7397 D(G(z)): 0.1204\n",
      "[16/100][2000/3165] Loss_D: 0.0952 Loss_G: 0.9073 D(x): 0.8360 D(G(z)): 0.1705\n",
      "[16/100][2500/3165] Loss_D: 0.1241 Loss_G: 0.7154 D(x): 0.7149 D(G(z)): 0.1128\n",
      "[16/100][3000/3165] Loss_D: 0.1063 Loss_G: 0.8659 D(x): 0.8126 D(G(z)): 0.1404\n",
      "[17/100][0/3165] Loss_D: 0.1234 Loss_G: 0.7159 D(x): 0.7419 D(G(z)): 0.1923\n",
      "[17/100][500/3165] Loss_D: 0.1061 Loss_G: 0.9955 D(x): 0.9130 D(G(z)): 0.2037\n",
      "[17/100][1000/3165] Loss_D: 0.1223 Loss_G: 0.8143 D(x): 0.7936 D(G(z)): 0.1163\n",
      "[17/100][1500/3165] Loss_D: 0.1383 Loss_G: 0.5838 D(x): 0.6310 D(G(z)): 0.0183\n",
      "[17/100][2000/3165] Loss_D: 0.1116 Loss_G: 1.0008 D(x): 0.8889 D(G(z)): 0.2029\n",
      "[17/100][2500/3165] Loss_D: 0.1023 Loss_G: 0.8385 D(x): 0.8052 D(G(z)): 0.1707\n",
      "[17/100][3000/3165] Loss_D: 0.1060 Loss_G: 0.8316 D(x): 0.8023 D(G(z)): 0.1827\n",
      "[18/100][0/3165] Loss_D: 0.1071 Loss_G: 0.7938 D(x): 0.7768 D(G(z)): 0.0797\n",
      "[18/100][500/3165] Loss_D: 0.0885 Loss_G: 0.8293 D(x): 0.7843 D(G(z)): -0.0214\n",
      "[18/100][1000/3165] Loss_D: 0.1073 Loss_G: 0.8955 D(x): 0.8161 D(G(z)): 0.1853\n",
      "[18/100][1500/3165] Loss_D: 0.1022 Loss_G: 0.7457 D(x): 0.7394 D(G(z)): 0.1021\n",
      "[18/100][2000/3165] Loss_D: 0.1129 Loss_G: 0.7169 D(x): 0.7100 D(G(z)): 0.1088\n",
      "[18/100][2500/3165] Loss_D: 0.1169 Loss_G: 0.8017 D(x): 0.7664 D(G(z)): 0.1607\n",
      "[18/100][3000/3165] Loss_D: 0.1296 Loss_G: 0.9559 D(x): 0.8758 D(G(z)): 0.2142\n",
      "[19/100][0/3165] Loss_D: 0.1089 Loss_G: 0.8149 D(x): 0.7842 D(G(z)): 0.1557\n",
      "[19/100][500/3165] Loss_D: 0.0829 Loss_G: 0.8167 D(x): 0.7852 D(G(z)): 0.1118\n",
      "[19/100][1000/3165] Loss_D: 0.0780 Loss_G: 0.9847 D(x): 0.8631 D(G(z)): 0.0830\n",
      "[19/100][1500/3165] Loss_D: 0.1179 Loss_G: 0.7863 D(x): 0.7709 D(G(z)): 0.1861\n",
      "[19/100][2000/3165] Loss_D: 0.1126 Loss_G: 0.6831 D(x): 0.7076 D(G(z)): 0.0983\n",
      "[19/100][2500/3165] Loss_D: 0.1136 Loss_G: 0.7548 D(x): 0.7643 D(G(z)): 0.1714\n",
      "[19/100][3000/3165] Loss_D: 0.0975 Loss_G: 0.8213 D(x): 0.7806 D(G(z)): 0.1179\n",
      "[20/100][0/3165] Loss_D: 0.1076 Loss_G: 0.8211 D(x): 0.7742 D(G(z)): 0.1100\n",
      "[20/100][500/3165] Loss_D: 0.1718 Loss_G: 1.0173 D(x): 0.9125 D(G(z)): 0.2989\n",
      "[20/100][1000/3165] Loss_D: 0.0923 Loss_G: 0.8362 D(x): 0.7947 D(G(z)): 0.1496\n",
      "[20/100][1500/3165] Loss_D: 0.0971 Loss_G: 1.0508 D(x): 0.9308 D(G(z)): 0.1846\n",
      "[20/100][2000/3165] Loss_D: 0.0678 Loss_G: 0.9053 D(x): 0.8124 D(G(z)): 0.1239\n",
      "[20/100][2500/3165] Loss_D: 0.0938 Loss_G: 0.9181 D(x): 0.8367 D(G(z)): 0.1490\n",
      "[20/100][3000/3165] Loss_D: 0.1071 Loss_G: 1.0058 D(x): 0.8678 D(G(z)): 0.1988\n",
      "[21/100][0/3165] Loss_D: 0.0729 Loss_G: 0.8720 D(x): 0.8115 D(G(z)): 0.0896\n",
      "[21/100][500/3165] Loss_D: 0.0910 Loss_G: 0.9535 D(x): 0.8710 D(G(z)): 0.1243\n",
      "[21/100][1000/3165] Loss_D: 0.0783 Loss_G: 0.7503 D(x): 0.7423 D(G(z)): 0.0685\n",
      "[21/100][1500/3165] Loss_D: 0.0676 Loss_G: 0.8757 D(x): 0.8365 D(G(z)): 0.1192\n",
      "[21/100][2000/3165] Loss_D: 0.1018 Loss_G: 0.7297 D(x): 0.7201 D(G(z)): 0.0492\n",
      "[21/100][2500/3165] Loss_D: 0.0815 Loss_G: 0.9531 D(x): 0.8679 D(G(z)): 0.1375\n",
      "[21/100][3000/3165] Loss_D: 0.0889 Loss_G: 0.7799 D(x): 0.7590 D(G(z)): 0.0881\n",
      "[22/100][0/3165] Loss_D: 0.1131 Loss_G: 0.7473 D(x): 0.7382 D(G(z)): 0.0595\n",
      "[22/100][500/3165] Loss_D: 0.0758 Loss_G: 0.7695 D(x): 0.7687 D(G(z)): 0.0399\n",
      "[22/100][1000/3165] Loss_D: 0.0882 Loss_G: 0.7804 D(x): 0.7603 D(G(z)): 0.0714\n",
      "[22/100][1500/3165] Loss_D: 0.1008 Loss_G: 0.8088 D(x): 0.7752 D(G(z)): 0.0992\n",
      "[22/100][2000/3165] Loss_D: 0.0993 Loss_G: 0.9012 D(x): 0.8140 D(G(z)): 0.1161\n",
      "[22/100][2500/3165] Loss_D: 0.0972 Loss_G: 0.8563 D(x): 0.8019 D(G(z)): 0.1253\n",
      "[22/100][3000/3165] Loss_D: 0.1224 Loss_G: 0.6010 D(x): 0.6629 D(G(z)): 0.0035\n",
      "[23/100][0/3165] Loss_D: 0.0986 Loss_G: 0.9488 D(x): 0.8564 D(G(z)): 0.1771\n",
      "[23/100][500/3165] Loss_D: 0.0806 Loss_G: 0.9028 D(x): 0.8413 D(G(z)): 0.1282\n",
      "[23/100][1000/3165] Loss_D: 0.1012 Loss_G: 0.8163 D(x): 0.7780 D(G(z)): 0.1357\n",
      "[23/100][1500/3165] Loss_D: 0.1029 Loss_G: 0.9182 D(x): 0.8275 D(G(z)): 0.1862\n",
      "[23/100][2000/3165] Loss_D: 0.1074 Loss_G: 0.7745 D(x): 0.7548 D(G(z)): 0.1091\n",
      "[23/100][2500/3165] Loss_D: 0.0886 Loss_G: 0.8726 D(x): 0.8031 D(G(z)): 0.1390\n",
      "[23/100][3000/3165] Loss_D: 0.0957 Loss_G: 0.8884 D(x): 0.8256 D(G(z)): 0.1545\n",
      "[24/100][0/3165] Loss_D: 0.1102 Loss_G: 0.9724 D(x): 0.8427 D(G(z)): 0.1700\n",
      "[24/100][500/3165] Loss_D: 0.0980 Loss_G: 0.6758 D(x): 0.7187 D(G(z)): 0.0568\n",
      "[24/100][1000/3165] Loss_D: 0.0750 Loss_G: 1.1239 D(x): 0.9437 D(G(z)): 0.0620\n",
      "[24/100][1500/3165] Loss_D: 0.0575 Loss_G: 0.9064 D(x): 0.8362 D(G(z)): -0.0636\n",
      "[24/100][2000/3165] Loss_D: 0.0607 Loss_G: 0.9917 D(x): 0.8891 D(G(z)): 0.0979\n",
      "[24/100][2500/3165] Loss_D: 0.0972 Loss_G: 0.8114 D(x): 0.7752 D(G(z)): 0.1093\n",
      "[24/100][3000/3165] Loss_D: 0.1549 Loss_G: 1.0266 D(x): 0.9282 D(G(z)): 0.3005\n",
      "[25/100][0/3165] Loss_D: 0.0914 Loss_G: 0.8868 D(x): 0.8219 D(G(z)): 0.1308\n",
      "[25/100][500/3165] Loss_D: 0.1085 Loss_G: 0.7615 D(x): 0.7524 D(G(z)): 0.1363\n",
      "[25/100][1000/3165] Loss_D: 0.0735 Loss_G: 0.9387 D(x): 0.8746 D(G(z)): 0.1315\n",
      "[25/100][1500/3165] Loss_D: 0.0771 Loss_G: 0.8892 D(x): 0.8222 D(G(z)): 0.0954\n",
      "[25/100][2000/3165] Loss_D: 0.0849 Loss_G: 0.9851 D(x): 0.8593 D(G(z)): 0.1844\n",
      "[25/100][2500/3165] Loss_D: 0.1102 Loss_G: 0.6784 D(x): 0.6989 D(G(z)): 0.0737\n",
      "[25/100][3000/3165] Loss_D: 0.0943 Loss_G: 0.8986 D(x): 0.8052 D(G(z)): 0.1413\n",
      "[26/100][0/3165] Loss_D: 0.0879 Loss_G: 0.8908 D(x): 0.8217 D(G(z)): 0.0622\n",
      "[26/100][500/3165] Loss_D: 0.1245 Loss_G: 0.6232 D(x): 0.6628 D(G(z)): 0.0260\n",
      "[26/100][1000/3165] Loss_D: 0.0776 Loss_G: 0.8565 D(x): 0.8146 D(G(z)): 0.0426\n",
      "[26/100][1500/3165] Loss_D: 0.0700 Loss_G: 1.0080 D(x): 0.9087 D(G(z)): 0.1157\n",
      "[26/100][2000/3165] Loss_D: 0.0720 Loss_G: 1.0816 D(x): 0.9340 D(G(z)): 0.1511\n",
      "[26/100][2500/3165] Loss_D: 0.0894 Loss_G: 0.9636 D(x): 0.8605 D(G(z)): 0.2007\n",
      "[26/100][3000/3165] Loss_D: 0.1037 Loss_G: 0.6770 D(x): 0.7012 D(G(z)): 0.0617\n",
      "[27/100][0/3165] Loss_D: 0.0941 Loss_G: 0.8142 D(x): 0.7775 D(G(z)): 0.0867\n",
      "[27/100][500/3165] Loss_D: 0.0914 Loss_G: 0.7078 D(x): 0.7293 D(G(z)): 0.0591\n",
      "[27/100][1000/3165] Loss_D: 0.1067 Loss_G: 0.9069 D(x): 0.8298 D(G(z)): 0.1808\n",
      "[27/100][1500/3165] Loss_D: 0.0925 Loss_G: 1.0403 D(x): 0.9085 D(G(z)): 0.1924\n",
      "[27/100][2000/3165] Loss_D: 0.1010 Loss_G: 0.9850 D(x): 0.8793 D(G(z)): 0.1731\n",
      "[27/100][2500/3165] Loss_D: 0.1077 Loss_G: 1.0435 D(x): 0.8917 D(G(z)): 0.1733\n",
      "[27/100][3000/3165] Loss_D: 0.0718 Loss_G: 0.9004 D(x): 0.8246 D(G(z)): 0.1036\n",
      "[28/100][0/3165] Loss_D: 0.0770 Loss_G: 0.8612 D(x): 0.8100 D(G(z)): 0.0782\n",
      "[28/100][500/3165] Loss_D: 0.0760 Loss_G: 0.8211 D(x): 0.7924 D(G(z)): 0.0945\n",
      "[28/100][1000/3165] Loss_D: 0.1003 Loss_G: 0.9308 D(x): 0.8386 D(G(z)): 0.1762\n",
      "[28/100][1500/3165] Loss_D: 0.0843 Loss_G: 0.8101 D(x): 0.7782 D(G(z)): 0.0699\n",
      "[28/100][2000/3165] Loss_D: 0.0936 Loss_G: 0.8007 D(x): 0.7629 D(G(z)): 0.1319\n",
      "[28/100][2500/3165] Loss_D: 0.0601 Loss_G: 0.9839 D(x): 0.8643 D(G(z)): 0.0104\n",
      "[28/100][3000/3165] Loss_D: 0.0442 Loss_G: 1.1346 D(x): 0.9393 D(G(z)): 0.0783\n",
      "[29/100][0/3165] Loss_D: 0.0406 Loss_G: 0.9187 D(x): 0.8490 D(G(z)): -0.0020\n",
      "[29/100][500/3165] Loss_D: 0.0438 Loss_G: 0.9065 D(x): 0.8424 D(G(z)): -0.0004\n",
      "[29/100][1000/3165] Loss_D: 0.0760 Loss_G: 0.9607 D(x): 0.8354 D(G(z)): 0.1731\n",
      "[29/100][1500/3165] Loss_D: 0.0705 Loss_G: 0.7756 D(x): 0.7694 D(G(z)): 0.0555\n",
      "[29/100][2000/3165] Loss_D: 0.0825 Loss_G: 0.7708 D(x): 0.7664 D(G(z)): 0.0871\n",
      "[29/100][2500/3165] Loss_D: 0.0880 Loss_G: 0.6886 D(x): 0.7197 D(G(z)): 0.0499\n",
      "[29/100][3000/3165] Loss_D: 0.0826 Loss_G: 0.8904 D(x): 0.8419 D(G(z)): 0.1273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/100][0/3165] Loss_D: 0.0918 Loss_G: 0.7599 D(x): 0.7571 D(G(z)): 0.0973\n",
      "[30/100][500/3165] Loss_D: 0.0897 Loss_G: 0.6813 D(x): 0.7023 D(G(z)): 0.0340\n",
      "[30/100][1000/3165] Loss_D: 0.0843 Loss_G: 0.8088 D(x): 0.7823 D(G(z)): 0.0836\n",
      "[30/100][1500/3165] Loss_D: 0.0889 Loss_G: 0.8853 D(x): 0.8278 D(G(z)): 0.1365\n",
      "[30/100][2000/3165] Loss_D: 0.0696 Loss_G: 0.8821 D(x): 0.8200 D(G(z)): 0.0610\n",
      "[30/100][2500/3165] Loss_D: 0.1017 Loss_G: 1.2029 D(x): 0.9132 D(G(z)): 0.1338\n",
      "[30/100][3000/3165] Loss_D: 0.0745 Loss_G: 1.1442 D(x): 0.9218 D(G(z)): 0.1292\n",
      "[31/100][0/3165] Loss_D: 0.0636 Loss_G: 0.8458 D(x): 0.7979 D(G(z)): -0.0429\n",
      "[31/100][500/3165] Loss_D: 0.0712 Loss_G: 0.8968 D(x): 0.8307 D(G(z)): 0.1150\n",
      "[31/100][1000/3165] Loss_D: 0.1006 Loss_G: 1.0465 D(x): 0.8866 D(G(z)): 0.1738\n",
      "[31/100][1500/3165] Loss_D: 0.0846 Loss_G: 0.7162 D(x): 0.7280 D(G(z)): 0.1020\n",
      "[31/100][2000/3165] Loss_D: 0.0928 Loss_G: 0.9708 D(x): 0.8604 D(G(z)): 0.2150\n",
      "[31/100][2500/3165] Loss_D: 0.1042 Loss_G: 0.6721 D(x): 0.7014 D(G(z)): -0.0301\n",
      "[31/100][3000/3165] Loss_D: 0.0996 Loss_G: 0.7592 D(x): 0.7564 D(G(z)): 0.0830\n",
      "[32/100][0/3165] Loss_D: 0.0867 Loss_G: 0.8905 D(x): 0.8297 D(G(z)): 0.1520\n",
      "[32/100][500/3165] Loss_D: 0.0961 Loss_G: 0.7010 D(x): 0.7093 D(G(z)): 0.0183\n",
      "[32/100][1000/3165] Loss_D: 0.1125 Loss_G: 0.6465 D(x): 0.6751 D(G(z)): -0.0128\n",
      "[32/100][1500/3165] Loss_D: 0.0949 Loss_G: 0.7350 D(x): 0.7366 D(G(z)): 0.1143\n",
      "[32/100][2000/3165] Loss_D: 0.1140 Loss_G: 0.7830 D(x): 0.7561 D(G(z)): 0.1034\n",
      "[32/100][2500/3165] Loss_D: 0.1095 Loss_G: 1.0403 D(x): 0.9192 D(G(z)): 0.2077\n",
      "[32/100][3000/3165] Loss_D: 0.0690 Loss_G: 0.8883 D(x): 0.8333 D(G(z)): 0.0366\n",
      "[33/100][0/3165] Loss_D: 0.0421 Loss_G: 0.9497 D(x): 0.8571 D(G(z)): 0.0095\n",
      "[33/100][500/3165] Loss_D: 0.0810 Loss_G: 1.0552 D(x): 0.9200 D(G(z)): 0.1341\n",
      "[33/100][1000/3165] Loss_D: 0.0723 Loss_G: 0.8755 D(x): 0.7943 D(G(z)): 0.0125\n",
      "[33/100][1500/3165] Loss_D: 0.0830 Loss_G: 0.9625 D(x): 0.8597 D(G(z)): 0.1846\n",
      "[33/100][2000/3165] Loss_D: 0.0718 Loss_G: 0.8299 D(x): 0.8032 D(G(z)): 0.1256\n",
      "[33/100][2500/3165] Loss_D: 0.0875 Loss_G: 0.8680 D(x): 0.8222 D(G(z)): 0.1664\n",
      "[33/100][3000/3165] Loss_D: 0.0926 Loss_G: 0.6517 D(x): 0.7024 D(G(z)): 0.0630\n",
      "[34/100][0/3165] Loss_D: 0.0961 Loss_G: 0.8284 D(x): 0.7803 D(G(z)): 0.1672\n",
      "[34/100][500/3165] Loss_D: 0.1084 Loss_G: 0.6794 D(x): 0.7077 D(G(z)): 0.0332\n",
      "[34/100][1000/3165] Loss_D: 0.0686 Loss_G: 0.8189 D(x): 0.7885 D(G(z)): 0.0924\n",
      "[34/100][1500/3165] Loss_D: 0.1143 Loss_G: 0.9784 D(x): 0.8760 D(G(z)): 0.2018\n",
      "[34/100][2000/3165] Loss_D: 0.0796 Loss_G: 0.7375 D(x): 0.7307 D(G(z)): 0.0547\n",
      "[34/100][2500/3165] Loss_D: 0.0888 Loss_G: 0.7584 D(x): 0.7517 D(G(z)): 0.0673\n",
      "[34/100][3000/3165] Loss_D: 0.0921 Loss_G: 1.0534 D(x): 0.8918 D(G(z)): 0.1880\n",
      "[35/100][0/3165] Loss_D: 0.1033 Loss_G: 0.6382 D(x): 0.6969 D(G(z)): 0.0485\n",
      "[35/100][500/3165] Loss_D: 0.0896 Loss_G: 0.9392 D(x): 0.8372 D(G(z)): 0.1767\n",
      "[35/100][1000/3165] Loss_D: 0.0664 Loss_G: 1.0506 D(x): 0.8863 D(G(z)): 0.0730\n",
      "[35/100][1500/3165] Loss_D: 0.0522 Loss_G: 1.0400 D(x): 0.9091 D(G(z)): 0.1221\n",
      "[35/100][2000/3165] Loss_D: 0.0827 Loss_G: 0.9149 D(x): 0.8442 D(G(z)): 0.1483\n",
      "[35/100][2500/3165] Loss_D: 0.0787 Loss_G: 0.7305 D(x): 0.7283 D(G(z)): 0.0622\n",
      "[35/100][3000/3165] Loss_D: 0.0939 Loss_G: 0.8812 D(x): 0.8140 D(G(z)): 0.1474\n",
      "[36/100][0/3165] Loss_D: 0.0751 Loss_G: 0.8490 D(x): 0.8013 D(G(z)): 0.1393\n",
      "[36/100][500/3165] Loss_D: 0.0892 Loss_G: 0.9839 D(x): 0.8876 D(G(z)): 0.1809\n",
      "[36/100][1000/3165] Loss_D: 0.0859 Loss_G: 0.8691 D(x): 0.8043 D(G(z)): 0.1587\n",
      "[36/100][1500/3165] Loss_D: 0.1016 Loss_G: 0.6340 D(x): 0.6815 D(G(z)): -0.0140\n",
      "[36/100][2000/3165] Loss_D: 0.0737 Loss_G: 0.8267 D(x): 0.7891 D(G(z)): 0.1076\n",
      "[36/100][2500/3165] Loss_D: 0.0618 Loss_G: 0.9549 D(x): 0.8532 D(G(z)): 0.0786\n",
      "[36/100][3000/3165] Loss_D: 0.1118 Loss_G: 0.6091 D(x): 0.6687 D(G(z)): 0.0241\n",
      "[37/100][0/3165] Loss_D: 0.0866 Loss_G: 0.7584 D(x): 0.7355 D(G(z)): 0.0351\n",
      "[37/100][500/3165] Loss_D: 0.0656 Loss_G: 0.9480 D(x): 0.8527 D(G(z)): 0.1412\n",
      "[37/100][1000/3165] Loss_D: 0.0671 Loss_G: 0.8310 D(x): 0.7932 D(G(z)): 0.0827\n",
      "[37/100][1500/3165] Loss_D: 0.0518 Loss_G: 0.8479 D(x): 0.7913 D(G(z)): -0.0165\n",
      "[37/100][2000/3165] Loss_D: 0.0594 Loss_G: 1.1026 D(x): 0.9296 D(G(z)): 0.1061\n",
      "[37/100][2500/3165] Loss_D: 0.0783 Loss_G: 0.7445 D(x): 0.7537 D(G(z)): 0.0974\n",
      "[37/100][3000/3165] Loss_D: 0.0973 Loss_G: 1.0376 D(x): 0.9003 D(G(z)): 0.1978\n",
      "[38/100][0/3165] Loss_D: 0.0725 Loss_G: 0.9147 D(x): 0.8485 D(G(z)): 0.1195\n",
      "[38/100][500/3165] Loss_D: 0.0784 Loss_G: 0.8731 D(x): 0.8236 D(G(z)): 0.1687\n",
      "[38/100][1000/3165] Loss_D: 0.0867 Loss_G: 0.6294 D(x): 0.6813 D(G(z)): -0.0041\n",
      "[38/100][1500/3165] Loss_D: 0.0844 Loss_G: 0.8458 D(x): 0.7995 D(G(z)): 0.1496\n",
      "[38/100][2000/3165] Loss_D: 0.0639 Loss_G: 0.8315 D(x): 0.7969 D(G(z)): 0.0779\n",
      "[38/100][2500/3165] Loss_D: 0.0876 Loss_G: 0.9789 D(x): 0.8870 D(G(z)): 0.1925\n",
      "[38/100][3000/3165] Loss_D: 0.0653 Loss_G: 0.8702 D(x): 0.8188 D(G(z)): 0.1301\n",
      "[39/100][0/3165] Loss_D: 0.0972 Loss_G: 0.6664 D(x): 0.7206 D(G(z)): 0.0300\n",
      "[39/100][500/3165] Loss_D: 0.1198 Loss_G: 1.1025 D(x): 0.9198 D(G(z)): 0.2398\n",
      "[39/100][1000/3165] Loss_D: 0.0695 Loss_G: 0.9492 D(x): 0.8560 D(G(z)): 0.1019\n",
      "[39/100][1500/3165] Loss_D: 0.1093 Loss_G: 1.0036 D(x): 0.8888 D(G(z)): 0.2512\n",
      "[39/100][2000/3165] Loss_D: 0.0705 Loss_G: 0.8124 D(x): 0.7786 D(G(z)): 0.1146\n",
      "[39/100][2500/3165] Loss_D: 0.0991 Loss_G: 1.1152 D(x): 0.9041 D(G(z)): 0.1880\n",
      "[39/100][3000/3165] Loss_D: 0.0714 Loss_G: 0.8526 D(x): 0.8106 D(G(z)): 0.1049\n",
      "[40/100][0/3165] Loss_D: 0.0674 Loss_G: 0.8248 D(x): 0.7973 D(G(z)): 0.0847\n",
      "[40/100][500/3165] Loss_D: 0.0738 Loss_G: 0.8923 D(x): 0.8425 D(G(z)): 0.1547\n",
      "[40/100][1000/3165] Loss_D: 0.0659 Loss_G: 0.8772 D(x): 0.8135 D(G(z)): 0.0678\n",
      "[40/100][1500/3165] Loss_D: 0.0658 Loss_G: 0.8673 D(x): 0.8188 D(G(z)): 0.0454\n",
      "[40/100][2000/3165] Loss_D: 0.0664 Loss_G: 1.0461 D(x): 0.8940 D(G(z)): 0.1228\n",
      "[40/100][2500/3165] Loss_D: 0.0574 Loss_G: 0.8952 D(x): 0.8304 D(G(z)): 0.0262\n",
      "[40/100][3000/3165] Loss_D: 0.1012 Loss_G: 0.6891 D(x): 0.7115 D(G(z)): 0.0509\n",
      "[41/100][0/3165] Loss_D: 0.0996 Loss_G: 0.6268 D(x): 0.6808 D(G(z)): 0.0520\n",
      "[41/100][500/3165] Loss_D: 0.0693 Loss_G: 0.7854 D(x): 0.7620 D(G(z)): 0.0655\n",
      "[41/100][1000/3165] Loss_D: 0.0871 Loss_G: 0.7341 D(x): 0.7440 D(G(z)): 0.0569\n",
      "[41/100][1500/3165] Loss_D: 0.0880 Loss_G: 0.7614 D(x): 0.7511 D(G(z)): 0.0641\n",
      "[41/100][2000/3165] Loss_D: 0.0684 Loss_G: 0.8739 D(x): 0.8076 D(G(z)): 0.0914\n",
      "[41/100][2500/3165] Loss_D: 0.0941 Loss_G: 0.6887 D(x): 0.7000 D(G(z)): -0.0450\n",
      "[41/100][3000/3165] Loss_D: 0.1004 Loss_G: 0.9815 D(x): 0.8769 D(G(z)): 0.2049\n",
      "[42/100][0/3165] Loss_D: 0.0899 Loss_G: 0.9980 D(x): 0.8872 D(G(z)): 0.1682\n",
      "[42/100][500/3165] Loss_D: 0.0611 Loss_G: 1.0562 D(x): 0.9022 D(G(z)): 0.1125\n",
      "[42/100][1000/3165] Loss_D: 0.0692 Loss_G: 0.8304 D(x): 0.7849 D(G(z)): 0.0611\n",
      "[42/100][1500/3165] Loss_D: 0.1014 Loss_G: 0.9540 D(x): 0.8489 D(G(z)): 0.1991\n",
      "[42/100][2000/3165] Loss_D: 0.0967 Loss_G: 0.7290 D(x): 0.7301 D(G(z)): 0.0881\n",
      "[42/100][2500/3165] Loss_D: 0.0929 Loss_G: 0.8525 D(x): 0.7980 D(G(z)): 0.1435\n",
      "[42/100][3000/3165] Loss_D: 0.0894 Loss_G: 0.9517 D(x): 0.8817 D(G(z)): 0.2001\n",
      "[43/100][0/3165] Loss_D: 0.0737 Loss_G: 0.8903 D(x): 0.8342 D(G(z)): 0.1063\n",
      "[43/100][500/3165] Loss_D: 0.1082 Loss_G: 0.6581 D(x): 0.6933 D(G(z)): -0.0171\n",
      "[43/100][1000/3165] Loss_D: 0.0752 Loss_G: 0.8926 D(x): 0.8282 D(G(z)): 0.0909\n",
      "[43/100][1500/3165] Loss_D: 0.1014 Loss_G: 0.6148 D(x): 0.6852 D(G(z)): -0.0382\n",
      "[43/100][2000/3165] Loss_D: 0.0752 Loss_G: 0.7508 D(x): 0.7584 D(G(z)): 0.0421\n",
      "[43/100][2500/3165] Loss_D: 0.0699 Loss_G: 0.8525 D(x): 0.8023 D(G(z)): 0.0882\n",
      "[43/100][3000/3165] Loss_D: 0.0868 Loss_G: 0.9794 D(x): 0.8851 D(G(z)): 0.1621\n",
      "[44/100][0/3165] Loss_D: 0.0648 Loss_G: 0.9065 D(x): 0.8441 D(G(z)): 0.1043\n",
      "[44/100][500/3165] Loss_D: 0.0923 Loss_G: 1.2759 D(x): 1.0213 D(G(z)): 0.1790\n",
      "[44/100][1000/3165] Loss_D: 0.0613 Loss_G: 0.8267 D(x): 0.8045 D(G(z)): 0.0482\n",
      "[44/100][1500/3165] Loss_D: 0.0795 Loss_G: 0.7893 D(x): 0.7838 D(G(z)): 0.0971\n",
      "[44/100][2000/3165] Loss_D: 0.1008 Loss_G: 1.1296 D(x): 0.9185 D(G(z)): 0.1881\n",
      "[44/100][2500/3165] Loss_D: 0.0886 Loss_G: 0.9206 D(x): 0.8319 D(G(z)): 0.1390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44/100][3000/3165] Loss_D: 0.0650 Loss_G: 0.9215 D(x): 0.8489 D(G(z)): 0.0686\n",
      "[45/100][0/3165] Loss_D: 0.0603 Loss_G: 1.0598 D(x): 0.9171 D(G(z)): 0.1427\n",
      "[45/100][500/3165] Loss_D: 0.0800 Loss_G: 0.9534 D(x): 0.8319 D(G(z)): 0.1238\n",
      "[45/100][1000/3165] Loss_D: 0.0347 Loss_G: 0.8972 D(x): 0.8337 D(G(z)): 0.0168\n",
      "[45/100][1500/3165] Loss_D: 0.0659 Loss_G: 0.8714 D(x): 0.8123 D(G(z)): 0.0435\n",
      "[45/100][2000/3165] Loss_D: 0.1407 Loss_G: 1.3108 D(x): 0.9780 D(G(z)): 0.2760\n",
      "[45/100][2500/3165] Loss_D: 0.0728 Loss_G: 0.8498 D(x): 0.8075 D(G(z)): 0.0663\n"
     ]
    }
   ],
   "source": [
    "niter = 100\n",
    "d_fake_save = None\n",
    "for epoch in range(niter):\n",
    "    schedulerD.step()\n",
    "    schedulerG.step()\n",
    "#     if epoch < 17:\n",
    "#         continue\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # train D\n",
    "        netD.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "\n",
    "        real = real_cpu.cuda()\n",
    "        input.resize_as_(real).copy_(real)\n",
    "        \n",
    "        inputv = Variable(input)\n",
    "        inputv = add_noise(inputv, d_fake_save)\n",
    "        feature = vggnet(inputv)\n",
    "        \n",
    "        d_real = netD(inputv, feature)\n",
    "        d_real_mean = d_real.data.mean()\n",
    "        \n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev, feature)\n",
    "        \n",
    "        d_fake_save = d_fake = netD(fake.detach(), feature)\n",
    "        d_fake_mean = d_fake.data.mean()\n",
    "        \n",
    "        loss_d = criterion(d_real, label_real_smooth) + criterion(d_fake, label_fake)\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # train G\n",
    "        netG.zero_grad()\n",
    "        d_fake = netD(fake, feature)\n",
    "        loss_g = criterion(d_fake, label_real.detach())\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%500 == 0:\n",
    "            fake = netG(fixed_noise, feature)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03dstep_%04d.png' % (outf, epoch, i),\n",
    "                    normalize=True)\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f'\n",
    "                  % (epoch, niter, i, len(dataloader),\n",
    "                     loss_d.data[0], loss_g.data[0], d_real_mean, d_fake_mean))\n",
    "            \n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%03d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%03d.pth' % (outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

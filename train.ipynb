{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network as layer\n",
    "import torch.nn as nn\n",
    "\n",
    "nc = 3\n",
    "nz = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "lr     = 0.0002\n",
    "beta1  = 0.5   \n",
    "beta2  = 0.99     \n",
    "imageSize = 128\n",
    "batchSize = 32\n",
    "\n",
    "outf = \"./celeba_result/\"\n",
    "des_dir = \"./celeba/\"\n",
    "\n",
    "dataset = dset.ImageFolder(root=des_dir,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.CenterCrop(224),\n",
    "                               transforms.Resize(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size= batchSize,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers import *\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def make_dense(self, k_in, k_growth, n, options):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(Dense(layer.conv(k_in, k_growth, 3, 1, 1, **options)))\n",
    "            k_in += k_growth\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        options = {'leaky':True, 'bn':False, 'wn':True, 'pixel':True}\n",
    "        layers = []\n",
    "        k_growth = 12\n",
    "        layers.append(layer.conv(nz, 256, 4, 1, 3, **options))\n",
    "        \n",
    "        layers.append(self.make_dense(256, k_growth, 3, options))\n",
    "        layers.append(layer.conv(292, 256, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(256, 256, 3, 1, 1, **options))\n",
    "        # 4 x 4\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(self.make_dense(256, k_growth, 3, options))\n",
    "        layers.append(layer.conv(292, 256, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(256, 256, 3, 1, 1, **options))\n",
    "        # 8 x 8\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(self.make_dense(256, k_growth, 3, options))\n",
    "        layers.append(layer.conv(292, 256, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(256, 256, 3, 1, 1, **options))\n",
    "        # 16 x 16\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(self.make_dense(256, k_growth, 3, options))\n",
    "        layers.append(layer.conv(292, 128, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(128, 128, 3, 1, 1, **options))\n",
    "        # 32 x 32\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(self.make_dense(128, k_growth, 3, options))\n",
    "        layers.append(layer.conv(164, 64, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(64, 64, 3, 1, 1, **options))\n",
    "        # 64 x 64\n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers.append(self.make_dense(64, k_growth, 3, options))\n",
    "        layers.append(layer.conv(100, 32, 1, 1, 0, **options))\n",
    "        layers.append(layer.conv(32, 32, 3, 1, 1, **options))\n",
    "        # 128 x 128\n",
    "        layers.append(layer.conv(32, nc, 1, 1, 0, leaky=True, bn=False, wn=True, pixel=True, only=True))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "netG = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def make_dense(self, k_in, k_growth, n, options):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            layers.append(Dense(layer.conv(k_in, k_growth, 3, 1, 1, **options)))\n",
    "            k_in += k_growth\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        options = {'leaky':True, 'bn':False, 'wn':True, 'pixel':False, 'gdrop':True}\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(layer.conv(nc, 16, 1, 1, 0, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(layer.conv(16, 16, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(layer.conv(16, 32, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2))\n",
    "        \n",
    "        layers.append(layer.conv(32, 32, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(layer.conv(32, 64, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2))\n",
    "        \n",
    "        layers.append(layer.conv(64, 64, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(layer.conv(64, 128, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2))\n",
    "        \n",
    "        layers.append(layer.conv(128, 128, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(layer.conv(128, 256, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2))\n",
    "        \n",
    "        layers.append(layer.conv(256, 256, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(layer.conv(256, 512, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2))\n",
    "        \n",
    "        layers.append(minibatch_std_concat_layer())\n",
    "        layers.append(layer.conv(513, 512, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(layer.conv(512, 512, 4, 1, 0, leaky=True, bn=False, wn=True, pixel=False, gdrop=True))\n",
    "        layers.append(layer.linear(512, 1, sig=False, wn=True))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "netD = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, imageSize,imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "label_real = torch.FloatTensor(batchSize)\n",
    "label_fake = torch.FloatTensor(batchSize)\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "criterion.cuda()\n",
    "input, label_real, label_fake = input.cuda(), label_real.cuda(), label_fake.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "label_real.resize_(batchSize, 1).fill_(1)\n",
    "label_fake.resize_(batchSize, 1).fill_(0)\n",
    "label_real = Variable(label_real)\n",
    "label_fake = Variable(label_fake)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD.load_state_dict(torch.load(outf + 'netD_epoch_042.pth'))\n",
    "netG.load_state_dict(torch.load(outf + 'netG_epoch_042.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.00005, betas=(beta1, beta2))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0001, betas=(beta1, beta2))\n",
    "schedulerD = optim.lr_scheduler.MultiStepLR(optimizerD, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "schedulerG = optim.lr_scheduler.MultiStepLR(optimizerG, milestones=[4, 7, 11, 17], gamma=0.87)\n",
    "result_dict = {}\n",
    "loss_D,loss_G,score_D,score_G1,score_G2 = [],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "_d_ = None\n",
    "def add_noise(x, d_fake):\n",
    "    global _d_\n",
    "    if _d_ is not None:\n",
    "        _d_ = _d_ * 0.9 + torch.mean(d_fake).data[0] * 0.1\n",
    "        strength = 0.2 * max(0, _d_ - 0.5)**2\n",
    "        z = np.random.randn(*x.size()).astype(np.float32) * strength\n",
    "        z = Variable(torch.from_numpy(z)).cuda()\n",
    "        return x + z\n",
    "    else:\n",
    "        _d_ = 0.0\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43/100][0/6330] Loss_D: 0.1647 Loss_G: 0.7477 D(x): 0.8466 D(G(z)): 0.2126\n",
      "[43/100][250/6330] Loss_D: 0.3390 Loss_G: 0.4950 D(x): 0.6796 D(G(z)): 0.3228\n",
      "[43/100][500/6330] Loss_D: 0.3881 Loss_G: 0.5182 D(x): 0.6463 D(G(z)): 0.3603\n",
      "[43/100][750/6330] Loss_D: 0.3216 Loss_G: 0.4454 D(x): 0.7265 D(G(z)): 0.3679\n",
      "[43/100][1000/6330] Loss_D: 0.3811 Loss_G: 0.4784 D(x): 0.5347 D(G(z)): 0.2873\n",
      "[43/100][1250/6330] Loss_D: 0.2865 Loss_G: 0.5600 D(x): 0.8613 D(G(z)): 0.3481\n",
      "[43/100][1500/6330] Loss_D: 0.2970 Loss_G: 0.6008 D(x): 0.6394 D(G(z)): 0.2188\n",
      "[43/100][1750/6330] Loss_D: 0.3311 Loss_G: 0.5029 D(x): 0.7469 D(G(z)): 0.3473\n",
      "[43/100][2000/6330] Loss_D: 0.2757 Loss_G: 0.5429 D(x): 0.7649 D(G(z)): 0.2958\n",
      "[43/100][2250/6330] Loss_D: 0.2320 Loss_G: 0.5457 D(x): 0.7595 D(G(z)): 0.2908\n",
      "[43/100][2500/6330] Loss_D: 0.3165 Loss_G: 0.6064 D(x): 0.6812 D(G(z)): 0.2724\n",
      "[43/100][2750/6330] Loss_D: 0.3073 Loss_G: 0.8576 D(x): 0.8316 D(G(z)): 0.2490\n",
      "[43/100][3000/6330] Loss_D: 0.3311 Loss_G: 0.6148 D(x): 0.5940 D(G(z)): 0.2002\n",
      "[43/100][3250/6330] Loss_D: 0.2185 Loss_G: 0.5822 D(x): 0.7698 D(G(z)): 0.2878\n",
      "[43/100][3500/6330] Loss_D: 0.2510 Loss_G: 0.5778 D(x): 0.6873 D(G(z)): 0.2459\n",
      "[43/100][3750/6330] Loss_D: 0.2404 Loss_G: 0.6209 D(x): 0.7398 D(G(z)): 0.2460\n",
      "[43/100][4000/6330] Loss_D: 0.3529 Loss_G: 0.5403 D(x): 0.6196 D(G(z)): 0.2785\n",
      "[43/100][4250/6330] Loss_D: 0.2879 Loss_G: 0.5287 D(x): 0.7101 D(G(z)): 0.3246\n",
      "[43/100][4500/6330] Loss_D: 0.3199 Loss_G: 0.4666 D(x): 0.6968 D(G(z)): 0.3569\n",
      "[43/100][4750/6330] Loss_D: 0.2482 Loss_G: 0.6528 D(x): 0.9077 D(G(z)): 0.3278\n",
      "[43/100][5000/6330] Loss_D: 0.3181 Loss_G: 0.5998 D(x): 0.6288 D(G(z)): 0.2203\n",
      "[43/100][5250/6330] Loss_D: 0.2944 Loss_G: 0.4870 D(x): 0.7197 D(G(z)): 0.3546\n",
      "[43/100][5500/6330] Loss_D: 0.2162 Loss_G: 0.5926 D(x): 0.7968 D(G(z)): 0.2687\n",
      "[43/100][5750/6330] Loss_D: 0.2619 Loss_G: 0.5476 D(x): 0.7228 D(G(z)): 0.3153\n",
      "[43/100][6000/6330] Loss_D: 0.3356 Loss_G: 0.5524 D(x): 0.6280 D(G(z)): 0.3238\n",
      "[43/100][6250/6330] Loss_D: 0.2816 Loss_G: 0.5142 D(x): 0.7123 D(G(z)): 0.3178\n",
      "[44/100][0/6330] Loss_D: 0.2241 Loss_G: 0.6262 D(x): 0.7299 D(G(z)): 0.1988\n",
      "[44/100][250/6330] Loss_D: 0.2391 Loss_G: 0.5087 D(x): 0.7789 D(G(z)): 0.3245\n",
      "[44/100][500/6330] Loss_D: 0.3334 Loss_G: 0.5633 D(x): 0.5769 D(G(z)): 0.2205\n",
      "[44/100][750/6330] Loss_D: 0.1975 Loss_G: 0.6853 D(x): 0.8015 D(G(z)): 0.1705\n",
      "[44/100][1000/6330] Loss_D: 0.2441 Loss_G: 0.6235 D(x): 0.7532 D(G(z)): 0.2454\n",
      "[44/100][1250/6330] Loss_D: 0.2813 Loss_G: 0.5372 D(x): 0.7283 D(G(z)): 0.2931\n",
      "[44/100][1500/6330] Loss_D: 0.1843 Loss_G: 0.6708 D(x): 0.8358 D(G(z)): 0.2564\n",
      "[44/100][1750/6330] Loss_D: 0.2530 Loss_G: 0.5389 D(x): 0.7394 D(G(z)): 0.2900\n",
      "[44/100][2000/6330] Loss_D: 0.3924 Loss_G: 0.4945 D(x): 0.6471 D(G(z)): 0.3232\n",
      "[44/100][2250/6330] Loss_D: 0.2185 Loss_G: 0.7500 D(x): 0.7436 D(G(z)): 0.1666\n",
      "[44/100][2500/6330] Loss_D: 0.1877 Loss_G: 0.7630 D(x): 0.7908 D(G(z)): 0.1815\n",
      "[44/100][2750/6330] Loss_D: 0.2702 Loss_G: 0.4776 D(x): 0.6347 D(G(z)): 0.2875\n",
      "[44/100][3000/6330] Loss_D: 0.2789 Loss_G: 0.5443 D(x): 0.7203 D(G(z)): 0.2882\n",
      "[44/100][3250/6330] Loss_D: 0.2440 Loss_G: 0.6145 D(x): 0.7949 D(G(z)): 0.2754\n",
      "[44/100][3500/6330] Loss_D: 0.2231 Loss_G: 0.6184 D(x): 0.7009 D(G(z)): 0.1833\n",
      "[44/100][3750/6330] Loss_D: 0.2808 Loss_G: 0.6140 D(x): 0.7532 D(G(z)): 0.2559\n",
      "[44/100][4000/6330] Loss_D: 0.2600 Loss_G: 0.6638 D(x): 0.6354 D(G(z)): 0.1613\n",
      "[44/100][4250/6330] Loss_D: 0.3367 Loss_G: 0.4780 D(x): 0.6749 D(G(z)): 0.3465\n",
      "[44/100][4500/6330] Loss_D: 0.2338 Loss_G: 0.6590 D(x): 0.7167 D(G(z)): 0.2381\n",
      "[44/100][4750/6330] Loss_D: 0.2944 Loss_G: 0.5443 D(x): 0.7744 D(G(z)): 0.3024\n",
      "[44/100][5000/6330] Loss_D: 0.3470 Loss_G: 0.4849 D(x): 0.6281 D(G(z)): 0.2923\n",
      "[44/100][5250/6330] Loss_D: 0.3850 Loss_G: 0.4625 D(x): 0.5722 D(G(z)): 0.3350\n",
      "[44/100][5500/6330] Loss_D: 0.2474 Loss_G: 0.6292 D(x): 0.8299 D(G(z)): 0.3493\n",
      "[44/100][5750/6330] Loss_D: 0.2242 Loss_G: 0.5807 D(x): 0.7670 D(G(z)): 0.2899\n",
      "[44/100][6000/6330] Loss_D: 0.2461 Loss_G: 0.5296 D(x): 0.6931 D(G(z)): 0.2535\n",
      "[44/100][6250/6330] Loss_D: 0.1783 Loss_G: 0.6393 D(x): 0.7280 D(G(z)): 0.1763\n",
      "[45/100][0/6330] Loss_D: 0.1424 Loss_G: 0.7151 D(x): 0.8015 D(G(z)): 0.1800\n",
      "[45/100][250/6330] Loss_D: 0.2579 Loss_G: 0.5693 D(x): 0.6868 D(G(z)): 0.2393\n",
      "[45/100][500/6330] Loss_D: 0.3846 Loss_G: 0.4995 D(x): 0.5807 D(G(z)): 0.2836\n",
      "[45/100][750/6330] Loss_D: 0.2381 Loss_G: 0.5252 D(x): 0.7610 D(G(z)): 0.3333\n",
      "[45/100][1000/6330] Loss_D: 0.2692 Loss_G: 0.5396 D(x): 0.7531 D(G(z)): 0.2967\n",
      "[45/100][1250/6330] Loss_D: 0.2888 Loss_G: 0.6399 D(x): 0.7622 D(G(z)): 0.2613\n",
      "[45/100][1500/6330] Loss_D: 0.2345 Loss_G: 0.6002 D(x): 0.8115 D(G(z)): 0.2697\n",
      "[45/100][1750/6330] Loss_D: 0.2690 Loss_G: 0.5029 D(x): 0.7709 D(G(z)): 0.3545\n",
      "[45/100][2000/6330] Loss_D: 0.3922 Loss_G: 0.4861 D(x): 0.5673 D(G(z)): 0.3166\n",
      "[45/100][2250/6330] Loss_D: 0.2305 Loss_G: 0.6354 D(x): 0.6915 D(G(z)): 0.1951\n",
      "[45/100][2500/6330] Loss_D: 0.2499 Loss_G: 0.5179 D(x): 0.6808 D(G(z)): 0.2855\n",
      "[45/100][2750/6330] Loss_D: 0.3565 Loss_G: 0.5058 D(x): 0.6189 D(G(z)): 0.3067\n",
      "[45/100][3000/6330] Loss_D: 0.3205 Loss_G: 0.4660 D(x): 0.7084 D(G(z)): 0.3448\n",
      "[45/100][3250/6330] Loss_D: 0.3929 Loss_G: 0.4653 D(x): 0.7063 D(G(z)): 0.3963\n",
      "[45/100][3500/6330] Loss_D: 0.2534 Loss_G: 0.5510 D(x): 0.7772 D(G(z)): 0.3584\n",
      "[45/100][3750/6330] Loss_D: 0.2592 Loss_G: 0.5700 D(x): 0.7369 D(G(z)): 0.3438\n",
      "[45/100][4000/6330] Loss_D: 0.3179 Loss_G: 0.5209 D(x): 0.7940 D(G(z)): 0.3768\n",
      "[45/100][4250/6330] Loss_D: 0.2442 Loss_G: 0.5748 D(x): 0.6540 D(G(z)): 0.1995\n",
      "[45/100][4500/6330] Loss_D: 0.2801 Loss_G: 0.4425 D(x): 0.7017 D(G(z)): 0.3226\n",
      "[45/100][4750/6330] Loss_D: 0.2881 Loss_G: 0.4658 D(x): 0.7071 D(G(z)): 0.3029\n",
      "[45/100][5000/6330] Loss_D: 0.2536 Loss_G: 0.5147 D(x): 0.8205 D(G(z)): 0.3301\n",
      "[45/100][5250/6330] Loss_D: 0.2539 Loss_G: 0.5335 D(x): 0.7076 D(G(z)): 0.2793\n",
      "[45/100][5500/6330] Loss_D: 0.2651 Loss_G: 0.5341 D(x): 0.7335 D(G(z)): 0.2296\n",
      "[45/100][5750/6330] Loss_D: 0.4565 Loss_G: 0.4204 D(x): 0.5770 D(G(z)): 0.3938\n",
      "[45/100][6000/6330] Loss_D: 0.4234 Loss_G: 0.4854 D(x): 0.6037 D(G(z)): 0.3166\n",
      "[45/100][6250/6330] Loss_D: 0.2735 Loss_G: 0.5933 D(x): 0.6892 D(G(z)): 0.2176\n",
      "[46/100][0/6330] Loss_D: 0.3246 Loss_G: 0.6336 D(x): 0.5720 D(G(z)): 0.1674\n",
      "[46/100][250/6330] Loss_D: 0.1914 Loss_G: 0.6825 D(x): 0.8507 D(G(z)): 0.1866\n",
      "[46/100][500/6330] Loss_D: 0.2021 Loss_G: 0.8363 D(x): 0.9758 D(G(z)): 0.2459\n",
      "[46/100][750/6330] Loss_D: 0.3220 Loss_G: 0.5217 D(x): 0.5914 D(G(z)): 0.2576\n",
      "[46/100][1000/6330] Loss_D: 0.2270 Loss_G: 0.5537 D(x): 0.7114 D(G(z)): 0.2892\n",
      "[46/100][1250/6330] Loss_D: 0.2276 Loss_G: 0.5744 D(x): 0.8116 D(G(z)): 0.3126\n",
      "[46/100][1500/6330] Loss_D: 0.3353 Loss_G: 0.5206 D(x): 0.6240 D(G(z)): 0.3048\n",
      "[46/100][1750/6330] Loss_D: 0.2914 Loss_G: 0.5050 D(x): 0.6786 D(G(z)): 0.3062\n",
      "[46/100][2000/6330] Loss_D: 0.2628 Loss_G: 0.6471 D(x): 0.6645 D(G(z)): 0.1931\n",
      "[46/100][2250/6330] Loss_D: 0.2988 Loss_G: 0.5485 D(x): 0.6226 D(G(z)): 0.2365\n",
      "[46/100][2500/6330] Loss_D: 0.3066 Loss_G: 0.5057 D(x): 0.6913 D(G(z)): 0.3141\n",
      "[46/100][2750/6330] Loss_D: 0.3561 Loss_G: 0.4493 D(x): 0.6069 D(G(z)): 0.3157\n",
      "[46/100][3000/6330] Loss_D: 0.4005 Loss_G: 0.3856 D(x): 0.5977 D(G(z)): 0.3863\n",
      "[46/100][3250/6330] Loss_D: 0.4556 Loss_G: 0.5643 D(x): 0.4636 D(G(z)): 0.2066\n"
     ]
    }
   ],
   "source": [
    "niter = 100\n",
    "d_fake_save = None\n",
    "for epoch in range(niter):\n",
    "    schedulerD.step()\n",
    "    schedulerG.step()\n",
    "    if epoch < 43:\n",
    "        continue\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        # train D\n",
    "        netD.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "\n",
    "        real_cpu = real_cpu.cuda()\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "\n",
    "        inputv = Variable(input)\n",
    "        inputv = add_noise(inputv, d_fake_save)\n",
    "        \n",
    "        d_real = netD(inputv)\n",
    "        d_real_mean = d_real.data.mean()\n",
    "        \n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        \n",
    "        d_fake_save = d_fake = netD(fake.detach())\n",
    "        d_fake_mean = d_fake.data.mean()\n",
    "        \n",
    "        loss_d = criterion(d_real, label_real) + criterion(d_fake, label_fake)\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # train G\n",
    "        netG.zero_grad()\n",
    "        d_fake = netD(fake)\n",
    "        loss_g = criterion(d_fake, label_real.detach())\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%250 == 0:\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03dstep_%04d.png' % (outf, epoch, i),\n",
    "                    normalize=True)\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f'\n",
    "                  % (epoch, niter, i, len(dataloader),\n",
    "                     loss_d.data[0], loss_g.data[0], d_real_mean, d_fake_mean))\n",
    "            \n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%03d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%03d.pth' % (outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

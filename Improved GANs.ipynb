{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network as layer\n",
    "import torch.nn as nn\n",
    "\n",
    "nc = 3\n",
    "nz = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "lr     = 0.0002\n",
    "beta1  = 0.5      \n",
    "imageSize = 64    \n",
    "batchSize = 64    \n",
    "\n",
    "outf = \"./celeba_result/\"\n",
    "des_dir = \"./celeba_crop/\"\n",
    "\n",
    "dataset = dset.ImageFolder(root=des_dir,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size= batchSize,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        layers = []\n",
    "        layers = layer.deconv(layers, nz, 512, 4, 1, 3, leaky=True, bn=False, wn=True, pixel=True)\n",
    "        layers = layer.deconv(layers, 512, 512, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=True)\n",
    "        \n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers = layer.deconv(layers, 512, 256, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=True)\n",
    "        layers = layer.deconv(layers, 256, 256, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=True)\n",
    "        \n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers = layer.deconv(layers, 256, 128, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=True)\n",
    "        layers = layer.deconv(layers, 128, 128, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=True)\n",
    "        \n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers = layer.deconv(layers, 128, 64, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=True)\n",
    "        layers = layer.deconv(layers, 64, 64, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=True)\n",
    "        \n",
    "        layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        layers = layer.deconv(layers, 64, 32, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=True)\n",
    "        layers = layer.deconv(layers, 32, 32, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=True)\n",
    "    \n",
    "        layers = layer.deconv(layers, 32, nc, 1, 1, 0, leaky=True, bn=False, wn=True, pixel=True, only=True)\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "netG = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers import *\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        layers = []\n",
    "        layers = layer.conv(layers, nc, 32, 1, 1, 0, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers = layer.conv(layers, 32, 32, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers = layer.conv(layers, 32, 64, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2))\n",
    "        \n",
    "        layers = layer.conv(layers, 64, 64, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers = layer.conv(layers, 64, 128, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2))\n",
    "        \n",
    "        layers = layer.conv(layers, 128, 128, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers = layer.conv(layers, 128, 256, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2))\n",
    "        \n",
    "        layers = layer.conv(layers, 256, 256, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers = layer.conv(layers, 256, 512, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers.append(nn.AvgPool2d(kernel_size=2))\n",
    "        \n",
    "        layers.append(minibatch_std_concat_layer())\n",
    "        layers = layer.conv(layers, 513, 512, 3, 1, 1, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers = layer.conv(layers, 512, 512, 4, 1, 0, leaky=True, bn=False, wn=True, pixel=False, gdrop=True)\n",
    "        layers = layer.linear(layers, 512, 1, sig=False, wn=True)\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "netD = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, imageSize,imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "label_real = torch.FloatTensor(batchSize)\n",
    "label_fake = torch.FloatTensor(batchSize)\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "criterion.cuda()\n",
    "input, label_real, label_fake = input.cuda(), label_real.cuda(), label_fake.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "label_real.resize_(batchSize).fill_(1)\n",
    "label_fake.resize_(batchSize).fill_(0)\n",
    "label_real = Variable(label_real)\n",
    "label_fake = Variable(label_fake)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "result_dict = {}\n",
    "loss_D,loss_G,score_D,score_G1,score_G2 = [],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 100\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        # train D\n",
    "        netD.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "\n",
    "        real_cpu = real_cpu.cuda()\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "\n",
    "        inputv = Variable(input)\n",
    "\n",
    "        d_real = netD(inputv)\n",
    "        d_real_mean = d_real.data.mean()\n",
    "        \n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        \n",
    "        d_fake = netD(fake.detach())\n",
    "        d_fake_mean = d_fake.data.mean()\n",
    "        \n",
    "        loss_d = criterion(d_real, label_real) + criterion(d_fake, label_fake)\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # train G\n",
    "        netG.zero_grad()\n",
    "        d_fake = netD(fake)\n",
    "        loss_g = criterion(d_fake, label_real.detach())\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i%250 == 0:\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03dstep_%04d.png' % (outf, epoch, i),\n",
    "                    normalize=True)\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f'\n",
    "                  % (epoch, niter, i, len(dataloader),\n",
    "                     loss_d.data[0], loss_g.data[0], d_real_mean, d_fake_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

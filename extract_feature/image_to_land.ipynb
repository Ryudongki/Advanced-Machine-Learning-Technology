{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "from skimage import io\n",
    "from torch import FloatTensor\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_model = \"shape_predictor_68_face_landmarks.dat\"\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "face_pose_predictor = dlib.shape_predictor(predictor_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "\n",
    "def image_to_land(image):\n",
    "    detected_faces = face_detector(image, 1)\n",
    "\n",
    "    if len(detected_faces) is not 1:\n",
    "        print('no face detected')\n",
    "\n",
    "    eye_index = 20\n",
    "    for i, face_rect in enumerate(detected_faces):\n",
    "        pose_landmarks = face_pose_predictor(image, face_rect)\n",
    "\n",
    "        tensor = FloatTensor(224, 224).zero_()\n",
    "        points = [(it.y, it.x) for it in [pose_landmarks.part(part) for part in range(17, 68)]]\n",
    "        i_min = min(i for i, _ in points)\n",
    "        j_min = min(j for _, j in points)\n",
    "\n",
    "        points = [(i - i_min, j - j_min) for i, j in points]\n",
    "\n",
    "        i_max = max(i for i, _ in points)\n",
    "        j_max = max(j for _, j in points)\n",
    "\n",
    "        eye_i = points[eye_index][0]\n",
    "        bound = int(i_max - eye_i) * 1.5 #eye_i를 기준으로 최대 bound\n",
    "        ratio = 112 / bound #최대 바운드가 112 좌표로 가도록 조정해야 한다. \n",
    "        center_j = j_max // 2\n",
    "\n",
    "        points = [(112 + (i - eye_i) * ratio, 112 + (j - center_j) * ratio) for i, j in points]\n",
    "\n",
    "        for i, j in points:\n",
    "            tensor[round(i)][round(j)] = 1\n",
    "            \n",
    "    return tensor.view(1, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz     = 1000 #- feature_size     # dimension of noise vector + feature vector\n",
    "nc     = 3        # number of channel - RGB\n",
    "ngf    = 64       # generator 레이어들의 필터 개수를 조정하기 위한 값\n",
    "ndf    = 64       # discriminator 레이어들의 필터 개수를 조정하기 위한 값\n",
    "niter  = 200      # total number of epoch\n",
    "lr     = 0.0002   # learning rate\n",
    "beta1  = 0.5      # hyper parameter of Adam optimizer\n",
    "ngpu   = 1        # number of using GPU\n",
    "\n",
    "imageSize = 64    \n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(_netG, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.main1 = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.main2 = nn.Sequential(\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "        \n",
    "        self.feature_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, ngf, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.Conv2d(ngf, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.Conv2d(ngf * 4, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, feature):\n",
    "        x = self.main1(noise)\n",
    "        y = self.feature_conv(feature)\n",
    "        x = torch.cat([x, y], 1)\n",
    "        output = self.main2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_dict_dir = \"G_state_dict.pth\"\n",
    "netG = _netG(ngpu)\n",
    "# netG.apply(weights_init)\n",
    "netG.load_state_dict(torch.load(state_dict_dir))\n",
    "netG = netG.cpu()\n",
    "torch.save(netG.state_dict(), \"netG_CPU.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test_image.jpg'\n",
    "image = io.imread(filename)\n",
    "landmark = image_to_land(image)\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(imageSize),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "landmark = transform(landmark).view(1, 1, imageSize, imageSize)\n",
    "\n",
    "from torch.autograd import Variable\n",
    "noise = Variable(torch.FloatTensor(1, nz, 1, 1).normal_(0, 1))\n",
    "landmark = Variable(landmark)\n",
    "fake = netG(noise, landmark)\n",
    "\n",
    "vutils.save_image(fake.data, 'test_image_land.png', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
